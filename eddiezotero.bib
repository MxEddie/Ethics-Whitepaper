
@article{min_recent_2023,
	title = {Recent {Advances} in {Natural} {Language} {Processing} via {Large} {Pre}-trained {Language} {Models}: {A} {Survey}},
	volume = {56},
	issn = {0360-0300},
	shorttitle = {Recent {Advances} in {Natural} {Language} {Processing} via {Large} {Pre}-trained {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3605943},
	doi = {10.1145/3605943},
	abstract = {Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.},
	number = {2},
	urldate = {2024-09-16},
	journal = {ACM Comput. Surv.},
	author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
	month = sep,
	year = {2023},
	pages = {30:1--30:40},
}

@inproceedings{benotti_understanding_2023,
	address = {Dubrovnik, Croatia},
	title = {Understanding {Ethics} in {NLP} {Authoring} and {Reviewing}},
	url = {https://aclanthology.org/2023.eacl-tutorials.4},
	doi = {10.18653/v1/2023.eacl-tutorials.4},
	abstract = {With NLP research now quickly being transferred into real-world applications, it is important to be aware of and think through the consequences of our scientific investigation. Such ethical considerations are important in both authoring and reviewing. This tutorial will equip participants with basic guidelines for thinking deeply about ethical issues and review common considerations that recur in NLP research. The methodology is interactive and participatory, including case studies and working in groups. Importantly, the participants will be co-building the tutorial outcomes and will be working to create further tutorial materials to share as public outcomes.},
	urldate = {2024-09-16},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Tutorial} {Abstracts}},
	publisher = {Association for Computational Linguistics},
	author = {Benotti, Luciana and Fort, Karën and Kan, Min-Yen and Tsvetkov, Yulia},
	editor = {Zanzotto, Fabio Massimo and Pradhan, Sameer},
	month = may,
	year = {2023},
	pages = {19--24},
}

@article{mancosu_what_2020,
	title = {What {You} {Can} {Scrape} and {What} {Is} {Right} to {Scrape}: {A} {Proposal} for a {Tool} to {Collect} {Public} {Facebook} {Data}},
	volume = {6},
	issn = {2056-3051},
	shorttitle = {What {You} {Can} {Scrape} and {What} {Is} {Right} to {Scrape}},
	url = {https://doi.org/10.1177/2056305120940703},
	doi = {10.1177/2056305120940703},
	abstract = {In reaction to the Cambridge Analytica scandal, Facebook has restricted the access to its Application Programming Interface (API). This new policy has damaged the possibility for independent researchers to study relevant topics in political and social behavior. Yet, much of the public information that the researchers may be interested in is still available on Facebook, and can be still systematically collected through web scraping techniques. The goal of this article is twofold. First, we discuss some ethical and legal issues that researchers should consider as they plan their collection and possible publication of Facebook data. In particular, we discuss what kind of information can be ethically gathered about the users (public information), how published data should look like to comply with privacy regulations (like the GDPR), and what consequences violating Facebook’s terms of service may entail for the researcher. Second, we present a scraping routine for public Facebook posts, and discuss some technical adjustments that can be performed for the data to be ethically and legally acceptable. The code employs screen scraping to collect the list of reactions to a Facebook public post, and performs a one-way cryptographic hash function on the users’ identifiers to pseudonymize their personal information, while still keeping them traceable within the data. This article contributes to the debate around freedom of internet research and the ethical concerns that might arise by scraping data from the social web.},
	language = {en},
	number = {3},
	urldate = {2024-09-16},
	journal = {Social Media + Society},
	author = {Mancosu, Moreno and Vegetti, Federico},
	month = jul,
	year = {2020},
	note = {Publisher: SAGE Publications Ltd},
	pages = {2056305120940703},
}

@article{fukuda-parr_emerging_2021,
	title = {Emerging consensus on ‘ethical {AI}’: {Human} rights critique of stakeholder guidelines},
	volume = {12},
	journal = {Global Policy},
	author = {Fukuda-Parr, Sakiko and Gibbons, Elizabeth},
	year = {2021},
	note = {Publisher: Wiley Online Library},
	pages = {32--44},
}

@article{wittenberg_labeling_2024,
	title = {Labeling {AI}-{Generated} {Content}: {Promises}, {Perils}, and {Future} {Directions}},
	shorttitle = {Labeling {AI}-{Generated} {Content}},
	url = {https://mit-genai.pubpub.org/pub/hu71se89/release/1},
	doi = {10.21428/e4baedd9.0319e3a6},
	abstract = {Labeling is a commonly proposed strategy for reducing the risks of generative artificial intelligence (AI). This approach involves applying visible content warnings to alert users to the presence of AI-generated media online (e.g., on social media, news sites, or search . . .},
	language = {en},
	urldate = {2024-09-11},
	journal = {An MIT Exploration of Generative AI},
	author = {Wittenberg, Chloe and Epstein, Ziv and Berinsky, Adam J. and Rand, David G.},
	month = mar,
	year = {2024},
	note = {Publisher: MIT},
}

@article{fiesler_remember_2024,
	title = {Remember the {Human}: {A} {Systematic} {Review} of {Ethical} {Considerations} in {Reddit} {Research}},
	volume = {8},
	shorttitle = {Remember the {Human}},
	url = {https://dl.acm.org/doi/10.1145/3633070},
	doi = {10.1145/3633070},
	abstract = {Reddit is one of the world's most prominent social media platforms, and also a valuable source of data for internet researchers. However, working with this kind of data also presents novel ethical complications for researchers, including issues around privacy, vulnerable populations, and unintended consequences. This paper describes an analysis of 134 papers that rely on Reddit data while also including some discussion of ethical implications and/or considerations by the researchers. Our analysis of these papers reveals common ethical issues and ethically motivated methodological decisions, as described by the researchers themselves, while also exposing some gaps for further ethical contemplation for researchers relying on Reddit data. Based on these findings, we close with a set of recommendations for ethically-informed methods and reflection for researchers working with social data.},
	number = {GROUP},
	urldate = {2024-09-11},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Fiesler, Casey and Zimmer, Michael and Proferes, Nicholas and Gilbert, Sarah and Jones, Naiyan},
	month = feb,
	year = {2024},
	pages = {5:1--5:33},
}

@article{klassen_this_2022,
	title = {“{This} {Isn}’t {Your} {Data}, {Friend}”: {Black} {Twitter} as a {Case} {Study} on {Research} {Ethics} for {Public} {Data}},
	volume = {8},
	issn = {2056-3051},
	shorttitle = {“{This} {Isn}’t {Your} {Data}, {Friend}”},
	url = {https://doi.org/10.1177/20563051221144317},
	doi = {10.1177/20563051221144317},
	abstract = {While research has been conducted with and in marginalized or vulnerable groups, explicit guidelines and best practices centering on specific communities are nascent. An excellent case study to engage within this aspect of research is Black Twitter. This research project considers the history of research with Black communities, combined with empirical work that explores how people who engage with Black Twitter think about research and researchers in order to suggest potential good practices and what researchers should know when studying Black Twitter or other digital traces from marginalized or vulnerable online communities. From our interviews, we gleaned that Black Twitter users feel differently about their content contributing to a research study depending on, for example, the type of content and the positionality of the researcher. Much of the advice participants shared for researchers involved an encouragement to cultivate cultural competency, get to know the community before researching it, and conduct research transparently. Aiming to improve the experience of research for both Black Twitter and researchers, this project is a stepping stone toward future work that further establishes and expands user perceptions of research ethics for online communities composed of vulnerable populations.},
	language = {en},
	number = {4},
	urldate = {2024-01-17},
	journal = {Social Media + Society},
	author = {Klassen, Shamika and Fiesler, Casey},
	month = oct,
	year = {2022},
	note = {Publisher: SAGE Publications Ltd},
}

@article{kekulluoglu_preserving_2018,
	title = {Preserving {Privacy} as {Social} {Responsibility} in {Online} {Social} {Networks}},
	volume = {18},
	issn = {1533-5399},
	url = {https://dl.acm.org/doi/10.1145/3158373},
	doi = {10.1145/3158373},
	abstract = {Online social networks provide an environment for their users to share content with others, where the user who shares a content item is put in charge, generally ignoring others that might be affected by it. However, a content that is shared by one user can very well violate the privacy of other users. To remedy this, ideally, all users who are related to a content should get a say in how the content should be shared. Recent approaches advocate the use of agreement technologies to enable stakeholders of a post to discuss the privacy configurations of a post. This allows related individuals to express concerns so that various privacy violations are avoided up front. Existing techniques try to establish an agreement on a single post. However, most of the time, agreement should be established over multiple posts such that the user can tolerate slight breaches of privacy in return of a right to share posts themselves in future interactions. As a result, users can help each other preserve their privacy, viewing this as their social responsibility. This article develops a reciprocity-based negotiation for reaching privacy agreements among users and introduces a negotiation architecture that combines semantic privacy rules with utility functions. We evaluate our approach over multiagent simulations with software agents that mimic users based on a user study.},
	number = {4},
	urldate = {2024-09-10},
	journal = {ACM Trans. Internet Technol.},
	author = {Kekulluoglu, Dilara and Kokciyan, Nadin and Yolum, Pinar},
	month = apr,
	year = {2018},
	pages = {42:1--42:22},
}

@article{nalbandian_eye_2022,
	title = {An eye for an '{I}:' a critical assessment of artificial intelligence tools in migration and asylum management},
	volume = {10},
	issn = {2214-594X},
	shorttitle = {An eye for an '{I}},
	doi = {10.1186/s40878-022-00305-0},
	abstract = {The promise of artificial intelligence has been originally to put technology at the service of people utilizing powerful information processors and 'smart' algorithms to quickly perform time-consuming data analysis. It soon though became apparent that the capacity of artificial intelligence to scrape and analyze big data would be particularly useful in surveillance policies. In the wider areas of migration and asylum management, increasingly sophisticated artificial intelligence tools have been used to register and manage vulnerable populations without much concern about the potential misuses of the data collected and the overall ethical and legal underpinnings of these operations. This article examines three cases in point. The first case investigates the United Nations High Commissioner for Refugees' decision to deploy a biometric matching engine engaging artificial intelligence to make accessing identification documents easier for both refugees and asylum seekers and the states and organizations they interact with. The second case focuses on the New Zealand government's introduction of artificial intelligence to improve border security and streamline immigration. The third case looks at data scraping and biometric recognition tools implemented by the United States government to track (and eventually deport) undocumented migrants. The article first shows how states and international organizations are increasingly turning to artificial intelligence tools to support the implementation of their immigration policies and programs. Subsequently, the article also outlines how even despite well-intentioned efforts, the decision to use artificial intelligence tools to increase efficiency and support the implementation of migration or asylum management policies and programs often involves jeopardizing or altogether sacrificing individuals' human rights, including privacy and security, and raises concerns about vulnerability and transparency.},
	language = {eng},
	number = {1},
	journal = {Comparative Migration Studies},
	author = {Nalbandian, Lucia},
	year = {2022},
	pmid = {35967601},
	pmcid = {PMC9361936},
	keywords = {Artificial intelligence, Asylum, Digital technology, Efficiency, Migration, New Zealand, United Nations, United States, Vulnerability},
	pages = {32},
}

@article{munn_uselessness_2022,
	title = {The uselessness of {AI} ethics},
	issn = {2730-5961},
	url = {https://doi.org/10.1007/s43681-022-00209-w},
	doi = {10.1007/s43681-022-00209-w},
	abstract = {As the awareness of AI’s power and danger has risen, the dominant response has been a turn to ethical principles. A flood of AI guidelines and codes of ethics have been released in both the public and private sector in the last several years. However, these are meaningless principles which are contested or incoherent, making them difficult to apply; they are isolated principles situated in an industry and education system which largely ignores ethics; and they are toothless principles which lack consequences and adhere to corporate agendas. For these reasons, I argue that AI ethical principles are useless, failing to mitigate the racial, social, and environmental damages of AI technologies in any meaningful sense. The result is a gap between high-minded principles and technological practice. Even when this gap is acknowledged and principles seek to be “operationalized,” the translation from complex social concepts to technical rulesets is non-trivial. In a zero-sum world, the dominant turn to AI principles is not just fruitless but a dangerous distraction, diverting immense financial and human resources away from potentially more effective activity. I conclude by highlighting alternative approaches to AI justice that go beyond ethical principles: thinking more broadly about systems of oppression and more narrowly about accuracy and auditing.},
	language = {en},
	urldate = {2022-10-26},
	journal = {AI and Ethics},
	author = {Munn, Luke},
	month = aug,
	year = {2022},
	keywords = {AI, Artificial intelligence, Ethical principles, Ethics, Morality, Research, Social ills},
}

@article{vynck_google_2024,
	title = {Google takes down {Gemini} {AI} image generator. {Here}’s what you need to know.},
	issn = {0190-8286},
	url = {https://www.washingtonpost.com/technology/2024/02/22/google-gemini-ai-image-generation-pause/},
	abstract = {Critics said the Google’s Gemini image generator created images of a woman pope and Black founding father.},
	language = {en-US},
	urldate = {2024-08-19},
	journal = {Washington Post},
	author = {Vynck, Gerrit De and Tiku, Nitasha},
	month = feb,
	year = {2024},
}

@inproceedings{wang_factors_2020,
	address = {Honolulu HI USA},
	title = {Factors {Influencing} {Perceived} {Fairness} in {Algorithmic} {Decision}-{Making}: {Algorithm} {Outcomes}, {Development} {Procedures}, and {Individual} {Differences}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Factors {Influencing} {Perceived} {Fairness} in {Algorithmic} {Decision}-{Making}},
	url = {https://dl.acm.org/doi/10.1145/3313831.3376813},
	doi = {10.1145/3313831.3376813},
	language = {en},
	urldate = {2023-06-30},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Wang, Ruotong and Harper, F. Maxwell and Zhu, Haiyi},
	month = apr,
	year = {2020},
	pages = {1--14},
}

@misc{groeneveld_olmo_2024,
	title = {{OLMo}: {Accelerating} the {Science} of {Language} {Models}},
	shorttitle = {{OLMo}},
	url = {http://arxiv.org/abs/2402.00838},
	doi = {10.48550/arXiv.2402.00838},
	abstract = {Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi Raghavi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
	month = jun,
	year = {2024},
	note = {arXiv:2402.00838 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{huang_are_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Are {Large} {Pre}-{Trained} {Language} {Models} {Leaking} {Your} {Personal} {Information}?},
	url = {https://aclanthology.org/2022.findings-emnlp.148},
	doi = {10.18653/v1/2022.findings-emnlp.148},
	abstract = {Are Large Pre-Trained Language Models Leaking Your Personal Information? In this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to leaking personal information. Specifically, we query PLMs for email addresses with contexts of the email address or prompts containing the owner's name. We find that PLMs do leak personal information due to memorization. However, since the models are weak at association, the risk of specific personal information being extracted by attackers is low. We hope this work could help the community to better understand the privacy risk of PLMs and bring new insights to make PLMs safe.},
	urldate = {2024-08-22},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {2038--2047},
}

@inproceedings{pan_risk_2023,
	address = {Singapore},
	title = {On the {Risk} of {Misinformation} {Pollution} with {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-emnlp.97},
	doi = {10.18653/v1/2023.findings-emnlp.97},
	abstract = {We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation (up to 87\%) in the performance of ODQA systems. Moreover, we uncover disparities in the attributes associated with persuading humans and machines, presenting an obstacle to current human-centric approaches to combat misinformation. To mitigate the harm caused by LLM-generated misinformation, we propose three defense strategies: misinformation detection, vigilant prompting, and reader ensemble. These approaches have demonstrated promising results, albeit with certain associated costs. Lastly, we discuss the practicality of utilizing LLMs as automatic misinformation generators and provide relevant resources and code to facilitate future research in this area.},
	urldate = {2024-08-22},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, William},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {1389--1403},
}

@article{liang_gpt_2023,
	title = {{GPT} detectors are biased against non-native {English} writers},
	volume = {4},
	issn = {2666-3899},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389923001307},
	doi = {10.1016/j.patter.2023.100779},
	abstract = {GPT detectors frequently misclassify non-native English writing as AI generated, raising concerns about fairness and robustness. Addressing the biases in these detectors is crucial to prevent the marginalization of non-native English speakers in evaluative and educational settings and to create a more equitable digital landscape.},
	number = {7},
	urldate = {2024-08-22},
	journal = {Patterns},
	author = {Liang, Weixin and Yuksekgonul, Mert and Mao, Yining and Wu, Eric and Zou, James},
	month = jul,
	year = {2023},
	pages = {100779},
}

@article{schick_self-diagnosis_2021,
	title = {Self-{Diagnosis} and {Self}-{Debiasing}: {A} {Proposal} for {Reducing} {Corpus}-{Based} {Bias} in {NLP}},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {Self-{Diagnosis} and {Self}-{Debiasing}},
	url = {https://doi.org/10.1162/tacl_a_00434},
	doi = {10.1162/tacl_a_00434},
	abstract = {⚠ This paper contains prompts and model outputs that are offensive in nature.When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1},
	urldate = {2024-04-21},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Schick, Timo and Udupa, Sahana and Schütze, Hinrich},
	month = dec,
	year = {2021},
	pages = {1408--1424},
}

@article{mason_conducting_2012,
	title = {Conducting behavioral research on {Amazon}’s {Mechanical} {Turk}},
	volume = {44},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-011-0124-6},
	doi = {10.3758/s13428-011-0124-6},
	abstract = {Amazon’s Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.},
	language = {en},
	number = {1},
	urldate = {2024-08-21},
	journal = {Behavior Research Methods},
	author = {Mason, Winter and Suri, Siddharth},
	month = mar,
	year = {2012},
	keywords = {Artificial Intelligence, Crowdsourcing, Mechanical turk, Medical Ethics, Online research},
	pages = {1--23},
}

@misc{perrigo_150_2023,
	title = {150 {AI} {Workers} {Vote} to {Unionize} at {Nairobi} {Meeting}},
	url = {https://time.com/6275995/chatgpt-facebook-african-workers-union/},
	abstract = {More than 150 workers for Facebook, TikTok and ChatGPT pledged to establish the first African Content Moderators Union},
	language = {en},
	urldate = {2024-08-21},
	journal = {TIME},
	author = {Perrigo, Billy},
	month = may,
	year = {2023},
}

@article{kaun_prison_2020,
	title = {Prison media work: from manual labor to the work of being tracked},
	volume = {42},
	issn = {0163-4437},
	shorttitle = {Prison media work},
	url = {https://doi.org/10.1177/0163443719899809},
	doi = {10.1177/0163443719899809},
	abstract = {Incarcerated individuals have long contributed to crucial societal infrastructures. From being leased work force building the railway in the United States to constructing canal systems in Sweden, prisoners’ labor has been widespread as an important part of value production. Part of the labor conducted by incarcerated people is related to the production, repair, and maintenance of media devices and media infrastructures constituting what we call prison media work. In this article, we trace the changing logics of prison media work historically since the inception of the modern prison at the turn of the 20th century. Based on archival material, interviews, and field observations, we outline a shift from physical manual labor toward the work of being tracked that is constitutive of surveillance capitalism in- and outside of the prison. We argue that prison media work holds an ambiguous position combining elements of exploitation and rehabilitation, but most importantly it is a dystopian magnifying glass of media work under surveillance capitalism.},
	language = {en},
	number = {7-8},
	urldate = {2024-08-21},
	journal = {Media, Culture \& Society},
	author = {Kaun, Anne and Stiernstedt, Fredrik},
	month = oct,
	year = {2020},
	note = {Publisher: SAGE Publications Ltd},
	pages = {1277--1292},
}

@misc{sigurgeirsson_just_2024,
	title = {Just {Because} {We} {Camp}, {Doesn}'t {Mean} {We} {Should}: {The} {Ethics} of {Modelling} {Queer} {Voices}},
	shorttitle = {Just {Because} {We} {Camp}, {Doesn}'t {Mean} {We} {Should}},
	url = {http://arxiv.org/abs/2406.07504},
	doi = {10.48550/arXiv.2406.07504},
	abstract = {Modern voice cloning models claim to be able to capture a diverse range of voices. We test the ability of a typical pipeline to capture the style known colloquially as "gay voice" and notice a homogenisation effect: synthesised speech is rated as sounding significantly "less gay" (by LGBTQ+ participants) than its corresponding ground-truth for speakers with "gay voice", but ratings actually increase for control speakers. Loss of "gay voice" has implications for accessibility. We also find that for speakers with "gay voice", loss of "gay voice" corresponds to lower similarity ratings. However, we caution that improving the ability of such models to synthesise ``gay voice'' comes with a great number of risks. We use this pipeline as a starting point for a discussion on the ethics of modelling queer voices more broadly. Collecting "clean" queer data has safety and fairness ramifications, and the resulting technology may cause harms from mockery to death.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Sigurgeirsson, Atli and Ungless, Eddie L.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.07504 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{anwar_foundational_2024,
	title = {Foundational {Challenges} in {Assuring} {Alignment} and {Safety} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2404.09932},
	doi = {10.48550/arXiv.2404.09932},
	abstract = {This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose \$200+\$ concrete research questions.},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and Edelman, Benjamin L. and Zhang, Zhaowei and Günther, Mario and Korinek, Anton and Hernandez-Orallo, Jose and Hammond, Lewis and Bigelow, Eric and Pan, Alexander and Langosco, Lauro and Korbak, Tomasz and Zhang, Heidi and Zhong, Ruiqi and hÉigeartaigh, Seán Ó and Recchia, Gabriel and Corsi, Giulio and Chan, Alan and Anderljung, Markus and Edwards, Lilian and Bengio, Yoshua and Chen, Danqi and Albanie, Samuel and Maharaj, Tegan and Foerster, Jakob and Tramer, Florian and He, He and Kasirzadeh, Atoosa and Choi, Yejin and Krueger, David},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09932 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{bannour_evaluating_2021,
	address = {Virtual},
	title = {Evaluating the carbon footprint of {NLP} methods: a survey and analysis of existing tools},
	shorttitle = {Evaluating the carbon footprint of {NLP} methods},
	url = {https://aclanthology.org/2021.sustainlp-1.2},
	doi = {10.18653/v1/2021.sustainlp-1.2},
	abstract = {Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications. Due to the significant environmental impact of deep learning, cost-benefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment. In this paper, we review the tools that are available to measure energy use and CO2 emissions of NLP methods. We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational set-ups (local server vs. computing facility). Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.},
	urldate = {2024-03-26},
	booktitle = {Proceedings of the {Second} {Workshop} on {Simple} and {Efficient} {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bannour, Nesrine and Ghannay, Sahar and Névéol, Aurélie and Ligozat, Anne-Laure},
	editor = {Moosavi, Nafise Sadat and Gurevych, Iryna and Fan, Angela and Wolf, Thomas and Hou, Yufang and Marasović, Ana and Ravi, Sujith},
	month = nov,
	year = {2021},
	pages = {11--21},
}

@article{dev_building_2024,
	title = {Building socio-culturally inclusive stereotype resources with community engagement},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Dev, Sunipa and Goyal, Jaya and Tewari, Dinesh and Dave, Shachi and Prabhakaran, Vinodkumar},
	year = {2024},
}

@inproceedings{abdalla_grey_2021,
	address = {New York, NY, USA},
	series = {{AIES} '21},
	title = {The {Grey} {Hoodie} {Project}: {Big} {Tobacco}, {Big} {Tech}, and the {Threat} on {Academic} {Integrity}},
	isbn = {978-1-4503-8473-5},
	shorttitle = {The {Grey} {Hoodie} {Project}},
	url = {https://doi.org/10.1145/3461702.3462563},
	doi = {10.1145/3461702.3462563},
	abstract = {As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place.},
	urldate = {2024-08-05},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Abdalla, Mohamed and Abdalla, Moustafa},
	month = jul,
	year = {2021},
	pages = {287--297},
}

@misc{carlini_extracting_2021,
	title = {Extracting {Training} {Data} from {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2012.07805},
	doi = {10.48550/arXiv.2012.07805},
	abstract = {It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
	month = jun,
	year = {2021},
	note = {arXiv:2012.07805 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{carlini_secret_2019,
	title = {The {Secret} {Sharer}: {Evaluating} and {Testing} {Unintended} {Memorization} in {Neural} {Networks}},
	shorttitle = {The {Secret} {Sharer}},
	url = {http://arxiv.org/abs/1802.08232},
	doi = {10.48550/arXiv.1802.08232},
	abstract = {This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn},
	month = jul,
	year = {2019},
	note = {arXiv:1802.08232 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{subramani_detecting_2023,
	address = {Toronto, Canada},
	title = {Detecting {Personal} {Information} in {Training} {Corpora}: an {Analysis}},
	shorttitle = {Detecting {Personal} {Information} in {Training} {Corpora}},
	url = {https://aclanthology.org/2023.trustnlp-1.18},
	doi = {10.18653/v1/2023.trustnlp-1.18},
	abstract = {Large language models are trained on increasing quantities of unstructured text, the largest sources of which are scraped from the Web. These Web scrapes are mainly composed of heterogeneous collections of text from multiple domains with minimal documentation. While some work has been done to identify and remove toxic, biased, or sexual language, the topic of personal information (PI) in textual data used for training Natural Language Processing (NLP) models is relatively under-explored. In this work, we draw from definitions of PI across multiple countries to define the first PI taxonomy of its kind, categorized by type and risk level. We then conduct a case study on the Colossal Clean Crawled Corpus (C4) and the Pile, to detect some of the highest-risk personal information, such as email addresses and credit card numbers, and examine the differences between automatic and regular expression-based approaches for their detection. We identify shortcomings in modern approaches for PI detection, and propose a reframing of the problem that is informed by global perspectives and the goals in personal information detection.},
	urldate = {2024-08-08},
	booktitle = {Proceedings of the 3rd {Workshop} on {Trustworthy} {Natural} {Language} {Processing} ({TrustNLP} 2023)},
	publisher = {Association for Computational Linguistics},
	author = {Subramani, Nishant and Luccioni, Sasha and Dodge, Jesse and Mitchell, Margaret},
	editor = {Ovalle, Anaelia and Chang, Kai-Wei and Mehrabi, Ninareh and Pruksachatkun, Yada and Galystan, Aram and Dhamala, Jwala and Verma, Apurv and Cao, Trista and Kumar, Anoop and Gupta, Rahul},
	month = jul,
	year = {2023},
	pages = {208--220},
}

@misc{courty_mlco2codecarbon_2024,
	title = {mlco2/codecarbon: v2.4.1},
	url = {https://doi.org/10.5281/zenodo.11171501},
	publisher = {Zenodo},
	author = {Courty, Benoit and Schmidt, Victor and Luccioni, Sasha and {Goyal-Kamal} and {MarionCoutarel} and Feld, Boris and Lecourt, Jérémy and {LiamConnell} and Saboni, Amine and {Inimaz} and {supatomic} and Léval, Mathilde and Blanche, Luis and Cruveiller, Alexis and {ouminasara} and Zhao, Franklin and Joshi, Aditya and Bogroff, Alexis and Lavoreille, Hugues de and Laskaris, Niko and Abati, Edoardo and Blank, Douglas and Wang, Ziyao and Catovic, Armin and Alencon, Marc and Stęchły, Michał and Bauer, Christian and Araújo, Lucas Otávio N. de and {JPW} and {MinervaBooks}},
	month = may,
	year = {2024},
	doi = {10.5281/zenodo.11171501},
}

@misc{luccioni_counting_2023,
	title = {Counting {Carbon}: {A} {Survey} of {Factors} {Influencing} the {Emissions} of {Machine} {Learning}},
	shorttitle = {Counting {Carbon}},
	url = {http://arxiv.org/abs/2302.08476},
	doi = {10.48550/arXiv.2302.08476},
	abstract = {Machine learning (ML) requires using energy to carry out computations during the model training process. The generation of this energy comes with an environmental cost in terms of greenhouse gas emissions, depending on quantity used and the energy source. Existing research on the environmental impacts of ML has been limited to analyses covering a small number of models and does not adequately represent the diversity of ML models and tasks. In the current study, we present a survey of the carbon emissions of 95 ML models across time and different tasks in natural language processing and computer vision. We analyze them in terms of the energy sources used, the amount of CO2 emissions produced, how these emissions evolve across time and how they relate to model performance. We conclude with a discussion regarding the carbon footprint of our field and propose the creation of a centralized repository for reporting and tracking these emissions.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Luccioni, Alexandra Sasha and Hernandez-Garcia, Alex},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08476 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{wang_energy_2023,
	title = {Energy and {Carbon} {Considerations} of {Fine}-{Tuning} {BERT}},
	url = {http://arxiv.org/abs/2311.10267},
	doi = {10.48550/arXiv.2311.10267},
	abstract = {Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Wang, Xiaorong and Na, Clara and Strubell, Emma and Friedler, Sorelle and Luccioni, Sasha},
	month = nov,
	year = {2023},
	note = {arXiv:2311.10267 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{patterson_carbon_2022,
	title = {The {Carbon} {Footprint} of {Machine} {Learning} {Training} {Will} {Plateau}, {Then} {Shrink}},
	volume = {55},
	issn = {1558-0814},
	url = {https://ieeexplore.ieee.org/document/9810097/?arnumber=9810097},
	doi = {10.1109/MC.2022.3148714},
	abstract = {Machine learning (ML) workloads have rapidly grown, raising concerns about their carbon footprint. We show four best practices to reduce ML training energy and carbon dioxide emissions. If the whole ML field adopts best practices, we predict that by 2030, total carbon emissions from training will decline.},
	number = {7},
	urldate = {2024-08-08},
	journal = {Computer},
	author = {Patterson, David and Gonzalez, Joseph and Hölzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
	month = jul,
	year = {2022},
	note = {Conference Name: Computer},
	keywords = {Best practices, Carbon dioxide, Carbon footprint, Emissions, Machine learning, Training data},
	pages = {18--28},
}

@inproceedings{luccioni_power_2024,
	address = {Rio de Janeiro Brazil},
	title = {Power {Hungry} {Processing}: {Watts} {Driving} the {Cost} of {AI} {Deployment}?},
	isbn = {9798400704505},
	shorttitle = {Power {Hungry} {Processing}},
	url = {https://dl.acm.org/doi/10.1145/3630106.3658542},
	doi = {10.1145/3630106.3658542},
	language = {en},
	urldate = {2024-08-08},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Luccioni, Sasha and Jernite, Yacine and Strubell, Emma},
	month = jun,
	year = {2024},
	pages = {85--99},
}

@misc{ovalle_are_2023,
	title = {Are you talking to ['xem'] or ['x', 'em']? {On} {Tokenization} and {Addressing} {Misgendering} in {LLMs} with {Pronoun} {Tokenization} {Parity}},
	shorttitle = {Are you talking to ['xem'] or ['x', 'em']?},
	url = {http://arxiv.org/abs/2312.11779},
	abstract = {A large body of NLP research has documented the ways gender biases manifest and amplify within large language models (LLMs), though this research has predominantly operated within a gender binary-centric context. A growing body of work has identified the harmful limitations of this gender-exclusive framing; many LLMs cannot correctly and consistently refer to persons outside the gender binary, especially if they use neopronouns. While data scarcity has been identified as a possible culprit, the precise mechanisms through which it influences LLM misgendering remain underexplored. Our work addresses this gap by studying data scarcity's role in subword tokenization and, consequently, the formation of LLM word representations. We uncover how the Byte-Pair Encoding (BPE) tokenizer, a backbone for many popular LLMs, contributes to neopronoun misgendering through out-of-vocabulary behavior. We introduce pronoun tokenization parity (PTP), a novel approach to reduce LLM neopronoun misgendering by preserving a token's functional structure. We evaluate PTP's efficacy using pronoun consistency-based metrics and a novel syntax-based metric. Through several controlled experiments, finetuning LLMs with PTP improves neopronoun consistency from 14.5\% to 58.4\%, highlighting the significant role tokenization plays in LLM pronoun consistency.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Ovalle, Anaelia and Mehrabi, Ninareh and Goyal, Palash and Dhamala, Jwala and Chang, Kai-Wei and Zemel, Richard and Galstyan, Aram and Gupta, Rahul},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11779 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{edwards_ai-powered_2023,
	title = {{AI}-powered {Bing} {Chat} spills its secrets via prompt injection attack [{Updated}]},
	url = {https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-spills-its-secrets-via-prompt-injection-attack/},
	author = {Edwards, Benj},
	month = feb,
	year = {2023},
	note = {Publication Title: Ars Technica},
}

@misc{noauthor_how_nodate,
	title = {How {ChatGPT} has been prompted to respect safety, fairness, and copyright {\textbar} {Ethics} in {AI}},
	url = {https://www.oxford-aiethics.ox.ac.uk/blog/how-chatgpt-has-been-prompted-respect-safety-fairness-and-copyright},
	urldate = {2024-07-12},
}

@misc{shin_can_2024,
	title = {Can {Prompt} {Modifiers} {Control} {Bias}? {A} {Comparative} {Analysis} of {Text}-to-{Image} {Generative} {Models}},
	shorttitle = {Can {Prompt} {Modifiers} {Control} {Bias}?},
	url = {http://arxiv.org/abs/2406.05602},
	doi = {10.48550/arXiv.2406.05602},
	abstract = {It has been shown that many generative models inherit and amplify societal biases. To date, there is no uniform/systematic agreed standard to control/adjust for these biases. This study examines the presence and manipulation of societal biases in leading text-to-image models: Stable Diffusion, DALL-E 3, and Adobe Firefly. Through a comprehensive analysis combining base prompts with modifiers and their sequencing, we uncover the nuanced ways these AI technologies encode biases across gender, race, geography, and region/culture. Our findings reveal the challenges and potential of prompt engineering in controlling biases, highlighting the critical need for ethical AI development promoting diversity and inclusivity. This work advances AI ethics by not only revealing the nuanced dynamics of bias in text-to-image generation models but also by offering a novel framework for future research in controlling bias. Our contributions-panning comparative analyses, the strategic use of prompt modifiers, the exploration of prompt sequencing effects, and the introduction of a bias sensitivity taxonomy-lay the groundwork for the development of common metrics and standard analyses for evaluating whether and how future AI models exhibit and respond to requests to adjust for inherent biases.},
	urldate = {2024-07-12},
	publisher = {arXiv},
	author = {Shin, Philip Wootaek and Ahn, Jihyun Janice and Yin, Wenpeng and Sampson, Jack and Narayanan, Vijaykrishnan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.05602 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_notitle_nodate,
	url = {https://computationalcreativity.net/iccc23/papers/ICCC-2023_paper_97.pdf},
	urldate = {2024-07-12},
}

@inproceedings{bansal_how_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {How well can {Text}-to-{Image} {Generative} {Models} understand {Ethical} {Natural} {Language} {Interventions}?},
	url = {https://aclanthology.org/2022.emnlp-main.88},
	doi = {10.18653/v1/2022.emnlp-main.88},
	abstract = {Text-to-image generative models have achieved unprecedented success in generating high-quality images based on natural language descriptions. However, it is shown that these models tend to favor specific social groups when prompted with neutral text descriptions (e.g., `a photo of a lawyer'). Following Zhao et al. (2021), we study the effect on the diversity of the generated images when adding ethical intervention that supports equitable judgment (e.g., `if all individuals can be a lawyer irrespective of their gender') in the input prompts. To this end, we introduce an Ethical NaTural Language Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset to evaluate the change in image generations conditional on ethical interventions across three social axes – gender, skin color, and culture. Through CLIP-based and human evaluation on minDALL.E, DALL.E-mini and Stable Diffusion, we find that the model generations cover diverse social groups while preserving the image quality. In some cases, the generations would be anti-stereotypical (e.g., models tend to create images with individuals that are perceived as man when fed with prompts about makeup) in the presence of ethical intervention. Preliminary studies indicate that a large change in the model predictions is triggered by certain phrases such as `irrespective of gender' in the context of gender bias in the ethical interventions. We release code and annotated data at https://github.com/Hritikbansal/entigen\_emnlp.},
	urldate = {2024-07-12},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bansal, Hritik and Yin, Da and Monajatipoor, Masoud and Chang, Kai-Wei},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {1358--1370},
}

@misc{cuthbertson_chatgpt_2023,
	title = {{ChatGPT} “grandma exploit” helps people pirate software},
	url = {https://www.independent.co.uk/tech/chatgpt-microsoft-windows-11-grandma-exploit-b2360213.html},
	author = {Cuthbertson, Anthony},
	month = jun,
	year = {2023},
	note = {Publication Title: The Independent},
}

@article{meng_owning_2021,
	title = {Owning and {Sharing}: {Privacy} {Perceptions} of {Smart} {Speaker} {Users}},
	volume = {5},
	shorttitle = {Owning and {Sharing}},
	url = {https://doi.org/10.1145/3449119},
	doi = {10.1145/3449119},
	abstract = {Intelligent personal assistants (IPA), such as Amazon Alexa and Google Assistant, are becoming increasingly present in multi-user households leading to questions about privacy and consent, particularly for those who do not directly own the device they interact with. When these devices are placed in shared spaces, every visitor and cohabitant becomes an indirect user, potentially leading to discomfort, misuse of services, or unintentional sharing of personal data. To better understand how owners and visitors perceive IPAs, we interviewed 10 in-house users (account owners and cohabitants) and 9 visitors from a student and young professionals sample who have interacted with such devices on various occasions. We find that cohabitants in shared households with regular IPA interactions see themselves as owners of the device, although not having the same controls as the account owner. Further, we determine the existence of a smart speaker etiquette which doubles as trust-based boundary management. Both in-house users and visitors demonstrate similar attitudes and concerns around data use, constant monitoring by the device, and the lack of transparency around device operations. We discuss interviewees' system understanding, concerns, and protection strategies and make recommendation to avoid tensions around shared devices.},
	number = {CSCW1},
	urldate = {2024-07-08},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	author = {Meng, Nicole and Keküllüoğlu, Dilara and Vaniea, Kami},
	month = apr,
	year = {2021},
	pages = {45:1--45:29},
}

@article{scheuerman_human_2023,
	title = {From {Human} to {Data} to {Dataset}: {Mapping} the {Traceability} of {Human} {Subjects} in {Computer} {Vision} {Datasets}},
	volume = {7},
	issn = {2573-0142},
	shorttitle = {From {Human} to {Data} to {Dataset}},
	url = {https://dl.acm.org/doi/10.1145/3579488},
	doi = {10.1145/3579488},
	abstract = {Computer vision is a "data hungry" field. Researchers and practitioners who work on human-centric computer vision, like facial recognition, emphasize the necessity of vast amounts of data for more robust and accurate models. Humans are seen as a data resource which can be converted into datasets. The necessity of data has led to a proliferation of gathering data from easily available sources, including "public" data from the web. Yet the use of public data has significant ethical implications for the human subjects in datasets. We bridge academic conversations on the ethics of using publicly obtained data with concerns about privacy and agency associated with computer vision applications. Specifically, we examine how practices of dataset construction from public data-not only from websites, but also from public settings and public records-make it extremely difficult for human subjects to trace their images as they are collected, converted into datasets, distributed for use, and, in some cases, retracted. We discuss two interconnected barriers current data practices present to providing an ethics of traceability for human subjects: awareness and control. We conclude with key intervention points for enabling traceability for data subjects. We also offer suggestions for an improved ethics of traceability to enable both awareness and control for individual subjects in dataset curation practices.},
	language = {en},
	number = {CSCW1},
	urldate = {2024-07-05},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Scheuerman, Morgan Klaus and Weathington, Katy and Mugunthan, Tarun and Denton, Emily and Fiesler, Casey},
	month = apr,
	year = {2023},
	pages = {1--33},
}

@article{kim_propile_2023,
	title = {{ProPILE}: {Probing} {Privacy} {Leakage} in {Large} {Language} {Models}},
	volume = {36},
	shorttitle = {{ProPILE}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/420678bb4c8251ab30e765bc27c3b047-Abstract-Conference.html},
	language = {en},
	urldate = {2024-07-05},
	journal = {Advances in Neural Information Processing Systems},
	author = {Kim, Siwon and Yun, Sangdoo and Lee, Hwaran and Gubri, Martin and Yoon, Sungroh and Oh, Seong Joon},
	month = dec,
	year = {2023},
	pages = {20750--20762},
}

@article{xiao_bad_2020,
	title = {Bad {Bots}: {Regulating} the {Scraping} of {Public} {Personal} {Information} {Notes}},
	volume = {34},
	url = {https://heinonline.org/HOL/P?h=hein.journals/hjlt34&i=713},
	language = {eng},
	number = {2},
	journal = {Harvard Journal of Law \& Technology (Harvard JOLT)},
	author = {Xiao, Geoffrey},
	year = {2020},
	pages = {701--732},
}

@article{winograd_loose-lipped_2022,
	title = {Loose-{Lipped} {Large} {Language} {Models} {Spill} {Your} {Secrets}: {The} {Privacy} {Implications} of {Large} {Language} {Models} {Notes}},
	volume = {36},
	url = {https://heinonline.org/HOL/P?h=hein.journals/hjlt36&i=640},
	language = {eng},
	number = {2},
	journal = {Harvard Journal of Law \& Technology (Harvard JOLT)},
	author = {Winograd, Amy},
	year = {2022},
	pages = {615--656},
}

@article{fiesler_participant_2018,
	title = {“{Participant}” {Perceptions} of {Twitter} {Research} {Ethics}},
	volume = {4},
	issn = {2056-3051, 2056-3051},
	url = {http://journals.sagepub.com/doi/10.1177/2056305118763366},
	doi = {10.1177/2056305118763366},
	abstract = {Social computing systems such as Twitter present new research sites that have provided billions of data points to researchers. However, the availability of public social media data has also presented ethical challenges. As the research community works to create ethical norms, we should be considering users’ concerns as well. With this in mind, we report on an exploratory survey of Twitter users’ perceptions of the use of tweets in research. Within our survey sample, few users were previously aware that their public tweets could be used by researchers, and the majority felt that researchers should not be able to use tweets without consent. However, we find that these attitudes are highly contextual, depending on factors such as how the research is conducted or disseminated, who is conducting it, and what the study is about. The findings of this study point to potential best practices for researchers conducting observation and analysis of public data.},
	language = {en},
	number = {1},
	urldate = {2024-07-05},
	journal = {Social Media + Society},
	author = {Fiesler, Casey and Proferes, Nicholas},
	month = jan,
	year = {2018},
	pages = {205630511876336},
}

@inproceedings{ovalle_factoring_2023,
	address = {Montr{\textbackslash}'\{e\}al QC Canada},
	title = {Factoring the {Matrix} of {Domination}: {A} {Critical} {Review} and {Reimagination} of {Intersectionality} in {AI} {Fairness}},
	isbn = {9798400702310},
	shorttitle = {Factoring the {Matrix} of {Domination}},
	url = {https://dl.acm.org/doi/10.1145/3600211.3604705},
	doi = {10.1145/3600211.3604705},
	language = {en},
	urldate = {2024-07-05},
	booktitle = {Proceedings of the 2023 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Ovalle, Anaelia and Subramonian, Arjun and Gautam, Vagrant and Gee, Gilbert and Chang, Kai-Wei},
	month = aug,
	year = {2023},
	pages = {496--511},
}

@article{widder_epistemic_2024,
	title = {Epistemic {Power} in {AI} {Ethics} {Labor}: {Legitimizing} {Located} {Complaints}},
	abstract = {What counts as legitimate AI ethics labor, and consequently, what are the epistemic terms on which AI ethics claims are rendered legitimate? Based on 75 interviews with technologists including researchers, developers, open source contributors, and activists, this paper explores the various epistemic bases from which AI ethics is discussed and practiced. In the context of outside attacks on AI ethics as an impediment to “progress,” I show how some AI ethics practices have reached toward authority from automation and quantification, and achieved some legitimacy as a result, while those based on richly embodied and situated lived experience have not. This paper draws together the work of feminist Anthropology and Science and Technology Studies scholars Diana Forsythe and Lucy Suchman with the works of postcolonial feminist theorist Sara Ahmed and Black feminist theorist Kristie Dotson to examine the implications of dominant AI ethics practices.},
	language = {en},
	author = {Widder, David Gray},
	year = {2024},
}

@inproceedings{qadri_ais_2023,
	address = {New York, NY, USA},
	series = {{FAccT} '23},
	title = {{AI}’s {Regimes} of {Representation}: {A} {Community}-centered {Study} of {Text}-to-{Image} {Models} in {South} {Asia}},
	isbn = {9798400701924},
	shorttitle = {{AI}’s {Regimes} of {Representation}},
	url = {https://dl.acm.org/doi/10.1145/3593013.3594016},
	doi = {10.1145/3593013.3594016},
	abstract = {This paper presents a community-centered study of cultural limitations of text-to-image (T2I) models in the South Asian context. We theorize these failures using scholarship on dominant media regimes of representations and locate them within participants’ reporting of their existing social marginalizations. We thus show how generative AI can reproduce an outsiders gaze for viewing South Asian cultures, shaped by global and regional power inequities. By centering communities as experts and soliciting their perspectives on T2I limitations, our study adds rich nuance into existing evaluative frameworks and deepens our understanding of the culturally-specific ways AI technologies can fail in non-Western and Global South settings. We distill lessons for responsible development of T2I models, recommending concrete pathways forward that can allow for recognition of structural inequalities.},
	urldate = {2024-06-13},
	booktitle = {Proceedings of the 2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Qadri, Rida and Shelby, Renee and Bennett, Cynthia L. and Denton, Emily},
	month = jun,
	year = {2023},
	keywords = {AI harms, South Asia, cultural harms of AI, failure modes, generative AI, human-centered AI, non-western AI fairness, qualitative research in AI, text-to-image models},
	pages = {506--517},
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	copyright = {Copyright © 2017, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/356/6334/183},
	doi = {10.1126/science.aal4230},
	abstract = {Machines learn what people know implicitly
AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.
Science, this issue p. 183; see also p. 133
Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.
Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.
Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.},
	language = {en},
	number = {6334},
	urldate = {2020-10-05},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	pmid = {28408601},
	note = {Publisher: American Association for the Advancement of Science
Section: Reports},
	pages = {183--186},
}

@incollection{tan_assessing_2019,
	address = {Red Hook, NY, USA},
	title = {Assessing social and intersectional biases in contextualized word representations},
	abstract = {Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.},
	number = {1185},
	urldate = {2024-01-19},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Tan, Yi Chern and Celis, L. Elisa},
	month = dec,
	year = {2019},
	pages = {13230--13241},
}

@inproceedings{may_measuring_2019,
	address = {Minneapolis, Minnesota},
	title = {On {Measuring} {Social} {Biases} in {Sentence} {Encoders}},
	url = {https://www.aclweb.org/anthology/N19-1063},
	doi = {10.18653/v1/N19-1063},
	abstract = {The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test's assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.},
	urldate = {2020-10-05},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R. and Rudinger, Rachel},
	month = jun,
	year = {2019},
	keywords = {social bias},
	pages = {622--628},
}

@inproceedings{guo_detecting_2021,
	address = {Virtual Event USA},
	title = {Detecting {Emergent} {Intersectional} {Biases}: {Contextualized} {Word} {Embeddings} {Contain} a {Distribution} of {Human}-like {Biases}},
	isbn = {978-1-4503-8473-5},
	shorttitle = {Detecting {Emergent} {Intersectional} {Biases}},
	url = {https://dl.acm.org/doi/10.1145/3461702.3462536},
	doi = {10.1145/3461702.3462536},
	abstract = {With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that occur in contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models.},
	language = {en},
	urldate = {2022-01-31},
	booktitle = {Proceedings of the 2021 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Guo, Wei and Caliskan, Aylin},
	month = jul,
	year = {2021},
	pages = {122--133},
}

@inproceedings{ungless_robust_2022,
	address = {Abu Dhabi, UAE},
	title = {A {Robust} {Bias} {Mitigation} {Procedure} {Based} on the {Stereotype} {Content} {Model}},
	url = {https://aclanthology.org/2022.nlpcss-1.23},
	abstract = {The Stereotype Content model (SCM) states that we tend to perceive minority groups as cold, incompetent or both. In this paper we adapt existing work to demonstrate that the Stereotype Content model holds for contextualised word embeddings, then use these results to evaluate a fine-tuning process designed to drive a language model away from stereotyped portrayals of minority groups. We find the SCM terms are better able to capture bias than demographic agnostic terms related to pleasantness. Further, we were able to reduce the presence of stereotypes in the model through a simple fine-tuning procedure that required minimal human and computer resources, without harming downstream performance. We present this work as a prototype of a debiasing procedure that aims to remove the need for a priori knowledge of the specifics of bias in the model.},
	booktitle = {Proceedings of the {Fifth} {Workshop} on {Natural} {Language} {Processing} and {Computational} {Social} {Science} ({NLP}+{CSS})},
	publisher = {Association for Computational Linguistics},
	author = {Ungless, Eddie L. and Rafferty, Amy and Nag, Hrichika and Ross, Björn},
	month = nov,
	year = {2022},
	pages = {207--217},
}

@inproceedings{orgad_how_2022,
	address = {Seattle, United States},
	title = {How {Gender} {Debiasing} {Affects} {Internal} {Model} {Representations}, and {Why} {It} {Matters}},
	url = {https://aclanthology.org/2022.naacl-main.188},
	doi = {10.18653/v1/2022.naacl-main.188},
	abstract = {Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models' internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream fine-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing. Through experiments on two tasks and multiple bias metrics, we show that our intrinsic bias metric is a better indicator of debiasing than (a contextual adaptation of) the standard WEAT metric, and can also expose cases of superficial debiasing. Our framework provides a comprehensive perspective on bias in NLP models, which can be applied to deploy NLP systems in a more informed manner. Our code and model checkpoints are publicly available.},
	urldate = {2024-07-04},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Orgad, Hadas and Goldfarb-Tarrant, Seraphina and Belinkov, Yonatan},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {2602--2628},
}

@article{welbl_challenges_2021,
	title = {Challenges in {Detoxifying} {Language} {Models}},
	url = {http://arxiv.org/abs/2109.07445},
	abstract = {Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the RealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions -- highlighting further the nuances involved in careful evaluation of LM toxicity.},
	urldate = {2021-09-21},
	journal = {arXiv:2109.07445 [cs]},
	author = {Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.07445},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, I.2.6, I.2.7},
}

@misc{briesch_large_2024,
	title = {Large {Language} {Models} {Suffer} {From} {Their} {Own} {Output}: {An} {Analysis} of the {Self}-{Consuming} {Training} {Loop}},
	shorttitle = {Large {Language} {Models} {Suffer} {From} {Their} {Own} {Output}},
	url = {http://arxiv.org/abs/2311.16822},
	doi = {10.48550/arXiv.2311.16822},
	abstract = {Large Language Models (LLM) are already widely used to generate content for a variety of online platforms. As we are not able to safely distinguish LLM-generated content from human-produced content, LLM-generated content is used to train the next generation of LLMs, giving rise to a self-consuming training loop. From the image generation domain we know that such a self-consuming training loop reduces both quality and diversity of images finally ending in a model collapse. However, it is unclear whether this alarming effect can also be observed for LLMs. Therefore, we present the first study investigating the self-consuming training loop for LLMs. Further, we propose a novel method based on logic expressions that allows us to unambiguously verify the correctness of LLM-generated content, which is difficult for natural language text. We find that the self-consuming training loop produces correct outputs, however, the output declines in its diversity depending on the proportion of the used generated data. Fresh data can slow down this decline, but not stop it. Given these concerning results, we encourage researchers to study methods to negate this process.},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Briesch, Martin and Sobania, Dominik and Rothlauf, Franz},
	month = jun,
	year = {2024},
	note = {arXiv:2311.16822 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{felkner_gpt_2024,
	title = {{GPT} is {Not} an {Annotator}: {The} {Necessity} of {Human} {Annotation} in {Fairness} {Benchmark} {Construction}},
	shorttitle = {{GPT} is {Not} an {Annotator}},
	url = {http://arxiv.org/abs/2405.15760},
	doi = {10.48550/arXiv.2405.15760},
	abstract = {Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Felkner, Virginia K. and Thompson, Jennifer A. and May, Jonathan},
	month = may,
	year = {2024},
	note = {arXiv:2405.15760 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, I.2.7, K.4.2},
}

@misc{bavaresco_llms_2024,
	title = {{LLMs} instead of {Human} {Judges}? {A} {Large} {Scale} {Empirical} {Study} across 20 {NLP} {Evaluation} {Tasks}},
	shorttitle = {{LLMs} instead of {Human} {Judges}?},
	url = {http://arxiv.org/abs/2406.18403},
	doi = {10.48550/arXiv.2406.18403},
	abstract = {There is an increasing trend towards evaluating NLP models with LLM-generated judgments instead of human judgments. In the absence of a comparison against human data, this raises concerns about the validity of these evaluations; in case they are conducted with proprietary models, this also raises concerns over reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with human annotations, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show that each LLM exhibits a large variance across datasets in its correlation to human judgments. We conclude that LLMs are not yet ready to systematically replace human judges in NLP.},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Bavaresco, Anna and Bernardi, Raffaella and Bertolazzi, Leonardo and Elliott, Desmond and Fernández, Raquel and Gatt, Albert and Ghaleb, Esam and Giulianelli, Mario and Hanna, Michael and Koller, Alexander and Martins, André F. T. and Mondorf, Philipp and Neplenbroek, Vera and Pezzelle, Sandro and Plank, Barbara and Schlangen, David and Suglia, Alessandro and Surikuchi, Aditya K. and Takmaz, Ece and Testoni, Alberto},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18403 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{rottger_xstest_2024,
	address = {Mexico City, Mexico},
	title = {{XSTest}: {A} {Test} {Suite} for {Identifying} {Exaggerated} {Safety} {Behaviours} in {Large} {Language} {Models}},
	shorttitle = {{XSTest}},
	url = {https://aclanthology.org/2024.naacl-long.301},
	abstract = {Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.},
	urldate = {2024-06-20},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Röttger, Paul and Kirk, Hannah and Vidgen, Bertie and Attanasio, Giuseppe and Bianchi, Federico and Hovy, Dirk},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	pages = {5377--5400},
}

@misc{raji_ai_2021,
	title = {{AI} and the {Everything} in the {Whole} {Wide} {World} {Benchmark}},
	url = {http://arxiv.org/abs/2111.15366},
	abstract = {There is a tendency across different subﬁelds in AI to valorize a small collection of inﬂuential benchmarks. These benchmarks operate as stand-ins for a range of anointed common problems that are frequently framed as foundational milestones on the path towards ﬂexible and generalizable AI systems. State-of-the-art performance on these benchmarks is widely understood as indicative of progress towards these long-term goals. In this position paper, we explore the limits of such benchmarks in order to reveal the construct validity issues in their framing as the functionally “general” broad measures of progress they are set up to be.},
	language = {en},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Raji, Inioluwa Deborah and Bender, Emily M. and Paullada, Amandalynne and Denton, Emily and Hanna, Alex},
	month = nov,
	year = {2021},
	note = {arXiv:2111.15366 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance},
}

@inproceedings{gonen_lipstick_2019,
	address = {Minneapolis, Minnesota},
	title = {Lipstick on a {Pig}: {Debiasing} {Methods} {Cover} up {Systematic} {Gender} {Biases} in {Word} {Embeddings} {But} do not {Remove} {Them}},
	shorttitle = {Lipstick on a {Pig}},
	url = {https://aclanthology.org/N19-1061},
	doi = {10.18653/v1/N19-1061},
	abstract = {Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.},
	urldate = {2024-01-26},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Gonen, Hila and Goldberg, Yoav},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {609--614},
}

@inproceedings{delobelle_measuring_2022,
	title = {Measuring fairness with biased rulers: {A} comparative study on bias metrics for pre-trained language models},
	shorttitle = {Measuring fairness with biased rulers},
	url = {https://lirias.kuleuven.be/retrieve/667403},
	urldate = {2024-06-15},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Delobelle, Pieter and Tokpo, Ewoenam Kwaku and Calders, Toon and Berendt, Bettina},
	year = {2022},
	pages = {1693--1706},
}

@inproceedings{liang_towards_2020,
	address = {Online},
	title = {Towards {Debiasing} {Sentence} {Representations}},
	url = {https://aclanthology.org/2020.acl-main.488},
	doi = {10.18653/v1/2020.acl-main.488},
	abstract = {As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.},
	urldate = {2024-01-29},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Liang, Paul Pu and Li, Irene Mengze and Zheng, Emily and Lim, Yao Chong and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {5502--5515},
}

@misc{hooker_characterising_2020,
	title = {Characterising {Bias} in {Compressed} {Models}},
	url = {http://arxiv.org/abs/2010.03058},
	doi = {10.48550/arXiv.2010.03058},
	abstract = {The popularity and widespread use of pruning and quantization is driven by the severe resource constraints of deploying deep neural networks to environments with strict latency, memory and energy requirements. These techniques achieve high levels of compression with negligible impact on top-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides disproportionately high errors on a small subset of examples; we call this subset Compression Identified Exemplars (CIE). We further establish that for CIE examples, compression amplifies existing algorithmic bias. Pruning disproportionately impacts performance on underrepresented features, which often coincides with considerations of fairness. Given that CIE is a relatively small subset but a great contributor of error in the model, we propose its use as a human-in-the-loop auditing tool to surface a tractable subset of the dataset for further inspection or annotation by a domain expert. We provide qualitative and quantitative support that CIE surfaces the most challenging examples in the data distribution for human-in-the-loop auditing.},
	urldate = {2024-06-10},
	publisher = {arXiv},
	author = {Hooker, Sara and Moorosi, Nyalleng and Clark, Gregory and Bengio, Samy and Denton, Emily},
	month = dec,
	year = {2020},
	note = {arXiv:2010.03058 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{hooker_moving_2021,
	title = {Moving beyond “algorithmic bias is a data problem”},
	volume = {2},
	issn = {2666-3899},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389921000611},
	doi = {10.1016/j.patter.2021.100241},
	abstract = {A surprisingly sticky belief is that a machine learning model merely reflects existing algorithmic bias in the dataset and does not itself contribute to harm. Why, despite clear evidence to the contrary, does the myth of the impartial model still hold allure for so many within our research community? Algorithms are not impartial, and some design choices are better than others. Recognizing how model design impacts harm opens up new mitigation techniques that are less burdensome than comprehensive data collection.},
	number = {4},
	urldate = {2024-06-10},
	journal = {Patterns},
	author = {Hooker, Sara},
	month = apr,
	year = {2021},
	pages = {100241},
}

@article{cabello_independence_2023,
	title = {On the {Independence} of {Association} {Bias} and {Empirical} {Fairness} in {Language} {Models}},
	url = {https://api.semanticscholar.org/CorpusID:258236466},
	journal = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
	author = {Cabello, Laura and Zee, Anna Katrine van and Søgaard, Anders},
	year = {2023},
}

@misc{hong_whos_2024,
	title = {Who's in and who's out? {A} case study of multimodal {CLIP}-filtering in {DataComp}},
	shorttitle = {Who's in and who's out?},
	url = {http://arxiv.org/abs/2405.08209},
	doi = {10.48550/arXiv.2405.08209},
	abstract = {As training datasets become increasingly drawn from unstructured, uncontrolled environments such as the web, researchers and industry practitioners have increasingly relied upon data filtering techniques to "filter out the noise" of web-scraped data. While datasets have been widely shown to reflect the biases and values of their creators, in this paper we contribute to an emerging body of research that assesses the filters used to create these datasets. We show that image-text data filtering also has biases and is value-laden, encoding specific notions of what is counted as "high-quality" data. In our work, we audit a standard approach of image-text CLIP-filtering on the academic benchmark DataComp's CommonPool by analyzing discrepancies of filtering through various annotation techniques across multiple modalities of image, text, and website source. We find that data relating to several imputed demographic groups -- such as LGBTQ+ people, older women, and younger men -- are associated with higher rates of exclusion. Moreover, we demonstrate cases of exclusion amplification: not only are certain marginalized groups already underrepresented in the unfiltered data, but CLIP-filtering excludes data from these groups at higher rates. The data-filtering step in the machine learning pipeline can therefore exacerbate representation disparities already present in the data-gathering step, especially when existing filters are designed to optimize a specifically-chosen downstream performance metric like zero-shot image classification accuracy. Finally, we show that the NSFW filter fails to remove sexually-explicit content from CommonPool, and that CLIP-filtering includes several categories of copyrighted content at high rates. Our conclusions point to a need for fundamental changes in dataset creation and filtering practices.},
	urldate = {2024-05-30},
	publisher = {arXiv},
	author = {Hong, Rachel and Agnew, William and Kohno, Tadayoshi and Morgenstern, Jamie},
	month = may,
	year = {2024},
	note = {arXiv:2405.08209 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{talat_disembodied_2021,
	title = {Disembodied {Machine} {Learning}: {On} the {Illusion} of {Objectivity} in {NLP}},
	shorttitle = {Disembodied {Machine} {Learning}},
	url = {http://arxiv.org/abs/2101.11974},
	abstract = {Machine Learning seeks to identify and encode bodies of knowledge within provided datasets. However, data encodes subjective content, which determines the possible outcomes of the models trained on it. Because such subjectivity enables marginalisation of parts of society, it is termed (social) `bias' and sought to be removed. In this paper, we contextualise this discourse of bias in the ML community against the subjective choices in the development process. Through a consideration of how choices in data and model development construct subjectivity, or biases that are represented in a model, we argue that addressing and mitigating biases is near-impossible. This is because both data and ML models are objects for which meaning is made in each step of the development pipeline, from data selection over annotation to model training and analysis. Accordingly, we find the prevalent discourse of bias limiting in its ability to address social marginalisation. We recommend to be conscientious of this, and to accept that de-biasing methods only correct for a fraction of biases.},
	urldate = {2021-04-08},
	journal = {arXiv:2101.11974 [cs]},
	author = {Talat, Zeerak and Lulz, Smarika and Bingel, Joachim and Augenstein, Isabelle},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.11974},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@inproceedings{guerdan_groundless_2023,
	address = {Chicago IL USA},
	title = {Ground(less) {Truth}: {A} {Causal} {Framework} for {Proxy} {Labels} in {Human}-{Algorithm} {Decision}-{Making}},
	isbn = {9798400701924},
	shorttitle = {Ground(less) {Truth}},
	url = {https://dl.acm.org/doi/10.1145/3593013.3594036},
	doi = {10.1145/3593013.3594036},
	language = {en},
	urldate = {2024-05-28},
	booktitle = {2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Guerdan, Luke and Coston, Amanda and Wu, Zhiwei Steven and Holstein, Kenneth},
	month = jun,
	year = {2023},
	pages = {688--704},
}

@inproceedings{fortuna_directions_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Directions for {NLP} {Practices} {Applied} to {Online} {Hate} {Speech} {Detection}},
	url = {https://aclanthology.org/2022.emnlp-main.809},
	doi = {10.18653/v1/2022.emnlp-main.809},
	abstract = {Addressing hate speech in online spaces has been conceptualized as a classification task that uses Natural Language Processing (NLP) techniques. Through this conceptualization, the hate speech detection task has relied on common conventions and practices from NLP. For instance, inter-annotator agreement is conceptualized as a way to measure dataset quality and certain metrics and benchmarks are used to assure model generalization. However, hate speech is a deeply complex and situated concept that eludes such static and disembodied practices. In this position paper, we critically reflect on these methodologies for hate speech detection, we argue that many conventions in NLP are poorly suited for the problem and encourage researchers to develop methods that are more appropriate for the task.},
	urldate = {2024-05-28},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Fortuna, Paula and Dominguez, Monica and Wanner, Leo and Talat, Zeerak},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {11794--11805},
}

@inproceedings{talat_understanding_2017,
	address = {Vancouver, BC, Canada},
	title = {Understanding {Abuse}: {A} {Typology} of {Abusive} {Language} {Detection} {Subtasks}},
	shorttitle = {Understanding {Abuse}},
	url = {https://aclanthology.org/W17-3012},
	doi = {10.18653/v1/W17-3012},
	abstract = {As the body of research on abusive language detection and analysis grows, there is a need for critical consideration of the relationships between different subtasks that have been grouped under this label. Based on work on hate speech, cyberbullying, and online abuse we propose a typology that captures central similarities and differences between subtasks and discuss the implications of this for data annotation and feature construction. We emphasize the practical actions that can be taken by researchers to best approach their abusive language detection subtask of interest.},
	urldate = {2024-05-17},
	booktitle = {Proceedings of the {First} {Workshop} on {Abusive} {Language} {Online}},
	publisher = {Association for Computational Linguistics},
	author = {Talat, Zeerak and Davidson, Thomas and Warmsley, Dana and Weber, Ingmar},
	editor = {Talat, Zeerak and Chung, Wendy Hui Kyong and Hovy, Dirk and Tetreault, Joel},
	month = aug,
	year = {2017},
	pages = {78--84},
}

@inproceedings{al_kuwatly_identifying_2020,
	address = {Online},
	title = {Identifying and {Measuring} {Annotator} {Bias} {Based} on {Annotators}' {Demographic} {Characteristics}},
	url = {https://aclanthology.org/2020.alw-1.21},
	doi = {10.18653/v1/2020.alw-1.21},
	abstract = {Machine learning is recently used to detect hate speech and other forms of abusive language in online platforms. However, a notable weakness of machine learning models is their vulnerability to bias, which can impair their performance and fairness. One type is annotator bias caused by the subjective perception of the annotators. In this work, we investigate annotator bias using classification models trained on data from demographically distinct annotator groups. To do so, we sample balanced subsets of data that are labeled by demographically distinct annotators. We then train classifiers on these subsets, analyze their performances on similarly grouped test sets, and compare them statistically. Our findings show that the proposed approach successfully identifies bias and that demographic features, such as first language, age, and education, correlate with significant performance differences.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the {Fourth} {Workshop} on {Online} {Abuse} and {Harms}},
	publisher = {Association for Computational Linguistics},
	author = {Al Kuwatly, Hala and Wich, Maximilian and Groh, Georg},
	editor = {Akiwowo, Seyi and Vidgen, Bertie and Prabhakaran, Vinodkumar and Talat, Zeerak},
	month = nov,
	year = {2020},
	pages = {184--190},
}

@inproceedings{manerba_fine-grained_2021,
	address = {Online},
	title = {Fine-{Grained} {Fairness} {Analysis} of {Abusive} {Language} {Detection} {Systems} with {CheckList}},
	url = {https://aclanthology.org/2021.woah-1.9},
	doi = {10.18653/v1/2021.woah-1.9},
	abstract = {Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender. This is a crucial issue, which may harm minorities and underrepresented groups if such systems were integrated in real-world applications. In this paper, we create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English. We compare the behaviour of two BERT-based models, one trained on a generic hate speech dataset and the other on a dataset for misogyny detection. Our evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation. We release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the 5th {Workshop} on {Online} {Abuse} and {Harms} ({WOAH} 2021)},
	publisher = {Association for Computational Linguistics},
	author = {Manerba, Marta Marchiori and Tonelli, Sara},
	editor = {Mostafazadeh Davani, Aida and Kiela, Douwe and Lambert, Mathias and Vidgen, Bertie and Prabhakaran, Vinodkumar and Talat, Zeerak},
	month = aug,
	year = {2021},
	pages = {81--91},
}

@inproceedings{talat_are_2016,
	address = {Austin, Texas},
	title = {Are {You} a {Racist} or {Am} {I} {Seeing} {Things}? {Annotator} {Influence} on {Hate} {Speech} {Detection} on {Twitter}},
	shorttitle = {Are {You} a {Racist} or {Am} {I} {Seeing} {Things}?},
	url = {https://aclanthology.org/W16-5618},
	doi = {10.18653/v1/W16-5618},
	urldate = {2024-05-28},
	booktitle = {Proceedings of the {First} {Workshop} on {NLP} and {Computational} {Social} {Science}},
	publisher = {Association for Computational Linguistics},
	author = {Talat, Zeerak},
	editor = {Bamman, David and Doğruöz, A. Seza and Eisenstein, Jacob and Hovy, Dirk and Jurgens, David and O'Connor, Brendan and Oh, Alice and Tsur, Oren and Volkova, Svitlana},
	month = nov,
	year = {2016},
	pages = {138--142},
}

@inproceedings{elsherief_latent_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Latent {Hatred}: {A} {Benchmark} for {Understanding} {Implicit} {Hate} {Speech}},
	shorttitle = {Latent {Hatred}},
	url = {https://aclanthology.org/2021.emnlp-main.29},
	doi = {10.18653/v1/2021.emnlp-main.29},
	abstract = {Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. We present systematic analyses of our dataset using contemporary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue.},
	urldate = {2024-05-28},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {ElSherief, Mai and Ziems, Caleb and Muchlinski, David and Anupindi, Vaishnavi and Seybolt, Jordyn and De Choudhury, Munmun and Yang, Diyi},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {345--363},
}

@article{friedman_bias_1996,
	title = {Bias in computer systems},
	volume = {14},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/230538.230561},
	doi = {10.1145/230538.230561},
	abstract = {From an analysis of actual cases, three categories of bias in computer systems have been developed: preexisting, technical, and emergent. Preexisting bias has its roots in social institutions, practices, and attitudes. Technical bias arises from technical constraints of considerations. Emergent bias arises in a context of use. Although others have pointed to bias inparticular computer systems and have noted the general problem, we know of no comparable work that examines this phenomenon comprehensively and which offers a framework for understanding and remedying it. We conclude by suggesting that freedom from bias should by counted amoung the select set of criteria—including reliability, accuracy, and efficiency—according to which the quality of systems in use in society should be judged.},
	number = {3},
	urldate = {2020-12-07},
	journal = {ACM Transactions on Information Systems},
	author = {Friedman, Batya and Nissenbaum, Helen},
	month = jul,
	year = {1996},
	keywords = {bias, computer ethics, computers and society, design methods, ethics, human values, social computing, social impact, standards, system design, universal design, values},
	pages = {330--347},
}

@inproceedings{suresh_framework_2021,
	address = {New York, NY, USA},
	series = {{EAAMO} '21},
	title = {A {Framework} for {Understanding} {Sources} of {Harm} throughout the {Machine} {Learning} {Life} {Cycle}},
	isbn = {978-1-4503-8553-4},
	url = {https://doi.org/10.1145/3465416.3483305},
	doi = {10.1145/3465416.3483305},
	abstract = {As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
	urldate = {2022-05-03},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {Association for Computing Machinery},
	author = {Suresh, Harini and Guttag, John},
	month = oct,
	year = {2021},
	keywords = {AI ethics, algorithmic bias, allocative harm, fairness in machine learning, representational harm, societal implications of machine learning},
	pages = {1--9},
}

@misc{thylstrup_detecting_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Detecting ‘{Dirt}’ and ‘{Toxicity}’: {Rethinking} {Content} {Moderation} as {Pollution} {Behaviour}},
	shorttitle = {Detecting ‘{Dirt}’ and ‘{Toxicity}’},
	url = {https://papers.ssrn.com/abstract=3709719},
	doi = {10.2139/ssrn.3709719},
	abstract = {"Dirt" and "toxicity" have become established ways of understanding and describing harmful content online. While the concepts have becoming naturalised as a way of expressing harm, its uses are also mobilised to very different ends. We wish to use this article to explore the role the notions of "toxic" and "dirt" have come to play in online content moderation, and what the politics of this discourse implies. Our overarching aim is to ask how content moderation infrastructures define toxic and dirty content and what the politics of these definitions are. We argue that content moderation’s historical reliance on binary categories – and those categories’ ongoing entanglements with social systems of racism and patriarchy – embeds the infrastructures in structures that risk reproducing inequalities.},
	language = {en},
	urldate = {2024-05-17},
	author = {Thylstrup, Nanna and Talat, Zeerak},
	month = oct,
	year = {2020},
	keywords = {Mary Douglas, NLP, Stuart Hall, content moderation, datafication, dirt, pollution behavior, racism, toxicity},
}

@inproceedings{rottger_hatecheck_2021,
	address = {Online},
	title = {{HateCheck}: {Functional} {Tests} for {Hate} {Speech} {Detection} {Models}},
	shorttitle = {{HateCheck}},
	url = {https://aclanthology.org/2021.acl-long.4},
	doi = {10.18653/v1/2021.acl-long.4},
	abstract = {Detecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted diagnostic insights, we introduce HateCheck, a suite of functional tests for hate speech detection models. We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders. We craft test cases for each functionality and validate their quality through a structured annotation process. To illustrate HateCheck's utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.},
	urldate = {2024-01-29},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Röttger, Paul and Vidgen, Bertie and Nguyen, Dong and Talat, Zeerak and Margetts, Helen and Pierrehumbert, Janet},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {41--58},
}

@inproceedings{gadiraju_i_2023,
	address = {New York, NY, USA},
	series = {{FAccT} '23},
	title = {"{I} wouldn’t say offensive but...": {Disability}-{Centered} {Perspectives} on {Large} {Language} {Models}},
	isbn = {9798400701924},
	shorttitle = {"{I} wouldn’t say offensive but..."},
	url = {https://dl.acm.org/doi/10.1145/3593013.3593989},
	doi = {10.1145/3593013.3593989},
	abstract = {Large language models (LLMs) trained on real-world data can inadvertently reflect harmful societal biases, particularly toward historically marginalized communities. While previous work has primarily focused on harms related to age and race, emerging research has shown that biases toward disabled communities exist. This study extends prior work exploring the existence of harms by identifying categories of LLM-perpetuated harms toward the disability community. We conducted 19 focus groups, during which 56 participants with disabilities probed a dialog model about disability and discussed and annotated its responses. Participants rarely characterized model outputs as blatantly offensive or toxic. Instead, participants used nuanced language to detail how the dialog model mirrored subtle yet harmful stereotypes they encountered in their lives and dominant media, e.g., inspiration porn and able-bodied saviors. Participants often implicated training data as a cause for these stereotypes and recommended training the model on diverse identities from disability-positive resources. Our discussion further explores representative data strategies to mitigate harm related to different communities through annotation co-design with ML researchers and developers.},
	urldate = {2024-01-22},
	booktitle = {Proceedings of the 2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Gadiraju, Vinitha and Kane, Shaun and Dev, Sunipa and Taylor, Alex and Wang, Ding and Denton, Emily and Brewer, Robin},
	month = jun,
	year = {2023},
	keywords = {algorithmic harms, artificial intelligence, chatbot, data annotation, dialog model, disability representation, large language models, qualitative},
	pages = {205--216},
}

@article{somepalli_diffusion_2022,
	title = {Diffusion {Art} or {Digital} {Forgery}? {Investigating} {Data} {Replication} in {Diffusion} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Diffusion {Art} or {Digital} {Forgery}?},
	url = {https://arxiv.org/abs/2212.03860},
	doi = {10.48550/ARXIV.2212.03860},
	abstract = {Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.},
	urldate = {2023-02-08},
	author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Computers and Society (cs.CY), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{heikkila_artists_2022,
	title = {Artists can now opt out of the next version of {Stable} {Diffusion}},
	url = {https://www.technologyreview.com/2022/12/16/1065247/artists-can-now-opt-out-of-the-next-version-of-stable-diffusion/},
	abstract = {The move follows a heated public debate between artists and tech companies over how text-to-image AI models are trained.},
	language = {en},
	urldate = {2024-05-17},
	journal = {MIT Technology Review},
	author = {Heikkilä, Melissa},
	month = dec,
	year = {2022},
}

@article{rocher_estimating_2019,
	title = {Estimating the success of re-identifications in incomplete datasets using generative models},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-10933-3},
	doi = {10.1038/s41467-019-10933-3},
	abstract = {While rich medical, behavioral, and socio-demographic data are key to modern data-driven research, their collection and use raise legitimate privacy concerns. Anonymizing datasets through de-identification and sampling before sharing them has been the main tool used to address those concerns. We here propose a generative copula-based method that can accurately estimate the likelihood of a specific person to be correctly re-identified, even in a heavily incomplete dataset. On 210 populations, our method obtains AUC scores for predicting individual uniqueness ranging from 0.84 to 0.97, with low false-discovery rate. Using our model, we find that 99.98\% of Americans would be correctly re-identified in any dataset using 15 demographic attributes. Our results suggest that even heavily sampled anonymized datasets are unlikely to satisfy the modern standards for anonymization set forth by GDPR and seriously challenge the technical and legal adequacy of the de-identification release-and-forget model.},
	language = {en},
	number = {1},
	urldate = {2022-03-25},
	journal = {Nature Communications},
	author = {Rocher, Luc and Hendrickx, Julien M. and de Montjoye, Yves-Alexandre},
	month = jul,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Social sciences},
	pages = {3069},
}

@article{levine_suicide_2022,
	title = {Suicide hotline shares data with for-profit spinoff, raising ethical questions},
	url = {https://www.politico.com/news/2022/01/28/suicide-hotline-silicon-valley-privacy-debates-00002617},
	abstract = {The Crisis Text Line’s AI-driven chat service has gathered troves of data from its conversations with people suffering life’s toughest situations.},
	language = {en},
	urldate = {2022-03-22},
	journal = {POLITICO},
	author = {Levine, Alexandra S.},
	year = {2022},
}

@article{levine_suicide_2022-1,
	title = {Suicide {Hotline} {Left} {Ethics} {Board} {Out} {Of} {The} {Loop} {About} {Data}-{Sharing} {With} {For}-{Profit} {Spinoff}},
	url = {https://www.forbes.com/sites/alexandralevine/2022/02/24/suicide-hotline-left-ethics-board-out-of-the-loop-about-data-sharing-with-for-profit-spinoff/},
	abstract = {Medical experts recruited to advise Crisis Text Line say they weren’t consulted about its data sharing and voiced objection to the arrangement. “This would never have passed a sniff test,” one says.},
	language = {en},
	urldate = {2022-03-23},
	journal = {Forbes},
	author = {Levine, Alexandra S.},
	year = {2022},
	note = {Section: Innovation},
}

@inproceedings{karamolegkou_copyright_2023,
	address = {Singapore},
	title = {Copyright {Violations} and {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.emnlp-main.458},
	doi = {10.18653/v1/2023.emnlp-main.458},
	abstract = {Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than verbatim reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text. We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations. Code is at https://github.com/coastalcph/CopyrightLLMs.},
	urldate = {2024-05-17},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Karamolegkou, Antonia and Li, Jiaang and Zhou, Li and Søgaard, Anders},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {7403--7412},
}

@article{lund_chatgpt_2023,
	title = {{ChatGPT} and a new academic reality: {Artificial} {Intelligence}‐written research papers and the ethics of the large language models in scholarly publishing},
	volume = {74},
	issn = {2330-1635, 2330-1643},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.24750},
	doi = {10.1002/asi.24750},
	abstract = {Abstract
            This article discusses OpenAI's ChatGPT, a generative pre‐trained transformer, which uses natural language processing to fulfill text‐based user requests (i.e., a “chatbot”). The history and principles behind ChatGPT and similar models are discussed. This technology is then discussed in relation to its potential impact on academia and scholarly research and publishing. ChatGPT is seen as a potential model for the automated preparation of essays and other types of scholarly manuscripts. Potential ethical issues that could arise with the emergence of large language models like GPT‐3, the underlying technology behind ChatGPT, and its usage by academics and researchers, are discussed and situated within the context of broader advancements in artificial intelligence, machine learning, and natural language processing for research and scholarly publishing.},
	language = {en},
	number = {5},
	urldate = {2024-04-12},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Lund, Brady D. and Wang, Ting and Mannuru, Nishith Reddy and Nie, Bing and Shimray, Somipam and Wang, Ziang},
	month = may,
	year = {2023},
	pages = {570--581},
}

@inproceedings{nathan_value_2007,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '07},
	title = {Value scenarios: a technique for envisioning systemic effects of new technologies},
	isbn = {978-1-59593-642-4},
	shorttitle = {Value scenarios},
	url = {https://doi.org/10.1145/1240866.1241046},
	doi = {10.1145/1240866.1241046},
	abstract = {In this paper we argue that there is a scarcity of methods which support critical, systemic, long-term thinking in current design practice, technology development and deployment. To address this need we introduce value scenarios, an extension of scenario-based design which can support envisioning the systemic effects of new technologies. We identify and describe five key elements of value scenarios; stakeholders, pervasiveness, time, systemic effects, and value implications. We provide two examples of value scenarios, which draw from our current work on urban simulation and human-robotic interaction . We conclude with suggestions for how value scenarios might be used by others.},
	urldate = {2024-05-02},
	booktitle = {{CHI} '07 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Nathan, Lisa P. and Klasnja, Predrag V. and Friedman, Batya},
	month = apr,
	year = {2007},
	keywords = {design method, design noir, scenario-based design, value scenarios, value sensitive design},
	pages = {2585--2590},
}

@article{langer_what_2021,
	title = {What do we want from {Explainable} {Artificial} {Intelligence} ({XAI})? – {A} stakeholder perspective on {XAI} and a conceptual model guiding interdisciplinary {XAI} research},
	volume = {296},
	issn = {0004-3702},
	shorttitle = {What do we want from {Explainable} {Artificial} {Intelligence} ({XAI})?},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221000242},
	doi = {10.1016/j.artint.2021.103473},
	abstract = {Previous research in Explainable Artificial Intelligence (XAI) suggests that a main aim of explainability approaches is to satisfy specific interests, goals, expectations, needs, and demands regarding artificial systems (we call these “stakeholders' desiderata”) in a variety of contexts. However, the literature on XAI is vast, spreads out across multiple largely disconnected disciplines, and it often remains unclear how explainability approaches are supposed to achieve the goal of satisfying stakeholders' desiderata. This paper discusses the main classes of stakeholders calling for explainability of artificial systems and reviews their desiderata. We provide a model that explicitly spells out the main concepts and relations necessary to consider and investigate when evaluating, adjusting, choosing, and developing explainability approaches that aim to satisfy stakeholders' desiderata. This model can serve researchers from the variety of different disciplines involved in XAI as a common ground. It emphasizes where there is interdisciplinary potential in the evaluation and the development of explainability approaches.},
	urldate = {2024-05-02},
	journal = {Artificial Intelligence},
	author = {Langer, Markus and Oster, Daniel and Speith, Timo and Hermanns, Holger and Kästner, Lena and Schmidt, Eva and Sesing, Andreas and Baum, Kevin},
	month = jul,
	year = {2021},
	keywords = {Explainability, Explainable Artificial Intelligence, Explanations, Human-Computer Interaction, Interdisciplinary Research, Interpretability, Understanding},
	pages = {103473},
}

@techreport{west_discriminating_2019,
	title = {Discriminating {Systems}: {Gender}, {Race} and {Power} in {AI}.},
	url = {Retrieved from https://ainowinstitute.org/ discriminatingsystems.html.},
	language = {en},
	institution = {AI Now Institute},
	author = {West, Sarah Myers and Whittaker, Meredith and Crawford, Kate},
	month = apr,
	year = {2019},
}

@article{feng_has_2022,
	title = {Has {CEO} {Gender} {Bias} {Really} {Been} {Fixed}? {Adversarial} {Attacking} and {Improving} {Gender} {Fairness} in {Image} {Search}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Has {CEO} {Gender} {Bias} {Really} {Been} {Fixed}?},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21445},
	doi = {10.1609/aaai.v36i11.21445},
	abstract = {Gender bias is one of the most common and well-studied demographic biases in information retrieval, and in general in AI systems. After discovering and reporting that gender bias for certain professions could change searchers’ worldviews, mainstreaming image search engines, such as Google, quickly took action to correct and fix such a bias. However, given the nature of these systems, viz., being opaque, it is unclear if they addressed unequal gender representation and gender stereotypes in image search results systematically and in a sustainable way. In this paper, we propose adversarial attack queries composed of professions and countries (e.g., ‘CEO United States’) to investigate whether gender bias is thoroughly mitigated by image search engines. Our experiments on Google, Baidu, Naver, and Yandex Image Search show that the proposed attack can trigger high levels of gender bias in image search results very effectively. To defend against such attacks and mitigate gender bias, we design and implement three novel re-ranking algorithms – epsilon-greedy algorithm, relevance-aware swapping algorithm, and fairness-greedy algorithm, to re-rank returned images for given image queries. Experiments on both simulated (three typical gender distributions) and real-world datasets demonstrate the proposed algorithms can mitigate gender bias effectively.},
	language = {en},
	number = {11},
	urldate = {2022-11-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Feng, Yunhe and Shah, Chirag},
	month = jun,
	year = {2022},
	pages = {11882--11890},
}

@article{rauh_characteristics_2022,
	title = {Characteristics of {Harmful} {Text}: {Towards} {Rigorous} {Benchmarking} of {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Characteristics of {Harmful} {Text}},
	url = {https://arxiv.org/abs/2206.08325},
	doi = {10.48550/ARXIV.2206.08325},
	abstract = {Large language models produce human-like text that drive a growing number of applications. However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful. Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward. To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks. We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks. Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.},
	urldate = {2024-04-12},
	author = {Rauh, Maribeth and Mellor, John and Uesato, Jonathan and Huang, Po-Sen and Welbl, Johannes and Weidinger, Laura and Dathathri, Sumanth and Glaese, Amelia and Irving, Geoffrey and Gabriel, Iason and Isaac, William and Hendricks, Lisa Anne},
	year = {2022},
	note = {Publisher: [object Object]
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences},
}

@inproceedings{zhao_ethical-advice_2021,
	address = {Online},
	title = {Ethical-{Advice} {Taker}: {Do} {Language} {Models} {Understand} {Natural} {Language} {Interventions}?},
	shorttitle = {Ethical-{Advice} {Taker}},
	url = {https://aclanthology.org/2021.findings-acl.364},
	doi = {10.18653/v1/2021.findings-acl.364},
	abstract = {Is it possible to use natural language to intervene in a model's behavior and alter its prediction in a desired way? We investigate the effectiveness of natural language interventions for reading-comprehension systems, studying this in the context of social stereotypes. Specifically, we propose a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a question-answering (QA) model's unethical behavior by communicating context-specific principles of ethics and equity to it. To this end, we build upon recent methods for quantifying a system's social stereotypes, augmenting them with different kinds of ethical interventions and the desired model behavior under such interventions. Our zero-shot evaluation finds that even today's powerful neural language models are extremely poor ethical-advice takers, that is, they respond surprisingly little to ethical interventions even though these interventions are stated as simple sentences. Few-shot learning improves model behavior but remains far from the desired outcome, especially when evaluated for various types of generalization. Our new task thus poses a novel language understanding challenge for the community.},
	language = {en},
	urldate = {2024-04-12},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jieyu and Khashabi, Daniel and Khot, Tushar and Sabharwal, Ashish and Chang, Kai-Wei},
	year = {2021},
	pages = {4158--4164},
}

@inproceedings{birhane_large_2021,
	address = {Los Alamitos, CA, USA},
	title = {Large image datasets: {A} pyrrhic win for computer vision?},
	url = {https://doi.ieeecomputersociety.org/10.1109/WACV48630.2021.00158},
	doi = {10.1109/WACV48630.2021.00158},
	abstract = {In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation.},
	booktitle = {2021 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE Computer Society},
	author = {Birhane, A. and Prabhu, V.},
	month = jan,
	year = {2021},
	keywords = {computer vision, conferences, faces, ieee constitution},
	pages = {1536--1546},
}

@misc{zhuo_red_2023,
	title = {Red teaming {ChatGPT} via {Jailbreaking}: {Bias}, {Robustness}, {Reliability} and {Toxicity}},
	shorttitle = {Red teaming {ChatGPT} via {Jailbreaking}},
	url = {http://arxiv.org/abs/2301.12867},
	doi = {10.48550/arXiv.2301.12867},
	abstract = {Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language models (LLMs) have significantly impacted businesses such as report summarization software and copywriters. Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs, there is little systematic examination and user study of the risks and harmful behaviors of current LLM usage. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method called ``red teaming'' on OpenAI's ChatGPT{\textbackslash}footnote\{In this paper, ChatGPT refers to the version released on Dec 15th.\} to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives: 1) {\textbackslash}textit\{Bias\} 2) {\textbackslash}textit\{Reliability\} 3) {\textbackslash}textit\{Robustness\} 4) {\textbackslash}textit\{Toxicity\}. In accordance with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample datasets. We find that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. In addition, we examine the implications of our findings on AI ethics and harmal behaviors of ChatGPT, as well as future problems and practical design considerations for responsible LLMs. We believe that our findings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in LLM applications.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
	month = may,
	year = {2023},
	note = {arXiv:2301.12867 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@article{cheng_marked_2023,
	title = {Marked {Personas}: {Using} {Natural} {Language} {Prompts} to {Measure} {Stereotypes} in {Language} {Models}},
	volume = {abs/2305.18189},
	url = {https://api.semanticscholar.org/CorpusID:258960243},
	journal = {ArXiv},
	author = {Cheng, Myra and Durmus, Esin and Jurafsky, Dan},
	year = {2023},
}

@article{ganguli_red_2022,
	title = {Red {Teaming} {Language} {Models} to {Reduce} {Harms}: {Methods}, {Scaling} {Behaviors}, and {Lessons} {Learned}},
	volume = {abs/2209.07858},
	url = {https://api.semanticscholar.org/CorpusID:252355458},
	journal = {ArXiv},
	author = {Ganguli, Deep and Lovitt, Liane and Kernion, John and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Benjamin and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Elhage, Nelson and El-Showk, Sheer and Fort, Stanislav and Dodds, Zachary and Henighan, Tom and Hernandez, Danny and Hume, Tristan and Jacobson, Josh and Johnston, Scott and Kravec, Shauna and Olsson, Catherine and Ringer, Sam and Tran-Johnson, Eli and Amodei, Dario and Brown, Tom B. and Joseph, Nicholas and McCandlish, Sam and Olah, Christopher and Kaplan, Jared and Clark, Jack},
	year = {2022},
}

@inproceedings{cao_theory-grounded_2022,
	title = {Theory-{Grounded} {Measurement} of {U}.{S}. {Social} {Stereotypes} in {English} {Language} {Models}},
	url = {https://api.semanticscholar.org/CorpusID:249319807},
	booktitle = {North {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Cao, Yang Trista and Sotnikova, Anna and Daum'e, Hal and Rudinger, Rachel and Zou, Linda X.},
	year = {2022},
}

@inproceedings{kirchenbauer_watermark_2023,
	title = {A {Watermark} for {Large} {Language} {Models}},
	url = {https://api.semanticscholar.org/CorpusID:256194179},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
	year = {2023},
}

@inproceedings{kumar_language_2022,
	title = {Language {Generation} {Models} {Can} {Cause} {Harm}: {So} {What} {Can} {We} {Do} {About} {It}? {An} {Actionable} {Survey}},
	url = {https://api.semanticscholar.org/CorpusID:252907607},
	booktitle = {Conference of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Kumar, Sachin and Balachandran, Vidhisha and Njoo, Lucille and Anastasopoulos, Antonios and Tsvetkov, Yulia},
	year = {2022},
}

@inproceedings{ramesh_comparative_2023,
	title = {A {Comparative} {Study} on the {Impact} of {Model} {Compression} {Techniques} on {Fairness} in {Language} {Models}},
	url = {https://api.semanticscholar.org/CorpusID:259370686},
	booktitle = {Annual {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Ramesh, Krithika and Chavan, Arnav and Pandit, Shrey and Sitaram, Sunayana},
	year = {2023},
}

@article{pan_privacy_2020,
	title = {Privacy {Risks} of {General}-{Purpose} {Language} {Models}},
	url = {https://api.semanticscholar.org/CorpusID:220938739},
	journal = {2020 IEEE Symposium on Security and Privacy (SP)},
	author = {Pan, Xudong and Zhang, Mi and Ji, Shouling and Yang, Min},
	year = {2020},
	pages = {1314--1331},
}

@article{lukas_analyzing_2023,
	title = {Analyzing {Leakage} of {Personally} {Identifiable} {Information} in {Language} {Models}},
	url = {https://api.semanticscholar.org/CorpusID:256459554},
	journal = {2023 IEEE Symposium on Security and Privacy (SP)},
	author = {Lukas, Nils and Salem, A. and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-B'eguelin, Santiago},
	year = {2023},
	pages = {346--363},
}

@article{ganguli_capacity_2023,
	title = {The {Capacity} for {Moral} {Self}-{Correction} in {Large} {Language} {Models}},
	volume = {abs/2302.07459},
	url = {https://api.semanticscholar.org/CorpusID:256868727},
	journal = {ArXiv},
	author = {Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and Liao, Thomas and Lukovsiut.e, Kamil e and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and Hernandez, Danny and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kernion, John and Kerr, Jamie and Mueller, Jared and Landau, Joshua D. and Ndousse, Kamal and Nguyen, Karina and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Mercado, Noem'i and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Kundu, Sandipan and Kadavath, Saurav and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Mann, Benjamin and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom B. and Olah, Christopher and Clark, Jack and Bowman, Sam and Kaplan, Jared},
	year = {2023},
}

@inproceedings{chiang_can_2023,
	title = {Can {Large} {Language} {Models} {Be} an {Alternative} to {Human} {Evaluations}?},
	url = {https://api.semanticscholar.org/CorpusID:258461287},
	booktitle = {Annual {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Chiang, Cheng-Han and Lee, Hung-yi},
	year = {2023},
}

@article{liu_trustworthy_2023,
	title = {Trustworthy {LLMs}: a {Survey} and {Guideline} for {Evaluating} {Large} {Language} {Models}' {Alignment}},
	volume = {abs/2308.05374},
	url = {https://api.semanticscholar.org/CorpusID:260775522},
	journal = {ArXiv},
	author = {Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Guo, Ruocheng and Cheng, Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hanguang},
	year = {2023},
}

@article{kirk_personalisation_2023,
	title = {Personalisation within bounds: {A} risk taxonomy and policy framework for the alignment of large language models with personalised feedback},
	volume = {abs/2303.05453},
	url = {https://api.semanticscholar.org/CorpusID:257427629},
	journal = {ArXiv},
	author = {Kirk, Hannah Rose and Vidgen, Bertie and Röttger, Paul and Hale, Scott A.},
	year = {2023},
}

@article{hendrycks_aligning_2020,
	title = {Aligning {AI} {With} {Shared} {Human} {Values}},
	volume = {abs/2008.02275},
	url = {https://api.semanticscholar.org/CorpusID:220968818},
	journal = {ArXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry Zheng and Song, Dawn Xiaodong and Steinhardt, Jacob},
	year = {2020},
}

@inproceedings{choudhury_how_2021,
	title = {How {Linguistically} {Fair} {Are} {Multilingual} {Pre}-{Trained} {Language} {Models}?},
	url = {https://api.semanticscholar.org/CorpusID:235349039},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Choudhury, Monojit and Deshpande, Amit},
	year = {2021},
}

@article{sun_bertscore_2022,
	title = {{BERTScore} is {Unfair}: {On} {Social} {Bias} in {Language} {Model}-{Based} {Metrics} for {Text} {Generation}},
	volume = {abs/2210.07626},
	url = {https://api.semanticscholar.org/CorpusID:252907549},
	journal = {ArXiv},
	author = {Sun, Tianxiang and He, Junliang and Qiu, Xipeng and Huang, Xuanjing},
	year = {2022},
}

@article{huang_trustgpt_2023,
	title = {{TrustGPT}: {A} {Benchmark} for {Trustworthy} and {Responsible} {Large} {Language} {Models}},
	volume = {abs/2306.11507},
	url = {https://api.semanticscholar.org/CorpusID:259202452},
	journal = {ArXiv},
	author = {Huang, Yue and Zhang, Qihui and Yu, Philip S. and Sun, Lichao},
	year = {2023},
}

@article{petrov_language_2023,
	title = {Language {Model} {Tokenizers} {Introduce} {Unfairness} {Between} {Languages}},
	volume = {abs/2305.15425},
	url = {https://api.semanticscholar.org/CorpusID:258888141},
	journal = {ArXiv},
	author = {Petrov, Aleksandar and Malfa, Emanuele La and Torr, Philip H. S. and Bibi, Adel},
	year = {2023},
}

@article{aher_using_2022,
	title = {Using {Large} {Language} {Models} to {Simulate} {Multiple} {Humans}},
	volume = {abs/2208.10264},
	url = {https://api.semanticscholar.org/CorpusID:260881158},
	journal = {ArXiv},
	author = {Aher, Gati and Arriaga, RosaI and Kalai, Adam Tauman},
	year = {2022},
}

@article{qi_fine-tuning_2023,
	title = {Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!},
	volume = {abs/2310.03693},
	url = {https://api.semanticscholar.org/CorpusID:263671523},
	journal = {ArXiv},
	author = {Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
	year = {2023},
}

@article{bucinca_aha_2023,
	title = {{AHA}!: {Facilitating} {AI} {Impact} {Assessment} by {Generating} {Examples} of {Harms}},
	volume = {abs/2306.03280},
	url = {https://api.semanticscholar.org/CorpusID:259088554},
	journal = {ArXiv},
	author = {Buçinca, Zana and Pham, Chau Minh and Jakesch, Maurice and Ribeiro, Marco Tulio and Olteanu, Alexandra and Amershi, Saleema},
	year = {2023},
}

@article{zhang_is_2023,
	title = {Is {ChatGPT} {Fair} for {Recommendation}? {Evaluating} {Fairness} in {Large} {Language} {Model} {Recommendation}},
	url = {https://api.semanticscholar.org/CorpusID:258676079},
	journal = {Proceedings of the 17th ACM Conference on Recommender Systems},
	author = {Zhang, Jizhi and Bao, Keqin and Zhang, Yang and Wang, Wenjie and Feng, Fuli and He, Xiangnan},
	year = {2023},
}

@article{liang_holistic_2023,
	title = {Holistic {Evaluation} of {Language} {Models}},
	volume = {1525},
	url = {https://api.semanticscholar.org/CorpusID:253553585},
	journal = {Annals of the New York Academy of Sciences},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and R'e, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, E. and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel J. and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan S. and Guha, Neel and Chatterji, Niladri S. and Khattab, O. and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas F. and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	year = {2023},
	pages = {140 -- 146},
}

@inproceedings{baldini_your_2021,
	title = {Your fairness may vary: {Pretrained} language model fairness in toxic text classification},
	url = {https://api.semanticscholar.org/CorpusID:248177981},
	booktitle = {Findings},
	author = {Baldini, Ioana and Wei, Dennis and Ramamurthy, Karthikeyan Natesan and Yurochkin, Mikhail and Singh, Moninder},
	year = {2021},
}

@article{sun_trustllm_2024,
	title = {{TrustLLM}: {Trustworthiness} in {Large} {Language} {Models}},
	volume = {abs/2401.05561},
	url = {https://api.semanticscholar.org/CorpusID:266933236},
	journal = {ArXiv},
	author = {Sun, Lichao and Huang, Yue and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and Liu, Zheng and Liu, Yixin and Wang, Yijue and Zhang, Zhikun and Kailkhura, Bhavya and Xiong, Caiming and Xiao, Chaowei and Li, Chun-Yan and Xing, Eric P. and Huang, Furong and Liu, Haodong and Ji, Heng and Wang, Hongyi and Zhang, Huan and Yao, Huaxiu and Kellis, Manolis and Zitnik, Marinka and Jiang, Meng and Bansal, Mohit and Zou, James and Pei, Jian and Liu, Jian and Gao, Jianfeng and Han, Jiawei and Zhao, Jieyu and Tang, Jiliang and Wang, Jindong and Mitchell, John and Shu, Kai and Xu, Kaidi and Chang, Kai-Wei and He, Lifang and Huang, Lifu and Backes, Michael and Gong, Neil Zhenqiang and Yu, Philip S. and Chen, Pin-Yu and Gu, Quanquan and Xu, Ran and Ying, Rex and Ji, Shuiwang and Jana, Suman Sekhar and Chen, Tian-Xiang and Liu, Tianming and Zhou, Tianying and Wang, William and Li, Xiang and Zhang, Xiang-Yu and Wang, Xiao and Xie, Xingyao and Chen, Xun and Wang, Xuyu and Liu, Yan and Ye, Yanfang and Cao, Yinzhi and Zhao, Yue},
	year = {2024},
}

@article{lee_language_2022,
	title = {Do {Language} {Models} {Plagiarize}?},
	url = {https://api.semanticscholar.org/CorpusID:247450984},
	journal = {Proceedings of the ACM Web Conference 2023},
	author = {Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
	year = {2022},
}

@inproceedings{levy_safetext_2022,
	title = {{SafeText}: {A} {Benchmark} for {Exploring} {Physical} {Safety} in {Language} {Models}},
	url = {https://api.semanticscholar.org/CorpusID:252968000},
	booktitle = {Conference on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Levy, Sharon and Allaway, Emily and Subbiah, Melanie and Chilton, Lydia B. and Patton, Desmond Upton and McKeown, Kathleen and Wang, William Yang},
	year = {2022},
}

@article{chang_survey_2023,
	title = {A {Survey} on {Evaluation} of {Large} {Language} {Models}},
	volume = {abs/2307.03109},
	url = {https://api.semanticscholar.org/CorpusID:259360395},
	journal = {ArXiv},
	author = {Chang, Yu-Chu and Wang, Xu and Wang, Jindong and Wu, Yuanyi and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Weirong and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qian and Xie, Xingxu},
	year = {2023},
}

@article{rae_scaling_2021,
	title = {Scaling {Language} {Models}: {Methods}, {Analysis} \& {Insights} from {Training} {Gopher}},
	volume = {abs/2112.11446},
	url = {https://api.semanticscholar.org/CorpusID:245353475},
	journal = {ArXiv},
	author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John F. J. and Higgins, Irina and Creswell, Antonia and McAleese, Nathan and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant M. and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, L. and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, N. K. and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Tobias and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew G. and Hechtman, Blake A. and Weidinger, Laura and Gabriel, Iason and Isaac, William S. and Lockhart, Edward and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem W. and Stanway, Jeff and Bennett, L. L. and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
	year = {2021},
}

@article{rastogi_supporting_2023,
	title = {Supporting {Human}-{AI} {Collaboration} in {Auditing} {LLMs} with {LLMs}},
	url = {https://api.semanticscholar.org/CorpusID:258236085},
	journal = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Rastogi, Charvi and Ribeiro, Marco Tulio and King, Nicholas and Amershi, Saleema},
	year = {2023},
}

@article{huang_survey_2023,
	title = {A {Survey} of {Safety} and {Trustworthiness} of {Large} {Language} {Models} through the {Lens} of {Verification} and {Validation}},
	volume = {abs/2305.11391},
	url = {https://api.semanticscholar.org/CorpusID:258823083},
	journal = {ArXiv},
	author = {Huang, Xiaowei and Ruan, Wenjie and Huang, Wei and Jin, Gao and Dong, Yizhen and Wu, Changshun and Bensalem, Saddek and Mu, Ronghui and Qi, Yi and Zhao, Xingyu and Cai, Kaiwen and Zhang, Yanghao and Wu, Sihao and Xu, Peipei and Wu, Dengyu and Freitas, André and Mustafa, Mustafa A.},
	year = {2023},
}

@article{mokander_auditing_2023,
	title = {Auditing large language models: a three-layered approach},
	volume = {abs/2302.08500},
	url = {https://api.semanticscholar.org/CorpusID:256901111},
	journal = {ArXiv},
	author = {Mokander, Jakob and Schuett, Jonas and Kirk, Hannah Rose and Floridi, Luciano},
	year = {2023},
}

@article{grinbaum_ethical_2022,
	title = {The {Ethical} {Need} for {Watermarks} in {Machine}-{Generated} {Language}},
	volume = {abs/2209.03118},
	url = {https://api.semanticscholar.org/CorpusID:252110611},
	journal = {ArXiv},
	author = {Grinbaum, Alexei and Adomaitis, Laurynas},
	year = {2022},
}

@article{xu_can_2022,
	title = {Can {Model} {Compression} {Improve} {NLP} {Fairness}},
	volume = {abs/2201.08542},
	url = {https://api.semanticscholar.org/CorpusID:246210322},
	journal = {ArXiv},
	author = {Xu, Guangxuan and Hu, Qingyuan},
	year = {2022},
}

@inproceedings{ramesh_fairness_2023,
	title = {Fairness in {Language} {Models} {Beyond} {English}: {Gaps} and {Challenges}},
	url = {https://api.semanticscholar.org/CorpusID:257206042},
	booktitle = {Findings},
	author = {Ramesh, Krithika and Sitaram, Sunayana and Choudhury, Monojit},
	year = {2023},
}

@article{spirling_why_2023,
	title = {Why open-source generative {AI} models are an ethical way forward for science},
	volume = {616},
	url = {https://api.semanticscholar.org/CorpusID:258183146},
	journal = {Nature},
	author = {Spirling, Arthur},
	year = {2023},
	pages = {413},
}

@article{sun_safety_2023,
	title = {Safety {Assessment} of {Chinese} {Large} {Language} {Models}},
	volume = {abs/2304.10436},
	url = {https://api.semanticscholar.org/CorpusID:258236069},
	journal = {ArXiv},
	author = {Sun, Hao and Zhang, Zhexin and Deng, Jiawen and Cheng, Jiale and Huang, Minlie},
	year = {2023},
}

@article{chan_gpt-3_2022,
	title = {{GPT}-3 and {InstructGPT}: technological dystopianism, utopianism, and “{Contextual}” perspectives in {AI} ethics and industry},
	volume = {3},
	url = {https://api.semanticscholar.org/CorpusID:247961196},
	journal = {AI and Ethics},
	author = {Chan, Anastasia},
	year = {2022},
	pages = {53--64},
}

@inproceedings{guo_auto-debias_2022,
	title = {Auto-{Debias}: {Debiasing} {Masked} {Language} {Models} with {Automated} {Biased} {Prompts}},
	url = {https://api.semanticscholar.org/CorpusID:248780440},
	booktitle = {Annual {Meeting} of the {Association} for {Computational} {Linguistics}},
	author = {Guo, Yue and Yang, Yi and Abbasi, A.},
	year = {2022},
}

@inproceedings{perez_red_2022,
	title = {Red {Teaming} {Language} {Models} with {Language} {Models}},
	url = {https://api.semanticscholar.org/CorpusID:246634238},
	booktitle = {Conference on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nathan and Irving, Geoffrey},
	year = {2022},
}

@article{head_large_2023,
	title = {Large language model applications for evaluation: {Opportunities} and ethical implications},
	volume = {2023},
	issn = {1097-6736, 1534-875X},
	shorttitle = {Large language model applications for evaluation},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/ev.20556},
	doi = {10.1002/ev.20556},
	abstract = {Abstract
            Large language models (LLMs) are a type of generative artificial intelligence (AI) designed to produce text‐based content. LLMs use deep learning techniques and massively large data sets to understand, summarize, generate, and predict new text. LLMs caught the public eye in early 2023 when ChatGPT (the first consumer facing LLM) was released. LLM technologies are driven by recent advances in deep‐learning AI techniques, where language models are trained on extremely large text data from the internet and then re‐used for downstream tasks with limited fine‐tuning required. They offer exciting opportunities for evaluators to automate and accelerate time‐consuming tasks involving text analytics and text generation. We estimate that over two‐thirds of evaluation tasks will be affected by LLMs in the next 5 years. Use‐case examples include summarizing text data, extracting key information from text, analyzing and classifying text content, writing text, and translation. Despite the advances, the technologies pose significant challenges and risks. Because LLM technologies are generally trained on text from the internet, they tend to perpetuate biases (racism, sexism, ethnocentrism, and more) and exclusion of non‐majority languages. Current tools like ChatGPT have not been specifically developed for monitoring, evaluation, research, and learning (MERL) purposes, possibly limiting their accuracy and usefulness for evaluation. In addition, technical limitations and challenges with bias can lead to real world harm. To overcome these technical challenges and ethical risks, the evaluation community will need to work collaboratively with the data science community to co‐develop tools and processes and to ensure the application of quality and ethical standards.},
	language = {en},
	number = {178-179},
	urldate = {2024-04-12},
	journal = {New Directions for Evaluation},
	author = {Head, Cari Beth and Jasper, Paul and McConnachie, Matthew and Raftree, Linda and Higdon, Grace},
	month = jun,
	year = {2023},
	pages = {33--46},
}

@article{hebenstreit_collection_2023,
	title = {A collection of principles for guiding and evaluating large language models},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2312.10059},
	doi = {10.48550/ARXIV.2312.10059},
	abstract = {Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.},
	urldate = {2024-04-12},
	author = {Hebenstreit, Konstantin and Praas, Robert and Samwald, Matthias},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences},
}

@article{watkins_guidance_2023,
	title = {Guidance for researchers and peer-reviewers on the ethical use of {Large} {Language} {Models} ({LLMs}) in scientific research workflows},
	issn = {2730-5961},
	url = {https://doi.org/10.1007/s43681-023-00294-5},
	doi = {10.1007/s43681-023-00294-5},
	abstract = {For researchers interested in exploring the exciting applications of Large Language Models (LLMs) in their scientific investigations, there is currently limited guidance and few norms for them to consult. Similarly, those providing peer-reviews on research articles where LLMs were used are without conventions or standards to apply or guidelines to follow. This situation is understandable given the rapid and recent development of LLMs that are capable of valuable contributions to research workflows (such as OpenAI’s ChatGPT). Nevertheless, now is the time to begin the development of norms, conventions, and standards that can be applied by researchers and peer-reviewers. By applying the principles of Artificial Intelligence (AI) ethics, we can better ensure that the use of LLMs in scientific research aligns with ethical principles and best practices. This editorial hopes to inspire further dialogue and research in this crucial area of scientific investigation.},
	language = {en},
	urldate = {2024-04-12},
	journal = {AI and Ethics},
	author = {Watkins, Ryan},
	month = may,
	year = {2023},
	keywords = {Conventions, LLM, Large Language Model, Norms, Research, Science, Standards},
}

@article{chun_informed_2024,
	title = {Informed {AI} {Regulation}: {Comparing} the {Ethical} {Frameworks} of {Leading} {LLM} {Chatbots} {Using} an {Ethics}-{Based} {Audit} to {Assess} {Moral} {Reasoning} and {Normative} {Values}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Informed {AI} {Regulation}},
	url = {https://arxiv.org/abs/2402.01651},
	doi = {10.48550/ARXIV.2402.01651},
	abstract = {With the rise of individual and collaborative networks of autonomous agents, AI is deployed in more key reasoning and decision-making roles. For this reason, ethics-based audits play a pivotal role in the rapidly growing fields of AI safety and regulation. This paper undertakes an ethics-based audit to probe the 8 leading commercial and open-source Large Language Models including GPT-4. We assess explicability and trustworthiness by a) establishing how well different models engage in moral reasoning and b) comparing normative values underlying models as ethical frameworks. We employ an experimental, evidence-based approach that challenges the models with ethical dilemmas in order to probe human-AI alignment. The ethical scenarios are designed to require a decision in which the particulars of the situation may or may not necessitate deviating from normative ethical principles. A sophisticated ethical framework was consistently elicited in one model, GPT-4. Nonetheless, troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies. Code is available at https://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.},
	urldate = {2024-04-12},
	author = {Chun, Jon and Elkins, Katherine},
	year = {2024},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {68T27, 68T30, 68T37, 91F20, 93B52, Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, I.2.7; K.4.1; I.2.11; I.2.0; K.6.5},
}

@article{reid_ethics_2021,
	title = {Ethics in global research: {Creating} a toolkit to support integrity and ethical action throughout the research journey},
	volume = {17},
	issn = {1747-0161, 2047-6094},
	shorttitle = {Ethics in global research},
	url = {http://journals.sagepub.com/doi/10.1177/1747016121997522},
	doi = {10.1177/1747016121997522},
	abstract = {Global challenge-led research seeks to contribute to solution-generation for complex problems. Multicultural, multidisciplinary, and multisectoral teams must be capable of operating in highly demanding contexts. This brings with it a swathe of ethical conflicts that require quick and effective solutions that respect both international conventions and cultural diversity. The objective of this article is to describe the process of creating a toolkit designed to support global researchers in navigating these ethical challenges. The process of creating the toolkit embodied the model of ethical research practice that it advocates. Specifically, at the heart of ethical decision-making is consideration of the following: Place, solutions must be relevant to the context in which they are to be used; People, those impacted by the outcomes must be partners in co-creation; Principles, ethical projects must be guided by clear values; and Precedent, the existing evidence-base should guide the project and, in turn, the project should extend the evidence-base. It is the thesis underlying the toolkit that consideration of these 4Ps provides a strong basis for understanding ethical conflicts and allows for the generation of potential solutions. This toolkit has been designed in two phases of collaborative work. More than 200 researchers participated from more than 30 countries and more than 60 different disciplines. This allowed us to develop a model for contextual, dynamic analysis of ethical conflicts in global research that is complementary to traditional codes of ethics. It emphasizes the need to consider ethical analysis as an iterative, reflective, process relevant at all stages of the research journey, including, ultimately, in evaluating the legacy of a project. The toolkit is presented as an open access website to promote universal access. A downloadable “pocket guide” version is also now available in 11 languages.},
	language = {en},
	number = {3},
	urldate = {2024-04-12},
	journal = {Research Ethics},
	author = {Reid, Corinne and Calia, Clara and Guerra, Cristóbal and Grant, Liz and Anderson, Matilda and Chibwana, Khama and Kawale, Paul and Amos, Action},
	month = jul,
	year = {2021},
	pages = {359--374},
}

@article{guleria_chatgpt_2023,
	title = {{ChatGPT}: ethical concerns and challenges in academics and research},
	volume = {17},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {1972-2680},
	shorttitle = {{ChatGPT}},
	url = {https://www.jidc.org/index.php/journal/article/view/18738},
	doi = {10.3855/jidc.18738},
	abstract = {Introduction: The emergence of artificial intelligence (AI) has presented several opportunities to ease human work. AI applications are available for almost every domain of life. A new technology, Chat Generative Pre-Trained Transformer (ChatGPT), was introduced by OpenAI in November 2022, and has become a topic of discussion across the world. ChatGPT-3 has brought many opportunities, as well as ethical and privacy considerations. ChatGPT is a large language model (LLM) which has been trained on the events that happened until 2021. The use of AI and its assisted technologies in scientific writing is against research and publication ethics. Therefore, policies and guidelines need to be developed over the use of such tools in scientific writing. The main objective of the present study was to highlight the use of AI and AI assisted technologies such as the ChatGPT and other chatbots in the scientific writing and in the research domain resulting in bias, spread of inaccurate information and plagiarism.
Methodology: Experiments were designed to test the accuracy of ChatGPT when used in research and academic writing.
Results: The information provided by ChatGPT was inaccurate and may have far-reaching implications in the field of medical science and engineering. Critical thinking should be encouraged among researchers to raise awareness about the associated privacy and ethical risks. 
Conclusions: Regulations for ethical and privacy concerns related to the use of ChatGPT in academics and research need to be developed.},
	number = {09},
	urldate = {2024-04-12},
	journal = {The Journal of Infection in Developing Countries},
	author = {Guleria, Ankita and Krishan, Kewal and Sharma, Vishal and Kanchan, Tanuj},
	month = sep,
	year = {2023},
	pages = {1292--1299},
}

@article{farina_ethical_2024,
	title = {Ethical considerations and policy interventions concerning the impact of generative {AI} tools in the economy and in society},
	issn = {2730-5961},
	url = {https://doi.org/10.1007/s43681-023-00405-2},
	doi = {10.1007/s43681-023-00405-2},
	abstract = {Artificial intelligence plays an increasingly important role in our daily lives as well as in the development of world economies. In this perspective article, we focus on a recent application of Artificial Intelligence (generative AI models, such as ChatGPT), which are based on machine learning (ML) and are applied to Natural Language Processing (NLP). We look (Sect. 2) at the potential positive impact of these models in the economy. In Sect. 3, we present a series of ethical, sociological, and political reflections that are supposed to highlight the dangers of the uncritical implementation of these tools in society. Specifically, we discuss (Sect. 3.1 and 3.2) two policy interventions aimed at mitigating the potentially nefarious effects of this technology. These interventions are designed to preserve social equality and ensure transparency and pluralism. We wrap up this paper by offering our readers the coordinates of a virtue ethics approach that can serve as a framework for further interventions aimed at pursuing human flourishing; that is, social and moral good.},
	language = {en},
	urldate = {2024-04-12},
	journal = {AI and Ethics},
	author = {Farina, Mirko and Yu, Xiao and Lavazza, Andrea},
	month = jan,
	year = {2024},
	keywords = {Chat GPT, Employment, Equality, Generative AI tools, Pluralism, Transparency},
}

@article{wei_ai_2022,
	title = {{AI} {Ethics} {Issues} in {Real} {World}: {Evidence} from {AI} {Incident} {Database}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{AI} {Ethics} {Issues} in {Real} {World}},
	url = {https://arxiv.org/abs/2206.07635},
	doi = {10.48550/ARXIV.2206.07635},
	abstract = {With the powerful performance of Artificial Intelligence (AI) also comes prevalent ethical issues. Though governments and corporations have curated multiple AI ethics guidelines to curb unethical behavior of AI, the effect has been limited, probably due to the vagueness of the guidelines. In this paper, we take a closer look at how AI ethics issues take place in real world, in order to have a more in-depth and nuanced understanding of different ethical issues as well as their social impact. With a content analysis of AI Incident Database, which is an effort to prevent repeated real world AI failures by cataloging incidents, we identified 13 application areas which often see unethical use of AI, with intelligent service robots, language/vision models and autonomous driving taking the lead. Ethical issues appear in 8 different forms, from inappropriate use and racial discrimination, to physical safety and unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI practitioners with a practical guideline when trying to deploy AI applications ethically.},
	urldate = {2024-04-12},
	author = {Wei, Mengyi and Zhou, Zhixuan},
	year = {2022},
	note = {Publisher: [object Object]
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences},
}

@article{uzun_are_2023,
	title = {Are {Concerns} {Related} to {Artificial} {Intelligence} {Development} and {Use} {Really} {Necessary}: {A} {Philosophical} {Discussion}},
	volume = {2},
	issn = {2731-4669},
	shorttitle = {Are {Concerns} {Related} to {Artificial} {Intelligence} {Development} and {Use} {Really} {Necessary}},
	url = {https://doi.org/10.1007/s44206-023-00070-2},
	doi = {10.1007/s44206-023-00070-2},
	abstract = {This article explores the philosophical considerations, concerns, and recommendations surrounding the development and use of artificial intelligence and large language models like ChatGPT. It addresses the concerns raised by educators and academics regarding academic integrity and the potential negative effects of LLMs. The article discusses the challenges posed by LLMs, such as plagiarism, and the opportunities they present, such as assisting students in the writing process and improving the quality of their work. It examines different philosophical approaches, including utilitarianism, deontological ethics, and virtue ethics, and their implications for the development and use of AI. The article also delves into key concerns related to privacy, bias, discrimination, and the impact on employment. It provides suggestions for a responsible and ethical approach, including prioritizing ethics and transparency in AI development, establishing clear regulations, and fostering responsible use by users. The importance of ongoing philosophical reflection, ethical considerations, and collaboration among stakeholders is emphasized. The article concludes by highlighting the need for future research to address these concerns and ensure that AI is developed and used in a manner consistent with ethical principles, values, and societal well-being.},
	language = {en},
	number = {3},
	urldate = {2024-04-12},
	journal = {Digital Society},
	author = {Uzun, Levent},
	month = sep,
	year = {2023},
	keywords = {Academic integrity, Artificial intelligence, ChatGPT, Ethical implications, Large language models (LLMs), Plagiarism},
	pages = {40},
}

@misc{li_newsbench_2024,
	title = {{NewsBench}: {Systematic} {Evaluation} of {LLMs} for {Writing} {Proficiency} and {Safety} {Adherence} in {Chinese} {Journalistic} {Editorial} {Applications}},
	shorttitle = {{NewsBench}},
	url = {http://arxiv.org/abs/2403.00862},
	abstract = {This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 10 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safety considerations.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Li, Miao and Chen, Ming-Bin and Tang, Bo and Hou, Shengbin and Wang, Pengyu and Deng, Haiying and Li, Zhiyu and Xiong, Feiyu and Mao, Keming and Cheng, Peng and Luo, Yi},
	month = mar,
	year = {2024},
	note = {arXiv:2403.00862 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{lewis_global_2020,
	title = {Global {Challenges} in the {Standardization} of {Ethics} for {Trustworthy} {AI}},
	issn = {2246-0853, 2245-800X},
	url = {https://journals.riverpublishers.com/index.php/JICTS/article/view/2645},
	doi = {10.13052/jicts2245-800X.823},
	abstract = {In this paper, we examine the challenges of developing international standards for Trustworthy AI that aim both to be global applicable and to address the ethical questions key to building trust at a commercial and societal level. We begin by examining the validity of grounding standards that aim for international reach on human right agreements, and the need to accommodate variations in prioritization and tradeoffs in implementing rights in different societal and cultural settings. We then examine the major recent proposals from the OECD, the EU and the IEEE on ethical governance of Trustworthy AI systems in terms of their scope and use of normative language. From this analysis, we propose a preliminary minimal model for the functional roles relevant to Trustworthy AI as a framing for further standards development in this area. We also identify the different types of interoperability reference points that may exist between these functional roles and remark on the potential role they could play in future standardization. Finally we examine a current AI standardization effort under ISO/IEC JTC1 to consider how future Trustworthy AI standards may be able to build on existing standards in developing ethical guidelines and in particular on the ISO standard on Social Responsibility.We conclude by proposing some future directions for research and development of Trustworthy AI standards.},
	urldate = {2024-04-12},
	journal = {Journal of ICT Standardization},
	author = {Lewis, Dave and Hogan, Linda and Filip, David and Wall, P. J.},
	month = apr,
	year = {2020},
}

@article{al-fedaghi_thinging_2018,
	title = {Thinging {Ethics} for {Software} {Engineers}},
	url = {https://www.semanticscholar.org/paper/b9fee63775028f002e7e714821e7cd8eb9d3cee2},
	abstract = {Ethical systems are usually described as principles for distinguishing right from wrong and forming beliefs about proper conduct. Ethical topics are complex, with excessively verbose accounts of mental models and intensely ingrained philosophical assumptions. From practical experience, in teaching ethics for software engineering students, an explanation of ethics alone often cannot provide insights of behavior and thought for students. Additionally, it seems that there has been no exploration into the development of a conceptual presentation of ethics that appeals to computer engineers. This is particularly clear in the area of software engineering, which focuses on software and associated tools such as algorithms, diagramming, documentation, modeling and design as applied to various types of data and conceptual artifacts. It seems that software engineers look at ethical materials as a collection of ideas and notions that lack systemization and uniformity. Accordingly, this paper explores a thinging schematization for ethical theories that can serve a role similar to that of modeling languages (e.g., UML). In this approach, thinging means actualization (existence, presence, being) of things and mechanisms that define a boundary around some region of ethically related reality, separating it from everything else. The resultant diagrammatic representation then developed to model the process of making ethical decisions in that region.},
	urldate = {2024-04-12},
	journal = {ArXiv},
	author = {Al-Fedaghi, S.},
	month = sep,
	year = {2018},
}

@misc{wga_negotiating_comittee_wga_2023,
	title = {{WGA} on {Strike}},
	url = {https://www.wgacontract2023.org/announcements/wga-on-strike},
	abstract = {Our negotiation with the studios and streamers has failed to reach an agreement. We are on strike.},
	language = {en},
	urldate = {2024-02-16},
	journal = {WGA on Strike},
	author = {{WGA Negotiating Comittee}},
	month = jan,
	year = {2023},
}

@misc{voronov_mind_2024,
	title = {Mind {Your} {Format}: {Towards} {Consistent} {Evaluation} of {In}-{Context} {Learning} {Improvements}},
	shorttitle = {Mind {Your} {Format}},
	url = {http://arxiv.org/abs/2401.06766},
	doi = {10.48550/arXiv.2401.06766},
	abstract = {Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples. The prompt template, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning. In this work, we conduct a comprehensive study of the template format's influence on the in-context learning performance. We evaluate the impact of the prompt template across models (from 770M to 70B parameters) and 4 standard classification datasets. We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level. More importantly, the best templates do not transfer between different setups and even between models of the same family. Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works. As a first step towards mitigating this issue, we propose Template Ensembles that aggregate model predictions across several templates. This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Voronov, Anton and Wolf, Lena and Ryabinin, Max},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06766 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{ungless_potential_2023,
	title = {Potential {Pitfalls} with {Automatic} {Sentiment} {Analysis}: {The} {Example} of {Queerphobic} {Bias}},
	issn = {0894-4393},
	shorttitle = {Potential {Pitfalls} with {Automatic} {Sentiment} {Analysis}},
	url = {https://www.research.ed.ac.uk/en/publications/potential-pitfalls-with-automatic-sentiment-analysis-the-example-},
	doi = {10.1177/08944393231152946},
	language = {English},
	urldate = {2023-10-25},
	journal = {Social science computer review},
	author = {Ungless, Eddie and Ross, Björn and Belle, Vaishak},
	month = feb,
	year = {2023},
}

@misc{camara_mapping_2022,
	title = {Mapping the {Multilingual} {Margins}: {Intersectional} {Biases} of {Sentiment} {Analysis} {Systems} in {English}, {Spanish}, and {Arabic}},
	shorttitle = {Mapping the {Multilingual} {Margins}},
	url = {http://arxiv.org/abs/2204.03558},
	doi = {10.48550/arXiv.2204.03558},
	abstract = {As natural language processing systems become more widespread, it is necessary to address fairness issues in their implementation and deployment to ensure that their negative impacts on society are understood and minimized. However, there is limited work that studies fairness using a multilingual and intersectional framework or on downstream tasks. In this paper, we introduce four multilingual Equity Evaluation Corpora, supplementary test sets designed to measure social biases, and a novel statistical framework for studying unisectional and intersectional social biases in natural language processing. We use these tools to measure gender, racial, ethnic, and intersectional social biases across five models trained on emotion regression tasks in English, Spanish, and Arabic. We find that many systems demonstrate statistically significant unisectional and intersectional social biases.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Câmara, António and Taneja, Nina and Azad, Tamjeed and Allaway, Emily and Zemel, Richard},
	month = apr,
	year = {2022},
	note = {arXiv:2204.03558 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{elazar_whats_2024,
	title = {What's {In} {My} {Big} {Data}?},
	url = {http://arxiv.org/abs/2310.20707},
	doi = {10.48550/arXiv.2310.20707},
	abstract = {Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50\% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Elazar, Yanai and Bhagia, Akshita and Magnusson, Ian and Ravichander, Abhilasha and Schwenk, Dustin and Suhr, Alane and Walsh, Pete and Groeneveld, Dirk and Soldaini, Luca and Singh, Sameer and Hajishirzi, Hanna and Smith, Noah A. and Dodge, Jesse},
	month = mar,
	year = {2024},
	note = {arXiv:2310.20707 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{bender_dangers_2021,
	address = {Virtual Event Canada},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? 🦜},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	language = {en},
	urldate = {2021-05-17},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@article{fort_last_2011,
	title = {Last {Words}: {Amazon} {Mechanical} {Turk}: {Gold} {Mine} or {Coal} {Mine}?},
	volume = {37},
	shorttitle = {Last {Words}},
	url = {https://aclanthology.org/J11-2010},
	doi = {10.1162/COLI_a_00057},
	number = {2},
	urldate = {2024-03-27},
	journal = {Computational Linguistics},
	author = {Fort, Karën and Adda, Gilles and Cohen, K. Bretonnel},
	month = jun,
	year = {2011},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {413--420},
}

@article{henderson_towards_2020,
	title = {Towards the {Systematic} {Reporting} of the {Energy} and {Carbon} {Footprints} of {Machine} {Learning}},
	url = {https://www.semanticscholar.org/paper/74b4f16c5ac91e3e7c88ae81cc8c91416b71d151},
	abstract = {Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.},
	urldate = {2024-03-26},
	journal = {ArXiv},
	author = {Henderson, Peter and Hu, Jie and Romoff, Joshua and Brunskill, E. and Jurafsky, Dan and Pineau, Joelle},
	month = jan,
	year = {2020},
}

@inproceedings{schwartz_green_2019,
	title = {Green {AI}},
	url = {https://www.semanticscholar.org/paper/Green-AI-Schwartz-Dodge/3c5f1ab37f70db503636075e15b3173f86eea00b},
	abstract = {The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or"price tag"of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.},
	urldate = {2024-03-26},
	author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
	month = jul,
	year = {2019},
}

@article{toxtli_quantifying_2021,
	title = {Quantifying the {Invisible} {Labor} in {Crowd} {Work}},
	volume = {5},
	issn = {2573-0142},
	url = {https://dl.acm.org/doi/10.1145/3476060},
	doi = {10.1145/3476060},
	abstract = {Crowdsourcing markets provide workers with a centralized place to find paid work. What may not be obvious at first glance is that, in addition to the work they do for pay, crowd workers also have to shoulder a variety of unpaid invisible labor in these markets, which ultimately reduces workers' hourly wages. Invisible labor includes finding good tasks, messaging requesters, or managing payments. However, we currently know little about how much time crowd workers actually spend on invisible labor or how much it costs them economically. To ensure a fair and equitable future for crowd work, we need to be certain that workers are being paid fairly for all of the work they do. In this paper, we conduct a field study to quantify the invisible labor in crowd work. We build a plugin to record the amount of time that 100 workers on Amazon Mechanical Turk dedicate to invisible labor while completing 40,903 tasks. If we ignore the time workers spent on invisible labor, workers' median hourly wage was \$3.76. But, we estimated that crowd workers in our study spent 33\% of their time daily on invisible labor, dropping their median hourly wage to \$2.83. We found that the invisible labor differentially impacts workers depending on their skill level and workers' demographics. The invisible labor category that took the most time and that was also the most common revolved around workers having to manage their payments. The second most time-consuming invisible labor category involved hyper-vigilance, where workers vigilantly watched over requesters' profiles for newly posted work or vigilantly searched for labor. We hope that through our paper, the invisible labor in crowdsourcing becomes more visible, and our results help to reveal the larger implications of the continuing invisibility of labor in crowdsourcing.},
	language = {en},
	number = {CSCW2},
	urldate = {2024-03-26},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Toxtli, Carlos and Suri, Siddharth and Savage, Saiph},
	month = oct,
	year = {2021},
	pages = {1--26},
}

@article{strubell_energy_2019,
	title = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},
	url = {https://www.aclweb.org/anthology/P19-1355},
	doi = {10.18653/v1/P19-1355},
	abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
	language = {en},
	urldate = {2024-03-26},
	journal = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	year = {2019},
	note = {Conference Name: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
Place: Florence, Italy
Publisher: Association for Computational Linguistics},
	pages = {3645--3650},
}

@article{hara_data-driven_2018,
	title = {A {Data}-{Driven} {Analysis} of {Workers}' {Earnings} on {Amazon} {Mechanical} {Turk}},
	url = {https://dl.acm.org/doi/10.1145/3173574.3174023},
	doi = {10.1145/3173574.3174023},
	abstract = {A growing number of people are working as part of on-line crowd work. Crowd work is often thought to be low wage work. However, we know little about the wage distribution in practice and what causes low/high earnings in this setting. We recorded 2,676 workers performing 3.8 million tasks on Amazon Mechanical Turk. Our task-level analysis revealed that workers earned a median hourly wage of only {\textasciitilde}\$2/h, and only 4\% earned more than \$7.25/h. While the average requester pays more than \$11/h, lower-paying requesters post much more work. Our wage calculations are influenced by how unpaid work is accounted for, e.g., time spent searching for tasks, working on tasks that are rejected, and working on tasks that are ultimately not submitted. We further explore the characteristics of tasks and working patterns that yield higher hourly wages. Our analysis informs platform design and worker tools to create a more positive future for crowd work.},
	language = {en},
	urldate = {2024-03-26},
	journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
	author = {Hara, Kotaro and Adams, Abigail and Milland, Kristy and Savage, Saiph and Callison-Burch, Chris and Bigham, Jeffrey P.},
	month = apr,
	year = {2018},
	note = {Conference Name: CHI '18: CHI Conference on Human Factors in Computing Systems
ISBN: 9781450356206
Place: Montreal QC Canada
Publisher: ACM},
	pages = {1--14},
}

@article{winner_artifacts_1980,
	title = {Do {Artifacts} {Have} {Politics}?},
	volume = {109},
	issn = {0011-5266},
	url = {https://www.jstor.org/stable/20024652},
	number = {1},
	urldate = {2024-03-26},
	journal = {Daedalus},
	author = {Winner, Langdon},
	year = {1980},
	note = {Publisher: The MIT Press},
	pages = {121--136},
}

@inproceedings{kann_towards_2019,
	address = {Hong Kong, China},
	title = {Towards {Realistic} {Practices} {In} {Low}-{Resource} {Natural} {Language} {Processing}: {The} {Development} {Set}},
	shorttitle = {Towards {Realistic} {Practices} {In} {Low}-{Resource} {Natural} {Language} {Processing}},
	url = {https://www.aclweb.org/anthology/D19-1329},
	doi = {10.18653/v1/D19-1329},
	abstract = {Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions: Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does it lead to overestimation or underestimation of performance? We repeat multiple experiments from recent work on neural models for low-resource NLP and compare results for models obtained by training with and without development sets. On average over languages, absolute accuracy differs by up to 1.4\%. However, for some languages and tasks, differences are as big as 18.0\% accuracy. Our results highlight the importance of realistic experimental setups in the publication of low-resource NLP research results.},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kann, Katharina and Cho, Kyunghyun and Bowman, Samuel R.},
	year = {2019},
	pages = {3340--3347},
}

@article{bederson_web_2011,
	title = {Web workers unite! addressing challenges of online laborers},
	url = {https://dl.acm.org/doi/10.1145/1979742.1979606},
	doi = {10.1145/1979742.1979606},
	abstract = {The ongoing rise of human computation as a means of solving computational problems has created an environment where human workers are often regarded as nameless, faceless computational resources. Some people have begun to think of online tasks as a "remote person call". In this paper, we summarize ethical and practical labor issues surrounding online labor, and offer a set of guidelines for designing and using online labor in ways that support more positive relationships between workers and requesters, so that both can gain the most benefit from the interaction.},
	language = {en},
	urldate = {2024-03-26},
	journal = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
	author = {Bederson, Benjamin B. and Quinn, Alexander J.},
	month = may,
	year = {2011},
	note = {Conference Name: CHI '11: CHI Conference on Human Factors in Computing Systems
ISBN: 9781450302685
Place: Vancouver BC Canada
Publisher: ACM},
	pages = {97--106},
}

@article{tan_mind_2020,
	title = {Mind {Your} {Inflections}! {Improving} {NLP} for {Non}-{Standard} {Englishes} with {Base}-{Inflection} {Encoding}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.455},
	doi = {10.18653/v1/2020.emnlp-main.455},
	abstract = {Morphological inflection is a process of word formation where base words are modified to express different grammatical categories such as tense, case, voice, person, or number. World Englishes, such as Colloquial Singapore English (CSE) and African American Vernacular English (AAVE), differ from Standard English dialects in inflection use. Although comprehension by human readers is usually unimpaired by non-standard inflection use, NLP systems are not so robust. We introduce a new Base-Inflection Encoding of English text that is achieved by combining linguistic and statistical techniques. Fine-tuning pre-trained NLP models for downstream tasks under this novel encoding achieves robustness to non-standard inflection use while maintaining performance on Standard English examples. Models using this encoding also generalize better to non-standard dialects without explicit training. We suggest metrics to evaluate tokenizers and extensive model-independent analyses demonstrate the efficacy of the encoding when used together with data-driven subword tokenizers.},
	language = {en},
	urldate = {2024-03-26},
	journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	author = {Tan, Samson and Joty, Shafiq and Varshney, Lav and Kan, Min-Yen},
	year = {2020},
	note = {Conference Name: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
Place: Online
Publisher: Association for Computational Linguistics},
	pages = {5647--5663},
}

@article{blodgett_racial_2017,
	title = {Racial {Disparity} in {Natural} {Language} {Processing}: {A} {Case} {Study} of {Social} {Media} {African}-{American} {English}},
	shorttitle = {Racial {Disparity} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/1707.00061},
	abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
	urldate = {2020-10-08},
	journal = {arXiv:1707.00061 [cs]},
	author = {Blodgett, Su Lin and O'Connor, Brendan},
	month = jun,
	year = {2017},
	note = {arXiv: 1707.00061},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@inproceedings{jurgens_incorporating_2017,
	address = {Vancouver, Canada},
	title = {Incorporating {Dialectal} {Variability} for {Socially} {Equitable} {Language} {Identification}},
	url = {https://aclanthology.org/P17-2009},
	doi = {10.18653/v1/P17-2009},
	abstract = {Language identification (LID) is a critical first step for processing multilingual text. Yet most LID systems are not designed to handle the linguistic diversity of global platforms like Twitter, where local dialects and rampant code-switching lead language classifiers to systematically miss minority dialect speakers and multilingual speakers. We propose a new dataset and a character-based sequence-to-sequence model for LID designed to support dialectal and multilingual language varieties. Our model achieves state-of-the-art performance on multiple LID benchmarks. Furthermore, in a case study using Twitter for health tracking, our method substantially increases the availability of texts written by underrepresented populations, enabling the development of “socially inclusive” NLP tools.},
	urldate = {2024-03-26},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Jurgens, David and Tsvetkov, Yulia and Jurafsky, Dan},
	editor = {Barzilay, Regina and Kan, Min-Yen},
	month = jul,
	year = {2017},
	pages = {51--57},
}

@inproceedings{bird_decolonising_2020,
	address = {Barcelona, Spain (Online)},
	title = {Decolonising {Speech} and {Language} {Technology}},
	url = {https://www.aclweb.org/anthology/2020.coling-main.313},
	doi = {10.18653/v1/2020.coling-main.313},
	abstract = {After generations of exploitation, Indigenous people often respond negatively to the idea that their languages are data ready for the taking. By treating Indigenous knowledge as a commodity, speech and language technologists risk disenfranchising local knowledge authorities, reenacting the causes of language endangerment. Scholars in related fields have responded to calls for decolonisation, and we in the speech and language technology community need to follow suit, and explore what this means for our practices that involve Indigenous languages and the communities who own them. This paper reviews colonising discourses in speech and language technology, and suggests new ways of working with Indigenous communities, and seeks to open a discussion of a postcolonial approach to computational methods for supporting language vitality.},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Bird, Steven},
	year = {2020},
	pages = {3504--3519},
}

@inproceedings{tatman_gender_2017,
	address = {Valencia, Spain},
	title = {Gender and {Dialect} {Bias} in {YouTube}'s {Automatic} {Captions}},
	url = {https://aclanthology.org/W17-1606},
	doi = {10.18653/v1/W17-1606},
	abstract = {This project evaluates the accuracy of YouTube's automatically-generated captions across two genders and five dialect groups. Speakers' dialect and gender was controlled for by using videos uploaded as part of the “accent tag challenge”, where speakers explicitly identify their language background. The results show robust differences in accuracy across both gender and dialect, with lower accuracy for 1) women and 2) speakers from Scotland. This finding builds on earlier research finding that speaker's sociolinguistic identity may negatively impact their ability to use automatic speech recognition, and demonstrates the need for sociolinguistically-stratified validation of systems.},
	urldate = {2024-03-26},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Tatman, Rachael},
	editor = {Hovy, Dirk and Spruit, Shannon and Mitchell, Margaret and Bender, Emily M. and Strube, Michael and Wallach, Hanna},
	month = apr,
	year = {2017},
	pages = {53--59},
}

@inproceedings{caglayan_curious_2020,
	address = {Barcelona, Spain (Online)},
	title = {Curious {Case} of {Language} {Generation} {Evaluation} {Metrics}: {A} {Cautionary} {Tale}},
	url = {https://aclanthology.org/2020.coling-main.210},
	doi = {10.18653/v1/2020.coling-main.210},
	abstract = {Automatic evaluation of language generation systems is a well-studied problem in Natural Language Processing. While novel metrics are proposed every year, a few popular metrics remain as the de facto metrics to evaluate tasks such as image captioning and machine translation, despite their known limitations. This is partly due to ease of use, and partly because researchers expect to see them and know how to interpret them. In this paper, we urge the community for more careful consideration of how they automatically evaluate their models by demonstrating important failure cases on multiple datasets, language pairs and tasks. Our experiments show that metrics (i) usually prefer system outputs to human-authored texts, (ii) can be insensitive to correct translations of rare words, (iii) can yield surprisingly high scores when given a single sentence as system output for the entire test set.},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Caglayan, Ozan and Madhyastha, Pranava and Specia, Lucia},
	editor = {Scott, Donia and Bel, Nuria and Zong, Chengqing},
	month = dec,
	year = {2020},
	pages = {2322--2328},
}

@inproceedings{przybyla_using_2022,
	address = {Dublin, Ireland},
	title = {Using {NLP} to quantify the environmental cost and diversity benefits of in-person {NLP} conferences},
	url = {https://aclanthology.org/2022.findings-acl.304},
	doi = {10.18653/v1/2022.findings-acl.304},
	abstract = {The environmental costs of research are progressively important to the NLP community and their associated challenges are increasingly debated. In this work, we analyse the carbon cost (measured as CO2-equivalent) associated with journeys made by researchers attending in-person NLP conferences. We obtain the necessary data by text-mining all publications from the ACL anthology available at the time of the study (n=60,572) and extracting information about an author’s affiliation, including their address. This allows us to estimate the corresponding carbon cost and compare it to previously known values for training large models. Further, we look at the benefits of in-person conferences by demonstrating that they can increase participation diversity by encouraging attendance from the region surrounding the host country. We show how the trade-off between carbon cost and diversity of an event depends on its location and type. Our aim is to foster further discussion on the best way to address the joint issue of emissions and diversity in the future.},
	language = {en},
	urldate = {2024-03-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Przybyła, Piotr and Shardlow, Matthew},
	year = {2022},
	pages = {3853--3863},
}

@inproceedings{mieskes_quantitative_2017,
	address = {Valencia, Spain},
	title = {A {Quantitative} {Study} of {Data} in the {NLP} community},
	url = {https://aclanthology.org/W17-1603},
	doi = {10.18653/v1/W17-1603},
	abstract = {We present results on a quantitative analysis of publications in the NLP domain on collecting, publishing and availability of research data. We find that a wide range of publications rely on data crawled from the web, but few give details on how potentially sensitive data was treated. Additionally, we find that while links to repositories of data are given, they often do not work even a short time after publication. We put together several suggestions on how to improve this situation based on publications from the NLP domain, but also other research areas.},
	urldate = {2024-03-26},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mieskes, Margot},
	editor = {Hovy, Dirk and Spruit, Shannon and Mitchell, Margaret and Bender, Emily M. and Strube, Michael and Wallach, Hanna},
	month = apr,
	year = {2017},
	pages = {23--29},
}

@inproceedings{fort_crowdsourcing_2014,
	address = {Cham},
	title = {Crowdsourcing for {Language} {Resource} {Development}: {Criticisms} {About} {Amazon} {Mechanical} {Turk} {Overpowering} {Use}},
	isbn = {978-3-319-08958-4},
	shorttitle = {Crowdsourcing for {Language} {Resource} {Development}},
	doi = {10.1007/978-3-319-08958-4_25},
	abstract = {This article is a position paper about Amazon Mechanical Turk, the use of which has been steadily growing in language processing in the past few years. According to the mainstream opinion expressed in articles of the domain, this type of on-line working platforms allows to develop quickly all sorts of quality language resources, at a very low price, by people doing that as a hobby. We shall demonstrate here that the situation is far from being that ideal. Our goal here is manifold: 1- to inform researchers, so that they can make their own choices, 2- to develop alternatives with the help of funding agencies and scientific associations, 3- to propose practical and organizational solutions in order to improve language resources development, while limiting the risks of ethical and legal issues without letting go price or quality, 4- to introduce an Ethics and Big Data Charter for the documentation of language resources.},
	language = {en},
	booktitle = {Human {Language} {Technology} {Challenges} for {Computer} {Science} and {Linguistics}},
	publisher = {Springer International Publishing},
	author = {Fort, Karën and Adda, Gilles and Sagot, Benoît and Mariani, Joseph and Couillault, Alain},
	editor = {Vetulani, Zygmunt and Mariani, Joseph},
	year = {2014},
	keywords = {Amazon Mechanical Turk, Ethics, Language resources},
	pages = {303--314},
}

@article{bender_data_2018,
	title = {Data {Statements} for {Natural} {Language} {Processing}: {Toward} {Mitigating} {System} {Bias} and {Enabling} {Better} {Science}},
	volume = {6},
	issn = {2307-387X},
	shorttitle = {Data {Statements} for {Natural} {Language} {Processing}},
	url = {https://direct.mit.edu/tacl/article/43452},
	doi = {10.1162/tacl_a_00041},
	abstract = {In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.},
	language = {en},
	urldate = {2024-03-25},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Bender, Emily M. and Friedman, Batya},
	month = dec,
	year = {2018},
	pages = {587--604},
}

@inproceedings{couillault_evaluating_2014,
	title = {Evaluating corpora documentation with regards to the {Ethics} and {Big} {Data} {Charter}},
	url = {https://www.semanticscholar.org/paper/Evaluating-corpora-documentation-with-regards-to-Couillault-Fort/c82848399fa374093afbdf4184310ec2c72ebeda},
	abstract = {The authors have written the Ethic and Big Data Charter in collaboration with various agencies, private bodies and associations. This Charter aims at describing any large or complex resources, and in particular language resources, from a legal and ethical viewpoint and ensuring the transparency of the process of creating and distributing such resources. We propose in this article an analysis of the documentation coverage of the most frequently mentioned language resources with regards to the Charter, in order to show the benefit it offers.},
	urldate = {2024-03-25},
	author = {Couillault, Alain and Fort, Karën and Adda, G. and Mazancourt, Hugues de},
	month = may,
	year = {2014},
}

@inproceedings{leidner_ethical_2017,
	address = {Valencia, Spain},
	title = {Ethical by {Design}: {Ethics} {Best} {Practices} for {Natural} {Language} {Processing}},
	shorttitle = {Ethical by {Design}},
	url = {http://aclweb.org/anthology/W17-1604},
	doi = {10.18653/v1/W17-1604},
	abstract = {Natural language processing (NLP) systems analyze and/or generate human language, typically on users’ behalf. One natural and necessary question that needs to be addressed in this context, both in research projects and in production settings, is the question how ethical the work is, both regarding the process and its outcome. Towards this end, we articulate a set of issues, propose a set of best practices, notably a process featuring an ethics review board, and sketch and how they could be meaningfully applied. Our main argument is that ethical outcomes ought to be achieved by design, i.e. by following a process aligned by ethical values. We also offer some response options for those facing ethics issues. While a number of previous works exist that discuss ethical issues, in particular around big data and machine learning, to the authors’ knowledge this is the first account of NLP and ethics from the perspective of a principled process.},
	language = {en},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Leidner, Jochen L. and Plachouras, Vassilis},
	year = {2017},
	pages = {30--40},
}

@inproceedings{tsvetkov_socially_2018,
	address = {New Orleans, Louisiana},
	title = {Socially {Responsible} {NLP}},
	url = {http://aclweb.org/anthology/N18-6005},
	doi = {10.18653/v1/N18-6005},
	abstract = {As language technologies have become increasingly prevalent, there is a growing awareness that decisions we make about our data, methods, and tools are often tied up with their impact on people and societies. This tutorial will provide an overview of real-world applications of language technologies and the potential ethical implications associated with them. We will discuss philosophical foundations of ethical research along with state of the art techniques. Through this tutorial, we intend to provide the NLP researcher with an overview of tools to ensure that the data, algorithms, and models that they build are socially responsible. These tools will include a checklist of common pitfalls that one should avoid (e.g., demographic bias in data collection), as well as methods to adequately mitigate these issues (e.g., adjusting sampling rates or de-biasing through regularization). The tutorial is based on a new course on Ethics and NLP developed at Carnegie Mellon University.},
	language = {en},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Tutorial} {Abstracts}},
	publisher = {Association for Computational Linguistics},
	author = {Tsvetkov, Yulia and Prabhakaran, Vinodkumar and Voigt, Rob},
	year = {2018},
	pages = {24--26},
}

@article{derczynski_assessing_2023,
	title = {Assessing {Language} {Model} {Deployment} with {Risk} {Cards}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2303.18190},
	doi = {10.48550/ARXIV.2303.18190},
	abstract = {This paper introduces RiskCards, a framework for structured assessment and documentation of risks associated with an application of language models. As with all language, text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text. Prior work establishes a wide variety of language model harms to many different actors: existing taxonomies identify categories of harms posed by language models; benchmarks establish automated tests of these harms; and documentation standards for models, tasks and datasets encourage transparent reporting. However, there is no risk-centric framework for documenting the complexity of a landscape in which some risks are shared across models and contexts, while others are specific, and where certain conditions may be required for risks to manifest as harms. RiskCards address this methodological gap by providing a generic framework for assessing the use of a given language model in a given scenario. Each RiskCard makes clear the routes for the risk to manifest harm, their placement in harm taxonomies, and example prompt-output pairs. While RiskCards are designed to be open-source, dynamic and participatory, we present a "starter set" of RiskCards taken from a broad literature survey, each of which details a concrete risk presentation. Language model RiskCards initiate a community knowledge base which permits the mapping of risks and harms to a specific model or its application scenario, ultimately contributing to a better, safer and shared understanding of the risk landscape.},
	urldate = {2024-03-25},
	author = {Derczynski, Leon and Kirk, Hannah Rose and Balachandran, Vidhisha and Kumar, Sachin and Tsvetkov, Yulia and Leiser, M. R. and Mohammad, Saif},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{ulmer_experimental_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Experimental {Standards} for {Deep} {Learning} in {Natural} {Language} {Processing} {Research}},
	url = {https://aclanthology.org/2022.findings-emnlp.196},
	doi = {10.18653/v1/2022.findings-emnlp.196},
	abstract = {The field of Deep Learning (DL) has undergone explosive growth during the last decade, with a substantial impact on Natural Language Processing (NLP) as well. Yet, compared to more established disciplines, a lack of common experimental standards remains an open challenge to the field at large. Starting from fundamental scientific principles, we distill ongoing discussions on experimental standards in NLP into a single, widely-applicable methodology. Following these best practices is crucial to strengthen experimental evidence, improve reproducibility and support scientific progress. These standards are further collected in a public repository to help them transparently adapt to future needs.},
	language = {en},
	urldate = {2024-03-25},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Ulmer, Dennis and Bassignana, Elisa and Müller-Eberstein, Max and Varab, Daniel and Zhang, Mike and Van Der Goot, Rob and Hardmeier, Christian and Plank, Barbara},
	year = {2022},
	pages = {2673--2692},
}

@article{kaffee_thorny_2023,
	title = {Thorny {Roses}: {Investigating} the {Dual} {Use} {Dilemma} in {Natural} {Language} {Processing}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Thorny {Roses}},
	url = {https://arxiv.org/abs/2304.08315},
	doi = {10.48550/ARXIV.2304.08315},
	abstract = {Dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of Natural Language Processing (NLP). However, as NLP technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. Therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. In this paper, we conduct a survey of NLP researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. Based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the NLP community. The survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. In light of the survey results, we discuss the current state and potential means for mitigating dual use in NLP and propose a checklist that can be integrated into existing conference ethics-frameworks, e.g., the ACL ethics checklist.},
	urldate = {2024-03-25},
	author = {Kaffee, Lucie-Aimée and Arora, Arnav and Talat, Zeerak and Augenstein, Isabelle},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{vida_values_2023,
	title = {Values, {Ethics}, {Morals}? {On} the {Use} of {Moral} {Concepts} in {NLP} {Research}},
	copyright = {Creative Commons Zero v1.0 Universal},
	shorttitle = {Values, {Ethics}, {Morals}?},
	url = {https://arxiv.org/abs/2310.13915},
	doi = {10.48550/ARXIV.2310.13915},
	abstract = {With language technology increasingly affecting individuals' lives, many recent works have investigated the ethical aspects of NLP. Among other topics, researchers focused on the notion of morality, investigating, for example, which moral judgements language models make. However, there has been little to no discussion of the terminology and the theories underpinning those efforts and their implications. This lack is highly problematic, as it hides the works' underlying assumptions and hinders a thorough and targeted scientific debate of morality in NLP. In this work, we address this research gap by (a) providing an overview of some important ethical concepts stemming from philosophy and (b) systematically surveying the existing literature on moral NLP w.r.t. their philosophical foundation, terminology, and data basis. For instance, we analyse what ethical theory an approach is based on, how this decision is justified, and what implications it entails. Our findings surveying 92 papers show that, for instance, most papers neither provide a clear definition of the terms they use nor adhere to definitions from philosophy. Finally, (c) we give three recommendations for future research in the field. We hope our work will lead to a more informed, careful, and sound discussion of morality in language technology.},
	urldate = {2024-03-25},
	author = {Vida, Karina and Simon, Judith and Lauscher, Anne},
	year = {2023},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences},
}

@article{wong_seeing_2023,
	title = {Seeing {Like} a {Toolkit}: {How} {Toolkits} {Envision} the {Work} of {AI} {Ethics}},
	volume = {7},
	issn = {2573-0142},
	shorttitle = {Seeing {Like} a {Toolkit}},
	url = {https://dl.acm.org/doi/10.1145/3579621},
	doi = {10.1145/3579621},
	abstract = {Numerous toolkits have been developed to support ethical AI development. However, toolkits, like all tools, encode assumptions in their design about what work should be done and how. In this paper, we conduct a qualitative analysis of 27 AI ethics toolkits to critically examine how the work of ethics is imagined and how it is supported by these toolkits. Specifically, we examine the discourses toolkits rely on when talking about ethical issues, who they imagine should do the work of ethics, and how they envision the work practices involved in addressing ethics. Among the toolkits, we identify a mismatch between the imagined work of ethics and the support the toolkits provide for doing that work. In particular, we identify a lack of guidance around how to navigate labor, organizational, and institutional power dynamics as they relate to performing ethical work. We use these omissions to chart future work for researchers and designers of AI ethics toolkits.},
	language = {en},
	number = {CSCW1},
	urldate = {2024-03-25},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Wong, Richmond Y. and Madaio, Michael A. and Merrill, Nick},
	month = apr,
	year = {2023},
	pages = {1--27},
}

@inproceedings{jurgens_just_2019,
	address = {Florence, Italy},
	title = {A {Just} and {Comprehensive} {Strategy} for {Using} {NLP} to {Address} {Online} {Abuse}},
	url = {https://aclanthology.org/P19-1357},
	doi = {10.18653/v1/P19-1357},
	abstract = {Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities.},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jurgens, David and Hemphill, Libby and Chandrasekharan, Eshwar},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	pages = {3658--3666},
}

@article{savoldi_gender_2021,
	title = {Gender {Bias} in {Machine} {Translation}},
	volume = {9},
	url = {https://aclanthology.org/2021.tacl-1.51},
	doi = {10.1162/tacl_a_00401},
	abstract = {AbstractMachine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, processing, and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, studies of gender bias in MT still lack cohesion. This advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.},
	urldate = {2024-03-25},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Savoldi, Beatrice and Gaido, Marco and Bentivogli, Luisa and Negri, Matteo and Turchi, Marco},
	editor = {Roark, Brian and Nenkova, Ani},
	year = {2021},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {845--874},
}

@inproceedings{subramanian_fairness-aware_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Fairness-aware {Class} {Imbalanced} {Learning}},
	url = {https://aclanthology.org/2021.emnlp-main.155},
	doi = {10.18653/v1/2021.emnlp-main.155},
	abstract = {Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Subramanian, Shivashankar and Rahimi, Afshin and Baldwin, Timothy and Cohn, Trevor and Frermann, Lea},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {2045--2051},
}

@inproceedings{haroutunian_ethical_2022,
	address = {Dublin, Ireland},
	title = {Ethical {Considerations} for {Low}-resourced {Machine} {Translation}},
	url = {https://aclanthology.org/2022.acl-srw.5},
	doi = {10.18653/v1/2022.acl-srw.5},
	abstract = {This paper considers some ethical implications of machine translation for low-resourced languages. I use Armenian as a case study and investigate specific needs for and concerns arising from the creation and deployment of improved machine translation between English and Armenian. To do this, I conduct stakeholder interviews and construct Value Scenarios (Nathan et al., 2007) from the themes that emerge. These scenarios illustrate some of the potential harms that low-resourced language communities may face due to the deployment of improved machine translation systems. Based on these scenarios, I recommend 1) collaborating with stakeholders in order to create more useful and reliable machine translation tools, and 2) determining which other forms of language technology should be developed alongside efforts to improve machine translation in order to mitigate harms rendered to vulnerable language communities. Both of these goals require treating low-resourced machine translation as a language-specific, rather than language-agnostic, task.},
	urldate = {2024-03-25},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Student} {Research} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Haroutunian, Levon},
	editor = {Louvan, Samuel and Madotto, Andrea and Madureira, Brielen},
	month = may,
	year = {2022},
	pages = {44--54},
}

@inproceedings{ribeiro_beyond_2020,
	address = {Online},
	title = {Beyond {Accuracy}: {Behavioral} {Testing} of {NLP} {Models} with {CheckList}},
	shorttitle = {Beyond {Accuracy}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.442},
	doi = {10.18653/v1/2020.acl-main.442},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
	urldate = {2020-10-28},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	month = jul,
	year = {2020},
	pages = {4902--4912},
}

@inproceedings{madnani_building_2017,
	address = {Valencia, Spain},
	title = {Building {Better} {Open}-{Source} {Tools} to {Support} {Fairness} in {Automated} {Scoring}},
	url = {https://aclanthology.org/W17-1605},
	doi = {10.18653/v1/W17-1605},
	abstract = {Automated scoring of written and spoken responses is an NLP application that can significantly impact lives especially when deployed as part of high-stakes tests such as the GRE® and the TOEFL®. Ethical considerations require that automated scoring algorithms treat all test-takers fairly. The educational measurement community has done significant research on fairness in assessments and automated scoring systems must incorporate their recommendations. The best way to do that is by making available automated, non-proprietary tools to NLP researchers that directly incorporate these recommendations and generate the analyses needed to help identify and resolve biases in their scoring systems. In this paper, we attempt to provide such a solution.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Madnani, Nitin and Loukina, Anastassia and von Davier, Alina and Burstein, Jill and Cahill, Aoife},
	editor = {Hovy, Dirk and Spruit, Shannon and Mitchell, Margaret and Bender, Emily M. and Strube, Michael and Wallach, Hanna},
	month = apr,
	year = {2017},
	pages = {41--52},
}

@inproceedings{tan_reliability_2021,
	address = {Online},
	title = {Reliability {Testing} for {Natural} {Language} {Processing} {Systems}},
	url = {https://aclanthology.org/2021.acl-long.321},
	doi = {10.18653/v1/2021.acl-long.321},
	abstract = {Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing — with an emphasis on interdisciplinary collaboration — will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Samson and Joty, Shafiq and Baxter, Kathy and Taeihagh, Araz and Bennett, Gregory A. and Kan, Min-Yen},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {4153--4169},
}

@inproceedings{barikeri_redditbias_2021,
	address = {Online},
	title = {{RedditBias}: {A} {Real}-{World} {Resource} for {Bias} {Evaluation} and {Debiasing} of {Conversational} {Language} {Models}},
	shorttitle = {{RedditBias}},
	url = {https://aclanthology.org/2021.acl-long.151},
	doi = {10.18653/v1/2021.acl-long.151},
	abstract = {Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Barikeri, Soumya and Lauscher, Anne and Vulić, Ivan and Glavaš, Goran},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {1941--1955},
}

@inproceedings{calabrese_aaa_2021,
	address = {Virtual Event United Kingdom},
	title = {{AAA}: {Fair} {Evaluation} for {Abuse} {Detection} {Systems} {Wanted}},
	isbn = {978-1-4503-8330-1},
	shorttitle = {{AAA}},
	url = {https://dl.acm.org/doi/10.1145/3447535.3462484},
	doi = {10.1145/3447535.3462484},
	language = {en},
	urldate = {2024-03-22},
	booktitle = {13th {ACM} {Web} {Science} {Conference} 2021},
	publisher = {ACM},
	author = {Calabrese, Agostina and Bevilacqua, Michele and Ross, Björn and Tripodi, Rocco and Navigli, Roberto},
	month = jun,
	year = {2021},
	pages = {243--252},
}

@misc{bhatt_re-contextualizing_2022,
	title = {Re-contextualizing {Fairness} in {NLP}: {The} {Case} of {India}},
	shorttitle = {Re-contextualizing {Fairness} in {NLP}},
	url = {http://arxiv.org/abs/2209.12226},
	doi = {10.48550/arXiv.2209.12226},
	abstract = {Recent research has revealed undesirable biases in NLP data and models. However, these efforts focus on social disparities in West, and are not directly portable to other geo-cultural contexts. In this paper, we focus on NLP fair-ness in the context of India. We start with a brief account of the prominent axes of social disparities in India. We build resources for fairness evaluation in the Indian context and use them to demonstrate prediction biases along some of the axes. We then delve deeper into social stereotypes for Region andReligion, demonstrating its prevalence in corpora and models. Finally, we outline a holistic research agenda to re-contextualize NLP fairness research for the Indian context, ac-counting for Indian societal context, bridging technological gaps in NLP capabilities and re-sources, and adapting to Indian cultural values. While we focus on India, this framework can be generalized to other geo-cultural contexts.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Bhatt, Shaily and Dev, Sunipa and Talukdar, Partha and Dave, Shachi and Prabhakaran, Vinodkumar},
	month = nov,
	year = {2022},
	note = {arXiv:2209.12226 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@inproceedings{gooding_ethical_2022,
	address = {Dublin, Ireland},
	title = {On the {Ethical} {Considerations} of {Text} {Simplification}},
	url = {https://aclanthology.org/2022.slpat-1.7},
	doi = {10.18653/v1/2022.slpat-1.7},
	abstract = {This paper outlines the ethical implications of text simplification within the framework of assistive systems. We argue that a distinction should be made between the technologies that perform text simplification and the realisation of these in assistive technologies. When using the latter as a motivation for research, it is important that the subsequent ethical implications be carefully considered. We provide guidelines for the framing of text simplification independently of assistive systems, as well as suggesting directions for future research and discussion based on the concerns raised.},
	urldate = {2024-03-22},
	booktitle = {Ninth {Workshop} on {Speech} and {Language} {Processing} for {Assistive} {Technologies} ({SLPAT}-2022)},
	publisher = {Association for Computational Linguistics},
	author = {Gooding, Sian},
	editor = {Ebling, Sarah and Prud'hommeaux, Emily and Vaidyanathan, Preethi},
	month = may,
	year = {2022},
	pages = {50--57},
}

@inproceedings{drugan_shared_2010,
	address = {Denver, Colorado, USA},
	title = {Shared {Resources}, {Shared} {Values}? {Ethical} {Implications} of {Sharing} {Translation} {Resources}},
	shorttitle = {Shared {Resources}, {Shared} {Values}?},
	url = {https://aclanthology.org/2010.jec-1.2},
	abstract = {The exploitation of large corpora to create and populate shared translation resources has been hampered in two areas: first, practical problems (“locked-in” data, ineffective exchange formats, client reservations); and second, ethical and legal problems. Recent developments, notably online collaborative translation environments (Desillets, 2007) and greater industry openness, might have been expected to highlight such issues. Yet the growing use of shared data is being addressed only gingerly. Good reasons lie behind the failure to broach the ethics of shared resources. The issues are challenging: confidentiality, ownership, copyright, authorial rights, attribution, the law, protectionism, costs, fairness, motivation, trust, quality, reliability. However, we argue that, though complex, these issues should not be swept under the carpet. The huge demand for translation cannot be met without intelligent sharing of resources (Kelly, 2009). Relevant ethical considerations have already been identified in translation and related domains, in such texts as Codes of Ethics, international conventions and declarations, and Codes of Professional Conduct; these can be useful here. We outline two case studies from current industry initiatives, highlighting their ethical implications. We identify questions which users and developers should be asking and relate these to existing debates and codes as a practical framework for their consideration.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the {Second} {Joint} {EM}+/{CNGL} {Workshop}: {Bringing} {MT} to the {User}: {Research} on {Integrating} {MT} in the {Translation} {Industry}},
	publisher = {Association for Machine Translation in the Americas},
	author = {Drugan, Jo and Babych, Bogdan},
	editor = {Zhechev, Ventsislav},
	month = nov,
	year = {2010},
	pages = {3--10},
}

@inproceedings{ravichander_breaking_2021,
	address = {Online},
	title = {Breaking {Down} {Walls} of {Text}: {How} {Can} {NLP} {Benefit} {Consumer} {Privacy}?},
	shorttitle = {Breaking {Down} {Walls} of {Text}},
	url = {https://aclanthology.org/2021.acl-long.319},
	doi = {10.18653/v1/2021.acl-long.319},
	abstract = {Privacy plays a crucial role in preserving democratic ideals and personal autonomy. The dominant legal approach to privacy in many jurisdictions is the “Notice and Choice” paradigm, where privacy policies are the primary instrument used to convey information to users. However, privacy policies are long and complex documents that are difficult for users to read and comprehend. We discuss how language technologies can play an important role in addressing this information gap, reporting on initial progress towards helping three specific categories of stakeholders take advantage of digital privacy policies: consumers, enterprises, and regulators. Our goal is to provide a roadmap for the development and use of language technologies to empower users to reclaim control over their privacy, limit privacy harms, and rally research efforts from the community towards addressing an issue with large social impact. We highlight many remaining opportunities to develop language technologies that are more precise or nuanced in the way in which they use the text of privacy policies.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ravichander, Abhilasha and Black, Alan W and Norton, Thomas and Wilson, Shomir and Sadeh, Norman},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {4125--4140},
}

@inproceedings{liu_not_2022,
	address = {Dublin, Ireland},
	title = {Not always about you: {Prioritizing} community needs when developing endangered language technology},
	shorttitle = {Not always about you},
	url = {https://aclanthology.org/2022.acl-long.272},
	doi = {10.18653/v1/2022.acl-long.272},
	abstract = {Languages are classified as low-resource when they lack the quantity of data necessary for training statistical and machine learning tools and models. Causes of resource scarcity vary but can include poor access to technology for developing these resources, a relatively small population of speakers, or a lack of urgency for collecting such resources in bilingual populations where the second language is high-resource. As a result, the languages described as low-resource in the literature are as different as Finnish on the one hand, with millions of speakers using it in every imaginable domain, and Seneca, with only a small-handful of fluent speakers using the language primarily in a restricted domain. While issues stemming from the lack of resources necessary to train models unite this disparate group of languages, many other issues cut across the divide between widely-spoken low-resource languages and endangered languages. In this position paper, we discuss the unique technological, cultural, practical, and ethical challenges that researchers and indigenous speech community members face when working together to develop language technology to support endangered language documentation and revitalization. We report the perspectives of language teachers, Master Speakers and elders from indigenous communities, as well as the point of view of academics. We describe an ongoing fruitful collaboration and make recommendations for future partnerships between academic researchers and language community stakeholders.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Zoey and Richardson, Crystal and Hatcher, Richard and Prud'hommeaux, Emily},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {3933--3944},
}

@inproceedings{hovy_social_2018,
	address = {New Orleans, Louisiana, USA},
	title = {The {Social} and the {Neural} {Network}: {How} to {Make} {Natural} {Language} {Processing} about {People} again},
	shorttitle = {The {Social} and the {Neural} {Network}},
	url = {https://aclanthology.org/W18-1106},
	doi = {10.18653/v1/W18-1106},
	abstract = {Over the years, natural language processing has increasingly focused on tasks that can be solved by statistical models, but ignored the social aspects of language. These limitations are in large part due to historically available data and the limitations of the models, but have narrowed our focus and biased the tools demographically. However, with the increased availability of data sets including socio-demographic information and more expressive (neural) models, we have the opportunity to address both issues. I argue that this combination can broaden the focus of NLP to solve a whole new range of tasks, enable us to generate novel linguistic insights, and provide fairer tools for everyone.},
	urldate = {2024-03-22},
	booktitle = {Proceedings of the {Second} {Workshop} on {Computational} {Modeling} of {People}'s {Opinions}, {Personality}, and {Emotions} in {Social} {Media}},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk},
	editor = {Nissim, Malvina and Patti, Viviana and Plank, Barbara and Wagner, Claudia},
	month = jun,
	year = {2018},
	pages = {42--49},
}

@inproceedings{lopez_long_interaction_2021,
	address = {Held Online},
	title = {On the {Interaction} between {Annotation} {Quality} and {Classifier} {Performance} in {Abusive} {Language} {Detection}},
	url = {https://aclanthology.org/2021.ranlp-1.99},
	abstract = {Abusive language detection has become an important tool for the cultivation of safe online platforms. We investigate the interaction of annotation quality and classifier performance. We use a new, fine-grained annotation scheme that allows us to distinguish between abusive language and colloquial uses of profanity that are not meant to harm. Our results show a tendency of crowd workers to overuse the abusive class, which creates an unrealistic class balance and affects classification accuracy. We also investigate different methods of distinguishing between explicit and implicit abuse and show lexicon-based approaches either over- or under-estimate the proportion of explicit abuse in data sets.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2021)},
	publisher = {INCOMA Ltd.},
	author = {Lopez Long, Holly and O'Neil, Alexandra and Kübler, Sandra},
	editor = {Mitkov, Ruslan and Angelova, Galia},
	month = sep,
	year = {2021},
	pages = {868--875},
}

@inproceedings{caselli_guiding_2021,
	address = {Online},
	title = {Guiding {Principles} for {Participatory} {Design}-inspired {Natural} {Language} {Processing}},
	url = {https://aclanthology.org/2021.nlp4posimpact-1.4},
	doi = {10.18653/v1/2021.nlp4posimpact-1.4},
	abstract = {We introduce 9 guiding principles to integrate Participatory Design (PD) methods in the development of Natural Language Processing (NLP) systems. The adoption of PD methods by NLP will help to alleviate issues concerning the development of more democratic, fairer, less-biased technologies to process natural language data. This short paper is the outcome of an ongoing dialogue between designers and NLP experts and adopts a non-standard format following previous work by Traum (2000); Bender (2013); Abzianidze and Bos (2019). Every section is a guiding principle. While principles 1–3 illustrate assumptions and methods that inform community-based PD practices, we used two fictional design scenarios (Encinas and Blythe, 2018), which build on top of situations familiar to the authors, to elicit the identification of the other 6. Principles 4–6 describes the impact of PD methods on the design of NLP systems, targeting two critical aspects: data collection \& annotation, and the deployment \& evaluation. Finally, principles 7–9 guide a new reflexivity of the NLP research with respect to its context, actors and participants, and aims. We hope this guide will offer inspiration and a road-map to develop a new generation of PD-inspired NLP.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 1st {Workshop} on {NLP} for {Positive} {Impact}},
	publisher = {Association for Computational Linguistics},
	author = {Caselli, Tommaso and Cibin, Roberto and Conforti, Costanza and Encinas, Enrique and Teli, Maurizio},
	editor = {Field, Anjalie and Prabhumoye, Shrimai and Sap, Maarten and Jin, Zhijing and Zhao, Jieyu and Brockett, Chris},
	month = aug,
	year = {2021},
	pages = {27--35},
}

@inproceedings{inie_idr_2021,
	address = {Online},
	title = {An {IDR} {Framework} of {Opportunities} and {Barriers} between {HCI} and {NLP}},
	url = {https://aclanthology.org/2021.hcinlp-1.16},
	abstract = {This paper presents a framework of opportunities and barriers/risks between the two research fields Natural Language Processing (NLP) and Human-Computer Interaction (HCI). The framework is constructed by following an interdisciplinary research-model (IDR), combining field-specific knowledge with existing work in the two fields. The resulting framework is intended as a departure point for discussion and inspiration for research collaborations.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the {First} {Workshop} on {Bridging} {Human}–{Computer} {Interaction} and {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Inie, Nanna and Derczynski, Leon},
	editor = {Blodgett, Su Lin and Madaio, Michael and O'Connor, Brendan and Wallach, Hanna and Yang, Qian},
	month = apr,
	year = {2021},
	pages = {101--108},
}

@misc{kawakami_situate_2024,
	title = {The {Situate} {AI} {Guidebook}: {Co}-{Designing} a {Toolkit} to {Support} {Multi}-{Stakeholder} {Early}-stage {Deliberations} {Around} {Public} {Sector} {AI} {Proposals}},
	shorttitle = {The {Situate} {AI} {Guidebook}},
	url = {http://arxiv.org/abs/2402.18774},
	doi = {10.1145/3613904.3642849},
	abstract = {Public sector agencies are rapidly deploying AI systems to augment or automate critical decisions in real-world contexts like child welfare, criminal justice, and public health. A growing body of work documents how these AI systems often fail to improve services in practice. These failures can often be traced to decisions made during the early stages of AI ideation and design, such as problem formulation. However, today, we lack systematic processes to support effective, early-stage decision-making about whether and under what conditions to move forward with a proposed AI project. To understand how to scaffold such processes in real-world settings, we worked with public sector agency leaders, AI developers, frontline workers, and community advocates across four public sector agencies and three community advocacy groups in the United States. Through an iterative co-design process, we created the Situate AI Guidebook: a structured process centered around a set of deliberation questions to scaffold conversations around (1) goals and intended use or a proposed AI system, (2) societal and legal considerations, (3) data and modeling constraints, and (4) organizational governance factors. We discuss how the guidebook's design is informed by participants' challenges, needs, and desires for improved deliberation processes. We further elaborate on implications for designing responsible AI toolkits in collaboration with public sector agency stakeholders and opportunities for future work to expand upon the guidebook. This design approach can be more broadly adopted to support the co-creation of responsible AI toolkits that scaffold key decision-making processes surrounding the use of AI in the public sector and beyond.},
	urldate = {2024-03-07},
	author = {Kawakami, Anna and Coston, Amanda and Zhu, Haiyi and Heidari, Hoda and Holstein, Kenneth},
	month = mar,
	year = {2024},
	note = {arXiv:2402.18774 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@inproceedings{rogers_just_2021,
	address = {Punta Cana, Dominican Republic},
	title = {`{Just} {What} do {You} {Think} {You}'re {Doing}, {Dave}?' {A} {Checklist} for {Responsible} {Data} {Use} in {NLP}},
	shorttitle = {`{Just} {What} do {You} {Think} {You}'re {Doing}, {Dave}?},
	url = {https://aclanthology.org/2021.findings-emnlp.414},
	doi = {10.18653/v1/2021.findings-emnlp.414},
	abstract = {A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a potential checklist for responsible data (re-)use that could both standardise the peer review of conference submissions, as well as enable a more in-depth view of published research across the community. Our proposal aims to contribute to the development of a consistent standard for data (re-)use, embraced across NLP conferences.},
	urldate = {2024-03-07},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Rogers, Anna and Baldwin, Timothy and Leins, Kobi},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	pages = {4821--4833},
}

@inproceedings{lignos_toward_2022,
	address = {Dublin, Ireland},
	title = {Toward {More} {Meaningful} {Resources} for {Lower}-resourced {Languages}},
	url = {https://aclanthology.org/2022.findings-acl.44},
	doi = {10.18653/v1/2022.findings-acl.44},
	abstract = {In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages. Before advancing that position, we first examine two massively multilingual resources used in language technology development, identifying shortcomings that limit their usefulness. We explore the contents of the names stored in Wikidata for a few lower-resourced languages and find that many of them are not in fact in the languages they claim to be, requiring non-trivial effort to correct. We discuss quality issues present in WikiAnn and evaluate whether it is a useful supplement to hand-annotated data. We then discuss the importance of creating annotations for lower-resourced languages in a thoughtful and ethical way that includes the language speakers as part of the development process. We conclude with recommended guidelines for resource development.},
	urldate = {2024-03-07},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Lignos, Constantine and Holley, Nolan and Palen-Michel, Chester and Sälevä, Jonne},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {523--532},
}

@inproceedings{benton_ethical_2017,
	address = {Valencia, Spain},
	title = {Ethical {Research} {Protocols} for {Social} {Media} {Health} {Research}},
	url = {https://aclanthology.org/W17-1612},
	doi = {10.18653/v1/W17-1612},
	abstract = {Social media have transformed data-driven research in political science, the social sciences, health, and medicine. Since health research often touches on sensitive topics that relate to ethics of treatment and patient privacy, similar ethical considerations should be acknowledged when using social media data in health research. While much has been said regarding the ethical considerations of social media research, health research leads to an additional set of concerns. We provide practical suggestions in the form of guidelines for researchers working with social media data in health research. These guidelines can inform an IRB proposal for researchers new to social media health research.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Benton, Adrian and Coppersmith, Glen and Dredze, Mark},
	editor = {Hovy, Dirk and Spruit, Shannon and Mitchell, Margaret and Bender, Emily M. and Strube, Michael and Wallach, Hanna},
	month = apr,
	year = {2017},
	pages = {94--102},
}

@inproceedings{parra_escartin_ethical_2017,
	address = {Valencia, Spain},
	title = {Ethical {Considerations} in {NLP} {Shared} {Tasks}},
	url = {https://aclanthology.org/W17-1608},
	doi = {10.18653/v1/W17-1608},
	abstract = {Shared tasks are increasingly common in our field, and new challenges are suggested at almost every conference and workshop. However, as this has become an established way of pushing research forward, it is important to discuss how we researchers organise and participate in shared tasks, and make that information available to the community to allow further research improvements. In this paper, we present a number of ethical issues along with other areas of concern that are related to the competitive nature of shared tasks. As such issues could potentially impact on research ethics in the Natural Language Processing community, we also propose the development of a framework for the organisation of and participation in shared tasks that can help mitigate against these issues arising.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Parra Escartín, Carla and Reijers, Wessel and Lynn, Teresa and Moorkens, Joss and Way, Andy and Liu, Chao-Hong},
	editor = {Hovy, Dirk and Spruit, Shannon and Mitchell, Margaret and Bender, Emily M. and Strube, Michael and Wallach, Hanna},
	month = apr,
	year = {2017},
	pages = {66--73},
}

@inproceedings{larson_gender_2017,
	address = {Valencia, Spain},
	title = {Gender as a {Variable} in {Natural}-{Language} {Processing}: {Ethical} {Considerations}},
	shorttitle = {Gender as a {Variable} in {Natural}-{Language} {Processing}},
	url = {https://aclanthology.org/W17-1601},
	doi = {10.18653/v1/W17-1601},
	abstract = {Researchers and practitioners in natural-language processing (NLP) and related fields should attend to ethical principles in study design, ascription of categories/variables to study participants, and reporting of findings or results. This paper discusses theoretical and ethical frameworks for using gender as a variable in NLP studies and proposes four guidelines for researchers and practitioners. The principles outlined here should guide practitioners, researchers, and peer reviewers, and they may be applicable to other social categories, such as race, applied to human beings connected to NLP research.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Larson, Brian},
	editor = {Hovy, Dirk and Spruit, Shannon and Mitchell, Margaret and Bender, Emily M. and Strube, Michael and Wallach, Hanna},
	month = apr,
	year = {2017},
	pages = {1--11},
}

@inproceedings{prabhumoye_case_2021,
	address = {Online},
	title = {Case {Study}: {Deontological} {Ethics} in {NLP}},
	shorttitle = {Case {Study}},
	url = {https://aclanthology.org/2021.naacl-main.297},
	doi = {10.18653/v1/2021.naacl-main.297},
	abstract = {Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Prabhumoye, Shrimai and Boldt, Brendon and Salakhutdinov, Ruslan and Black, Alan W},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {3784--3798},
}

@inproceedings{shmueli_beyond_2021,
	address = {Online},
	title = {Beyond {Fair} {Pay}: {Ethical} {Implications} of {NLP} {Crowdsourcing}},
	shorttitle = {Beyond {Fair} {Pay}},
	url = {https://aclanthology.org/2021.naacl-main.295},
	doi = {10.18653/v1/2021.naacl-main.295},
	abstract = {The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers.},
	urldate = {2022-11-28},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Shmueli, Boaz and Fell, Jan and Ray, Soumya and Ku, Lun-Wei},
	month = jun,
	year = {2021},
	pages = {3758--3769},
}

@inproceedings{schwartz_primum_2022,
	address = {Dublin, Ireland},
	title = {Primum {Non} {Nocere}: {Before} working with {Indigenous} data, the {ACL} must confront ongoing colonialism},
	shorttitle = {Primum {Non} {Nocere}},
	url = {https://aclanthology.org/2022.acl-short.82},
	doi = {10.18653/v1/2022.acl-short.82},
	abstract = {In this paper, we challenge the ACL community to reckon with historical and ongoing colonialism by adopting a set of ethical obligations and best practices drawn from the Indigenous studies literature. While the vast majority of NLP research focuses on a very small number of very high resource languages (English, Chinese, etc), some work has begun to engage with Indigenous languages. No research involving Indigenous language data can be considered ethical without first acknowledging that Indigenous languages are not merely very low resource languages. The toxic legacy of colonialism permeates every aspect of interaction between Indigenous communities and outside researchers. To this end, we propose that the ACL draft and adopt an ethical framework for NLP researchers and computational linguists wishing to engage in research involving Indigenous languages.},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Schwartz, Lane},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {724--731},
}

@inproceedings{mohammad_ethics_2022,
	address = {Dublin, Ireland},
	title = {Ethics {Sheets} for {AI} {Tasks}},
	url = {https://aclanthology.org/2022.acl-long.573},
	doi = {10.18653/v1/2022.acl-long.573},
	abstract = {Several high-profile events, such as the mass testing of emotion recognition systems on vulnerable sub-populations and using question answering systems to make moral judgments, have highlighted how technology will often lead to more adverse outcomes for those that are already marginalized. At issue here are not just individual systems and datasets, but also the AI tasks themselves. In this position paper, I make a case for thinking about ethical considerations not just at the level of individual models and datasets, but also at the level of AI tasks. I will present a new form of such an effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the assumptions and ethical considerations hidden in how a task is commonly framed and in the choices we make regarding the data, method, and evaluation. I will also present a template for ethics sheets with 50 ethical considerations, using the task of emotion recognition as a running example. Ethics sheets are a mechanism to engage with and document ethical considerations before building datasets and systems. Similar to survey articles, a small number of carefully created ethics sheets can serve numerous researchers and developers.},
	urldate = {2024-02-16},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Mohammad, Saif},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {8368--8379},
}

@misc{agnew_illusion_2024,
	title = {The illusion of artificial inclusion},
	url = {http://arxiv.org/abs/2401.08572},
	doi = {10.1145/3613904.3642703},
	abstract = {Human participants play a central role in the development of modern artificial intelligence (AI) technology, in psychological science, and in user research. Recent advances in generative AI have attracted growing interest to the possibility of replacing human participants in these domains with AI surrogates. We survey several such "substitution proposals" to better understand the arguments for and against substituting human participants with modern generative AI. Our scoping review indicates that the recent wave of these proposals is motivated by goals such as reducing the costs of research and development work and increasing the diversity of collected data. However, these proposals ignore and ultimately conflict with foundational values of work with human participants: representation, inclusion, and understanding. This paper critically examines the principles and goals underlying human participation to help chart out paths for future work that truly centers and empowers participants.},
	urldate = {2024-02-16},
	author = {Agnew, William and Bergman, A. Stevie and Chien, Jennifer and Díaz, Mark and El-Sayed, Seliem and Pittman, Jaylen and Mohamed, Shakir and McKee, Kevin R.},
	month = feb,
	year = {2024},
	note = {arXiv:2401.08572 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@misc{gupta_calm_2024,
	title = {{CALM} : {A} {Multi}-task {Benchmark} for {Comprehensive} {Assessment} of {Language} {Model} {Bias}},
	shorttitle = {{CALM}},
	url = {http://arxiv.org/abs/2308.12539},
	abstract = {As language models (LMs) become increasingly powerful and widely used, it is important to quantify them for sociodemographic bias with potential for harm. Prior measures of bias are sensitive to perturbations in the templates designed to compare performance across social groups, due to factors such as low diversity or limited number of templates. Also, most previous work considers only one NLP task. We introduce Comprehensive Assessment of Language Models (CALM) for robust measurement of two types of universally relevant sociodemographic bias, gender and race. CALM integrates sixteen datasets for question-answering, sentiment analysis and natural language inference. Examples from each dataset are filtered to produce 224 templates with high diversity (e.g., length, vocabulary). We assemble 50 highly frequent person names for each of seven distinct demographic groups to generate 78,400 prompts covering the three NLP tasks. Our empirical evaluation shows that CALM bias scores are more robust and far less sensitive than previous bias measurements to perturbations in the templates, such as synonym substitution, or to random subset selection of templates. We apply CALM to 20 large language models, and find that for 2 language model series, larger parameter models tend to be more biased than smaller ones. The T0 series is the least biased model families, of the 20 LLMs investigated here. The code is available at https://github.com/vipulgupta1011/CALM.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Gupta, Vipul and Venkit, Pranav Narayanan and Laurençon, Hugo and Wilson, Shomir and Passonneau, Rebecca J.},
	month = jan,
	year = {2024},
	note = {arXiv:2308.12539 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{nozza_pipelines_2022,
	address = {virtual+Dublin},
	title = {Pipelines for {Social} {Bias} {Testing} of {Large} {Language} {Models}},
	url = {https://aclanthology.org/2022.bigscience-1.6},
	abstract = {The maturity level of language models is now at a stage in which many companies rely on them to solve various tasks. However, while research has shown how biased and harmful these models are, systematic ways of integrating social bias tests into development pipelines are still lacking. This short paper suggests how to use these verification techniques in development pipelines. We take inspiration from software testing and suggest addressing social bias evaluation as software testing. We hope to open a discussion on the best methodologies to handle social bias testing in language models.},
	urldate = {2022-05-30},
	booktitle = {Proceedings of {BigScience} {Episode} \#5 – {Workshop} on {Challenges} \& {Perspectives} in {Creating} {Large} {Language} {Models}},
	publisher = {Association for Computational Linguistics},
	author = {Nozza, Debora and Bianchi, Federico and Hovy, Dirk},
	month = may,
	year = {2022},
	pages = {68--74},
}

@inproceedings{xu_detoxifying_2021,
	address = {Online},
	title = {Detoxifying {Language} {Models} {Risks} {Marginalizing} {Minority} {Voices}},
	url = {https://aclanthology.org/2021.naacl-main.190},
	doi = {10.18653/v1/2021.naacl-main.190},
	abstract = {Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.},
	urldate = {2024-02-16},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {2390--2397},
}

@article{ung_saferdialogues_2022,
	title = {{SaFeRDialogues}: {Taking} {Feedback} {Gracefully} after {Conversational} {Safety} {Failures}},
	shorttitle = {{SaFeRDialogues}},
	url = {https://www.semanticscholar.org/reader/0794333779d775abc2053052d1e7009066cbd4f1},
	doi = {10.18653/v1/2022.acl-long.447},
	abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
	language = {en},
	urldate = {2022-10-05},
	journal = {undefined},
	author = {Ung, Megan and Xu, Jing and Boureau, Y.-Lan},
	year = {2022},
}

@inproceedings{dinan_safetykit_2022,
	address = {Dublin, Ireland},
	title = {{SafetyKit}: {First} {Aid} for {Measuring} {Safety} in {Open}-domain {Conversational} {Systems}},
	shorttitle = {{SafetyKit}},
	url = {https://aclanthology.org/2022.acl-long.284},
	doi = {10.18653/v1/2022.acl-long.284},
	abstract = {The social impact of natural language processing and its applications has received increasing attention. In this position paper, we focus on the problem of safety for end-to-end conversational AI. We survey the problem landscape therein, introducing a taxonomy of three observed phenomena: the Instigator, Yea-Sayer, and Impostor effects. We then empirically assess the extent to which current tools can measure these effects and current systems display them. We release these tools as part of a “first aid kit” (SafetyKit) to quickly assess apparent safety concerns. Our results show that, while current tools are able to provide an estimate of the relative safety of systems in various settings, they still have several shortcomings. We suggest several future directions and discuss ethical considerations.},
	urldate = {2023-02-17},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dinan, Emily and Abercrombie, Gavin and Bergman, A. and Spruit, Shannon and Hovy, Dirk and Boureau, Y-Lan and Rieser, Verena},
	month = may,
	year = {2022},
	pages = {4113--4133},
}

@misc{smart_discipline_2024,
	title = {Discipline and {Label}: {A} {WEIRD} {Genealogy} and {Social} {Theory} of {Data} {Annotation}},
	shorttitle = {Discipline and {Label}},
	url = {http://arxiv.org/abs/2402.06811},
	doi = {10.48550/arXiv.2402.06811},
	abstract = {Data annotation remains the sine qua non of machine learning and AI. Recent empirical work on data annotation has begun to highlight the importance of rater diversity for fairness, model performance, and new lines of research have begun to examine the working conditions for data annotation workers, the impacts and role of annotator subjectivity on labels, and the potential psychological harms from aspects of annotation work. This paper outlines a critical genealogy of data annotation; starting with its psychological and perceptual aspects. We draw on similarities with critiques of the rise of computerized lab-based psychological experiments in the 1970's which question whether these experiments permit the generalization of results beyond the laboratory settings within which these results are typically obtained. Do data annotations permit the generalization of results beyond the settings, or locations, in which they were obtained? Psychology is overly reliant on participants from Western, Educated, Industrialized, Rich, and Democratic societies (WEIRD). Many of the people who work as data annotation platform workers, however, are not from WEIRD countries; most data annotation workers are based in Global South countries. Social categorizations and classifications from WEIRD countries are imposed on non-WEIRD annotators through instructions and tasks, and through them, on data, which is then used to train or evaluate AI models in WEIRD countries. We synthesize evidence from several recent lines of research and argue that data annotation is a form of automated social categorization that risks entrenching outdated and static social categories that are in reality dynamic and changing. We propose a framework for understanding the interplay of the global social conditions of data annotation with the subjective phenomenological experience of data annotation work.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Smart, Andrew and Wang, Ding and Monk, Ellis and Díaz, Mark and Kasirzadeh, Atoosa and Van Liemt, Erin and Schmer-Galunder, Sonja},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06811 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{steiger_psychological_2021,
	address = {Yokohama Japan},
	title = {The {Psychological} {Well}-{Being} of {Content} {Moderators}: {The} {Emotional} {Labor} of {Commercial} {Moderation} and {Avenues} for {Improving} {Support}},
	isbn = {978-1-4503-8096-6},
	shorttitle = {The {Psychological} {Well}-{Being} of {Content} {Moderators}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445092},
	doi = {10.1145/3411764.3445092},
	language = {en},
	urldate = {2024-02-16},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Steiger, Miriah and Bharucha, Timir J and Venkatagiri, Sukrit and Riedl, Martin J. and Lease, Matthew},
	month = may,
	year = {2021},
	pages = {1--14},
}

@article{huws_online_2015,
	title = {Online labour exchanges or 'crowdsourcing': {Implications} for occupational safety and health},
	shorttitle = {Online labour exchanges or 'crowdsourcing'},
	url = {https://researchprofiles.herts.ac.uk/en/publications/online-labour-exchanges-or-crowdsourcing-implications-for-occupat-2},
	language = {English},
	urldate = {2024-02-16},
	author = {Huws, Ursula},
	year = {2015},
	note = {Publisher: European Agency for Safety and Health at Work (EU-OSHA)},
}

@article{lacroix_metaethical_2022,
	title = {Metaethical {Perspectives} on '{Benchmarking}' {AI} {Ethics}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2204.05151},
	doi = {10.48550/ARXIV.2204.05151},
	abstract = {Benchmarks are seen as the cornerstone for measuring technical progress in Artificial Intelligence (AI) research and have been developed for a variety of tasks ranging from question answering to facial recognition. An increasingly prominent research area in AI is ethics, which currently has no set of benchmarks nor commonly accepted way for measuring the 'ethicality' of an AI system. In this paper, drawing upon research in moral philosophy and metaethics, we argue that it is impossible to develop such a benchmark. As such, alternative mechanisms are necessary for evaluating whether an AI system is 'ethical'. This is especially pressing in light of the prevalence of applied, industrial AI research. We argue that it makes more sense to talk about 'values' (and 'value alignment') rather than 'ethics' when considering the possible actions of present and future AI systems. We further highlight that, because values are unambiguously relative, focusing on values forces us to consider explicitly what the values are and whose values they are. Shifting the emphasis from ethics to values therefore gives rise to several new ways of understanding how researchers might advance research programmes for robustly safe or beneficial AI. We conclude by highlighting a number of possible ways forward for the field as a whole, and we advocate for different approaches towards more value-aligned AI research.},
	urldate = {2023-02-13},
	author = {LaCroix, Travis and Luccioni, Alexandra Sasha},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{perrigo_openai_2023,
	title = {{OpenAI} {Used} {Kenyan} {Workers} on {Less} {Than} \$2 {Per} {Hour}: {Exclusive} {\textbar} {Time}},
	url = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
	urldate = {2023-03-09},
	journal = {Time},
	author = {Perrigo, Billy},
	month = jan,
	year = {2023},
}

@inproceedings{jernite_data_2022,
	title = {Data {Governance} in the {Age} of {Large}-{Scale} {Data}-{Driven} {Language} {Technology}},
	url = {http://arxiv.org/abs/2206.03216},
	doi = {10.1145/3531146.3534637},
	abstract = {The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.},
	urldate = {2023-02-20},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Dupont, Gérard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Johnson, Isaac and Radev, Dragomir and Nikpoor, Somaieh and Frohberg, Jörg and Gokaslan, Aaron and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03216 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	pages = {2206--2222},
}

@inproceedings{luccioni_whats_2021,
	address = {Online},
	title = {What's in the {Box}? {An} {Analysis} of {Undesirable} {Content} in the {Common} {Crawl} {Corpus}},
	shorttitle = {What's in the {Box}?},
	url = {https://aclanthology.org/2021.acl-short.24},
	doi = {10.18653/v1/2021.acl-short.24},
	abstract = {Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit content, even after filtering procedures. We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis.},
	urldate = {2021-10-19},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Luccioni, Alexandra and Viviano, Joseph},
	month = aug,
	year = {2021},
	pages = {182--189},
}

@article{luccioni_estimating_2023,
	title = {Estimating the carbon footprint of bloom, a 176b parameter language model},
	volume = {24},
	url = {https://www.jmlr.org/papers/v24/23-0069.html},
	number = {253},
	urldate = {2024-02-09},
	journal = {Journal of Machine Learning Research},
	author = {Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
	year = {2023},
	pages = {1--15},
}

@inproceedings{dodge_measuring_2022,
	address = {Seoul Republic of Korea},
	title = {Measuring the {Carbon} {Intensity} of {AI} in {Cloud} {Instances}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533234},
	doi = {10.1145/3531146.3533234},
	language = {en},
	urldate = {2024-02-09},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Dodge, Jesse and Prewitt, Taylor and Tachet Des Combes, Remi and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A. and DeCario, Nicole and Buchanan, Will},
	month = jun,
	year = {2022},
	pages = {1877--1894},
}

@article{lacoste_quantifying_2019,
	title = {Quantifying the carbon emissions of machine learning},
	url = {https://scholar.google.com/scholar?cluster=12177325897378617879&hl=en&oi=scholarr},
	urldate = {2024-02-09},
	journal = {arXiv preprint arXiv:1910.09700},
	author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
	year = {2019},
}

@article{zisser_you_2020,
	title = {You want a hot body? {You} want a {Bugatti}? {You} better work(out): {FitBit}, neoliberalism, and the thin ideal},
	volume = {5},
	copyright = {Copyright (c) 2020 Katharine Zisser},
	issn = {2561-7397},
	shorttitle = {You want a hot body?},
	url = {https://theijournal.ca/index.php/ijournal/article/view/34467},
	doi = {10.33137/ijournal.v5i2.34467},
	abstract = {The Fitbit manifests an ideology of healthism that prioritizes the pursuit of physical health above all else. The device’s design, use, and underlying epistemic frameworks transform exercise into data, labour, and knowledge, respectively. Using\&nbsp;an accelerometer and green LED lights, the Fitbit translates the movements of human bodies into data. ‘Exercise’ is thus limited to what can be mechanically registered and algorithmically sorted into a pre-set category. This freely generated user data is aggregated into profitable datasets that Fitbit can sell to advertisers. Fitbit’s partnerships with insurers or employers further exploit workers by penalizing non-participants and users who generate undesirable data. Finally, the practice of activity tracking frames exercise as a health intervention and restricts the possibility of being absent from one’s body. Furthermore, Fitbit understands fitness through the lens of weight management, where the fit body is a conspicuously self-disciplined (read: thin) body.\&nbsp;By framing fitness as a choice, individuals are held\&nbsp;personally responsible for health outcomes, and being ‘unfit’ reflects a physical and moral failure. The insights produced by Fitbit thus restrain and shape users’ self-knowledge, perpetuating a cultural norm that understands ‘fit’ bodies as healthy, productive, and morally good.},
	language = {en},
	number = {2},
	urldate = {2022-07-25},
	journal = {The iJournal: Student Journal of the Faculty of Information},
	author = {Zisser, Katharine},
	month = jun,
	year = {2020},
	note = {Number: 2},
	keywords = {fitness culture, neoliberalism, political economy, self-tracking},
}

@article{beer_problem_2022,
	title = {The problem of researching a recursive society: {Algorithms}, data coils and the looping of the social},
	volume = {9},
	issn = {2053-9517},
	shorttitle = {The problem of researching a recursive society},
	url = {https://doi.org/10.1177/20539517221104997},
	doi = {10.1177/20539517221104997},
	abstract = {This commentary article outlines and explores the key problem that faces anyone interested in researching and understanding what might be thought of as a recursive society. It reflects on the problem that is posed by the layering of multiple feedback loops as a result of algorithmic sorting and data processes. This article is concerned with the difficulties of understanding the social where recursive algorithmic processes have repeatedly shaped outcomes, practices, relations and actions over time. This is not just about the sinking of algorithms into the everyday, it is about the way that loop-upon-loop of data processes lead to the social world itself being recursive. This repeated looping is described here as a kind of data coiling. The article argues for a focus on recursivity and for an engagement with the conceptual problems and questions that this notion implies.},
	language = {en},
	number = {2},
	urldate = {2024-02-09},
	journal = {Big Data \& Society},
	author = {Beer, David},
	month = jul,
	year = {2022},
	note = {Publisher: SAGE Publications Ltd},
	pages = {20539517221104997},
}

@article{mitchell_model_2019,
	title = {Model {Cards} for {Model} {Reporting}},
	url = {http://arxiv.org/abs/1810.03993},
	doi = {10.1145/3287560.3287596},
	abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
	language = {en},
	urldate = {2021-04-13},
	journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
	author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
	month = jan,
	year = {2019},
	note = {arXiv: 1810.03993},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {220--229},
}

@inproceedings{buolamwini_gender_2018,
	title = {Gender {Shades}: {Intersectional} {Accuracy} {Disparities} in {Commercial} {Gender} {Classification}},
	shorttitle = {Gender {Shades}},
	url = {https://proceedings.mlr.press/v81/buolamwini18a.html},
	language = {en},
	urldate = {2021-09-23},
	booktitle = {Conference on {Fairness}, {Accountability} and {Transparency}},
	publisher = {PMLR},
	author = {Buolamwini, Joy and Gebru, Timnit},
	month = jan,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {77--91},
}

@techreport{smith_im_2022,
	title = {"{I}'m sorry to hear that": finding bias in language models with a holistic descriptor dataset},
	shorttitle = {"{I}'m sorry to hear that"},
	url = {http://arxiv.org/abs/2205.09209},
	abstract = {As language models grow in popularity, their biases across all possible markers of demographic identity should be measured and addressed in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes, and are commonly used with preset bias tests that presuppose which types of biases the models exhibit. In this work, we present a new, more inclusive dataset, HOLISTICBIAS, which consists of nearly 600 descriptor terms across 13 different demographic axes. HOLISTICBIAS was assembled in conversation with experts and community members with lived experience through a participatory process. We use these descriptors combinatorially in a set of bias measurement templates to produce over 450,000 unique sentence prompts, and we use these prompts to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that our dataset is highly efficacious for measuring previously unmeasurable biases in token likelihoods and generations from language models, as well as in an offensiveness classifier. We will invite additions and amendments to the dataset, and we hope it will help serve as a basis for easy-to-use and more standardized methods for evaluating bias in NLP models.},
	number = {arXiv:2205.09209},
	urldate = {2022-05-30},
	institution = {arXiv},
	author = {Smith, Eric Michael and Kambadur, Melissa Hall Melanie and Presani, Eleonora and Williams, Adina},
	month = may,
	year = {2022},
	note = {arXiv:2205.09209 [cs]
version: 1
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@article{weidinger_ethical_2021,
	title = {Ethical and social risks of harm from {Language} {Models}},
	url = {https://www.semanticscholar.org/reader/fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf},
	abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
	language = {en},
	urldate = {2022-10-05},
	journal = {undefined},
	author = {Weidinger, Laura and Mellor, John F. J. and Rauh, M. and Griffin, C. and Uesato, J. and Huang, Po-Sen and Cheng, M. and Glaese, Mia and Balle, B. and Kasirzadeh, A. and Kenton, Z. and Brown, S. and Hawkins, W. and Stepleton, T. and Biles, C. and Birhane, A. and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William S. and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	year = {2021},
}

@misc{liang_reflexivity_2021,
	title = {Reflexivity, positionality, and disclosure in {HCI}},
	url = {https://medium.com/@caliang/reflexivity-positionality-and-disclosure-in-hci-3d95007e9916},
	abstract = {Are you an HCI researcher thinking about including a positionality statement? Here are some thoughts.},
	language = {en},
	urldate = {2022-11-23},
	journal = {Medium},
	author = {Liang, Calvin},
	month = sep,
	year = {2021},
}

@misc{fussell_how_2019,
	title = {How an {Attempt} at {Correcting} {Bias} in {Tech} {Goes} {Wrong}},
	url = {https://www.theatlantic.com/technology/archive/2019/10/google-allegedly-used-homeless-train-pixel-phone/599668/},
	abstract = {Google allegedly scanned volunteers with dark skin tones in order to perfect the Pixel phone’s face-unlock technology.},
	language = {en},
	urldate = {2023-03-28},
	journal = {The Atlantic},
	author = {Fussell, Sidney},
	month = oct,
	year = {2019},
	note = {Section: Technology},
}

@inproceedings{zilka_transparency_2022,
	address = {Oxford United Kingdom},
	title = {Transparency, {Governance} and {Regulation} of {Algorithmic} {Tools} {Deployed} in the {Criminal} {Justice} {System}: a {UK} {Case} {Study}},
	isbn = {978-1-4503-9247-1},
	shorttitle = {Transparency, {Governance} and {Regulation} of {Algorithmic} {Tools} {Deployed} in the {Criminal} {Justice} {System}},
	url = {https://dl.acm.org/doi/10.1145/3514094.3534200},
	doi = {10.1145/3514094.3534200},
	language = {en},
	urldate = {2023-11-02},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Zilka, Miri and Sargeant, Holli and Weller, Adrian},
	month = jul,
	year = {2022},
	pages = {880--889},
}

@inproceedings{bird_typology_2023,
	address = {New York, NY, USA},
	series = {{AIES} '23},
	title = {Typology of {Risks} of {Generative} {Text}-to-{Image} {Models}},
	isbn = {9798400702310},
	url = {https://dl.acm.org/doi/10.1145/3600211.3604722},
	doi = {10.1145/3600211.3604722},
	abstract = {This paper investigates the direct risks and harms associated with modern text-to-image generative models, such as DALL-E and Midjourney, through a comprehensive literature review. While these models offer unprecedented capabilities for generating images, their development and use introduce new types of risk that require careful consideration. Our review reveals significant knowledge gaps concerning the understanding and treatment of these risks despite some already being addressed. We offer a taxonomy of risks across six key stakeholder groups, inclusive of unexplored issues, and suggest future research directions. We identify 22 distinct risk types, spanning issues from data bias to malicious use. The investigation presented here is intended to enhance the ongoing discourse on responsible model development and deployment. By highlighting previously overlooked risks and gaps, it aims to shape subsequent research and governance initiatives, guiding them toward the responsible, secure, and ethically conscious evolution of text-to-image models.},
	urldate = {2023-11-09},
	booktitle = {Proceedings of the 2023 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Bird, Charlotte and Ungless, Eddie and Kasirzadeh, Atoosa},
	month = aug,
	year = {2023},
	keywords = {AI ethics, AI governance, AI risks, AI safety, Generative AI, Generative models, Responsible AI, Text-to-Image models},
	pages = {396--410},
}

@inproceedings{ehsan_algorithmic_2022,
	address = {New York, NY, USA},
	series = {{FAccT} '22},
	title = {The {Algorithmic} {Imprint}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533186},
	doi = {10.1145/3531146.3533186},
	abstract = {When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, just because an algorithm is removed does not imply its FATE-related issues cease to exist. In this paper, we introduce the notion of the “algorithmic imprint” to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We operationalize this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students’, teachers’, and parents’ lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of “what” happened in Bangladesh, contextualizing “why” and “how” they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, social, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.},
	urldate = {2024-01-26},
	booktitle = {Proceedings of the 2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Ehsan, Upol and Singh, Ranjit and Metcalf, Jacob and Riedl, Mark},
	month = jun,
	year = {2022},
	keywords = {Algorithmic Impact Assessment, Algorithmic Imprint, Folk Theories of Algorithms, Global South, Infrastructure, Situated Fairness, User Perceptions},
	pages = {1305--1317},
}

@article{walter_indigenous_2021,
	title = {Indigenous {Data} {Sovereignty} in the {Era} of {Big} {Data} and {Open} {Data}},
	volume = {56},
	issn = {1839-4655},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ajs4.141},
	doi = {10.1002/ajs4.141},
	abstract = {Indigenous Data Sovereignty, in its proclamation of the right of Indigenous peoples to govern the collection, ownership, and application of data, recognises data as a cultural and economic asset. The impact of data is magnified by the emergence of Big Data and the associated impetus to open publicly held data (Open Data). Aboriginal and Torres Strait Islander peoples, families and communities, heavily overrepresented in social disadvantage–related data will also be overrepresented in the application of these new technologies, but in a data landscape, Indigenous peoples remain largely alienated from the use of data and its utilization within the channels of policy power. Existing data infrastructure, and the emerging Open Data infrastructure, neither recognise Indigenous agency and worldviews nor consider Indigenous data needs. This is demonstrated in the absence of any consideration of Indigenous data issues in Open Data discussions and publication. Thus, while the potential benefits of this data revolution are trumpeted, our marginalised social, cultural and political location suggests we will not share equally in these benefits. This paper discusses the unforeseen (and likely unseen) consequences of the influence of Open Data and Big Data and discusses how Indigenous Data Sovereignty can mediate risks while providing pathways to collective benefits.},
	language = {en},
	number = {2},
	urldate = {2023-05-16},
	journal = {Australian Journal of Social Issues},
	author = {Walter, Maggie and Lovett, Raymond and Maher, Bobby and Williamson, Bhiamie and Prehn, Jacob and Bodkin-Andrews, Gawaian and Lee, Vanessa},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ajs4.141},
	keywords = {Aboriginal people, accountability, partnerships, policy, productivity},
	pages = {143--156},
}

@article{udupa_ethical_2023,
	title = {Ethical scaling for content moderation: {Extreme} speech and the (in)significance  of artificial intelligence},
	volume = {10},
	issn = {2053-9517},
	shorttitle = {Ethical scaling for content moderation},
	url = {https://doi.org/10.1177/20539517231172424},
	doi = {10.1177/20539517231172424},
	abstract = {In this article, we present new empirical evidence to demonstrate the severe limitations of existing machine learning content moderation methods to keep pace with, let alone stay ahead of, hateful language online. Building on the collaborative coding project “AI4Digntiy,” we outline the ambiguities and complexities of annotating problematic text in AI-assisted moderation systems. We diagnose the shortcomings of the content moderation and natural language processing approach as emerging from a broader epistemological trapping wrapped in the liberal-modern idea of “the human.” Presenting a decolonial critique of the “human vs machine” conundrum and drawing attention to the structuring effects of coloniality on extreme speech, we propose “ethical scaling” to highlight moderation process as political praxis. As a normative framework for platform governance, ethical scaling calls for a transparent, reflexive, and replicable process of iteration for content moderation with community participation and global parity, which should evolve in conjunction with addressing algorithmic amplification of divisive content and resource allocation for content moderation.},
	language = {en},
	number = {1},
	urldate = {2023-05-15},
	journal = {Big Data \& Society},
	author = {Udupa, Sahana and Maronikolakis, Antonis and Wisiorek, Axel},
	month = jan,
	year = {2023},
	note = {Publisher: SAGE Publications Ltd},
	pages = {20539517231172424},
}

@inproceedings{excell_towards_2021,
	address = {Online},
	title = {Towards {Equal} {Gender} {Representation} in the {Annotations} of {Toxic} {Language} {Detection}},
	url = {https://aclanthology.org/2021.gebnlp-1.7},
	doi = {10.18653/v1/2021.gebnlp-1.7},
	abstract = {Classifiers tend to propagate biases present in the data on which they are trained. Hence, it is important to understand how the demographic identities of the annotators of comments affect the fairness of the resulting model. In this paper, we focus on the differences in the ways men and women annotate comments for toxicity, investigating how these differences result in models that amplify the opinions of male annotators. We find that the BERT model associates toxic comments containing offensive words with male annotators, causing the model to predict 67.7\% of toxic comments as having been annotated by men. We show that this disparity between gender predictions can be mitigated by removing offensive words and highly toxic comments from the training data. We then apply the learned associations between gender and language to toxic language classifiers, finding that models trained exclusively on female-annotated data perform 1.8\% better than those trained solely on male-annotated data, and that training models on data after removing all offensive words reduces bias in the model by 55.5\% while increasing the sensitivity by 0.4\%.},
	urldate = {2024-02-09},
	booktitle = {Proceedings of the 3rd {Workshop} on {Gender} {Bias} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Excell, Elizabeth and Al Moubayed, Noura},
	editor = {Costa-jussa, Marta and Gonen, Hila and Hardmeier, Christian and Webster, Kellie},
	month = aug,
	year = {2021},
	pages = {55--65},
}

@inproceedings{steed_upstream_2022,
	title = {Upstream {Mitigation} {Is}   {Not}  {All} {You} {Need}: {Testing} the {Bias} {Transfer} {Hypothesis} in {Pre}-{Trained} {Language} {Models}},
	shorttitle = {Upstream {Mitigation} {Is}   {Not}  {All} {You} {Need}},
	doi = {10.18653/v1/2022.acl-long.247},
	abstract = {The bias transfer hypothesis is investigated: the theory that social biases internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier’s discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.},
	booktitle = {{ACL}},
	author = {Steed, Ryan and Panda, Swetasudha and Kobren, Ari and Wick, Michael L.},
	year = {2022},
}

@inproceedings{goldfarb-tarrant_intrinsic_2021,
	address = {Online},
	title = {Intrinsic {Bias} {Metrics} {Do} {Not} {Correlate} with {Application} {Bias}},
	url = {https://aclanthology.org/2021.acl-long.150},
	doi = {10.18653/v1/2021.acl-long.150},
	abstract = {Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Goldfarb-Tarrant, Seraphina and Marchant, Rebecca and Muñoz Sánchez, Ricardo and Pandya, Mugdha and Lopez, Adam},
	month = aug,
	year = {2021},
	pages = {1926--1940},
}

@inproceedings{cao_intrinsic_2022,
	address = {Dublin, Ireland},
	title = {On the {Intrinsic} and {Extrinsic} {Fairness} {Evaluation} {Metrics} for {Contextualized} {Language} {Representations}},
	url = {https://aclanthology.org/2022.acl-short.62},
	doi = {10.18653/v1/2022.acl-short.62},
	abstract = {Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.},
	language = {en},
	urldate = {2022-07-28},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Yang and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul and Kumar, Varun and Dhamala, Jwala and Galstyan, Aram},
	year = {2022},
	pages = {561--570},
}

@inproceedings{selbst_fairness_2019,
	address = {New York, NY, USA},
	series = {{FAT}* '19},
	title = {Fairness and {Abstraction} in {Sociotechnical} {Systems}},
	isbn = {978-1-4503-6125-5},
	url = {https://doi.org/10.1145/3287560.3287598},
	doi = {10.1145/3287560.3287598},
	abstract = {A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce "fair" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five "traps" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.},
	urldate = {2022-05-03},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Selbst, Andrew D. and Boyd, Danah and Friedler, Sorelle A. and Venkatasubramanian, Suresh and Vertesi, Janet},
	month = jan,
	year = {2019},
	keywords = {Fairness-aware Machine Learning, Interdisciplinary, Sociotechnical Systems},
	pages = {59--68},
}

@article{araujo_ai_2020,
	title = {In {AI} we trust? {Perceptions} about automated decision-making by artificial intelligence},
	abstract = {Fueled by ever-growing amounts of (digital) data and advances in artificial intelligence, decision-making in contemporary societies is increasingly delegated to automated processes. Drawing from social science theories and from the emerging body of research about algorithmic appreciation and algorithmic perceptions, the current study explores the extent to which personal characteristics can be linked to perceptions of automated decision-making by AI, and the boundary conditions of these perceptions, namely the extent to which such perceptions differ across media, (public) health, and judicial contexts. Data from a scenario-based survey experiment with a national sample (N = 958) show that people are by and large concerned about risks and have mixed opinions about fairness and usefulness of automated decision-making at a societal level, with general attitudes influenced by individual characteristics. Interestingly, decisions taken automatically by AI were often evaluated on par or even better than human experts for specific decisions. Theoretical and societal implications about these findings are discussed.},
	language = {en},
	number = {35},
	journal = {AI \& SOCIETY},
	author = {Araujo, Theo},
	year = {2020},
	pages = {13},
}

@article{shneiderman_human-centered_2020,
	title = {Human-{Centered} {Artificial} {Intelligence}: {Reliable}, {Safe} \& {Trustworthy}},
	volume = {36},
	issn = {1044-7318},
	shorttitle = {Human-{Centered} {Artificial} {Intelligence}},
	url = {https://doi.org/10.1080/10447318.2020.1741118},
	doi = {10.1080/10447318.2020.1741118},
	abstract = {Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance, leading to wider adoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies how to (1) design for high levels of human control and high levels of computer automation so as to increase human performance, (2) understand the situations in which full human control or full computer control are necessary, and (3) avoid the dangers of excessive human control or excessive computer control. The methods of HCAI are more likely to produce designs that are Reliable, Safe \& Trustworthy (RST). Achieving these goals will dramatically increase human performance, while supporting human self-efficacy, mastery, creativity, and responsibility.},
	number = {6},
	urldate = {2021-09-14},
	journal = {International Journal of Human–Computer Interaction},
	author = {Shneiderman, Ben},
	month = apr,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2020.1741118},
	keywords = {automation, human control, reliable},
	pages = {495--504},
}

@article{lee_understanding_2018,
	title = {Understanding perception of algorithmic decisions: {Fairness}, trust, and emotion in response to algorithmic management},
	volume = {5},
	issn = {2053-9517},
	shorttitle = {Understanding perception of algorithmic decisions},
	url = {https://doi.org/10.1177/2053951718756684},
	doi = {10.1177/2053951718756684},
	abstract = {Algorithms increasingly make managerial decisions that people used to make. Perceptions of algorithms, regardless of the algorithms' actual performance, can significantly influence their adoption, yet we do not fully understand how people perceive decisions made by algorithms as compared with decisions made by humans. To explore perceptions of algorithmic management, we conducted an online experiment using four managerial decisions that required either mechanical or human skills. We manipulated the decision-maker (algorithmic or human), and measured perceived fairness, trust, and emotional response. With the mechanical tasks, algorithmic and human-made decisions were perceived as equally fair and trustworthy and evoked similar emotions; however, human managers' fairness and trustworthiness were attributed to the manager's authority, whereas algorithms' fairness and trustworthiness were attributed to their perceived efficiency and objectivity. Human decisions evoked some positive emotion due to the possibility of social recognition, whereas algorithmic decisions generated a more mixed response ? algorithms were seen as helpful tools but also possible tracking mechanisms. With the human tasks, algorithmic decisions were perceived as less fair and trustworthy and evoked more negative emotion than human decisions. Algorithms' perceived lack of intuition and subjective judgment capabilities contributed to the lower fairness and trustworthiness judgments. Positive emotion from human decisions was attributed to social recognition, while negative emotion from algorithmic decisions was attributed to the dehumanizing experience of being evaluated by machines. This work reveals people's lay concepts of algorithmic versus human decisions in a management context and suggests that task characteristics matter in understanding people's experiences with algorithmic technologies.},
	number = {1},
	urldate = {2023-05-01},
	journal = {Big Data \& Society},
	author = {Lee, Min Kyung},
	month = jan,
	year = {2018},
	note = {Publisher: SAGE Publications Ltd},
	pages = {2053951718756684},
}

@inproceedings{ungless_stereotypes_2023,
	title = {Stereotypes and {Smut}: {The} ({Mis})representation of {Non}-cisgender {Identities} by {Text}-to-{Image} {Models}},
	shorttitle = {Stereotypes and {Smut}},
	url = {https://www.research.ed.ac.uk/en/publications/stereotypes-and-smut-the-misrepresentation-of-non-cisgender-ident},
	language = {English},
	urldate = {2023-10-25},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics (ACL)},
	author = {Ungless, Eddie and Ross, Björn and Lauscher, Anne},
	month = jul,
	year = {2023},
}

@article{raji_closing_2020,
	title = {Closing the {AI} {Accountability} {Gap}: {Defining} an {End}-to-{End} {Framework} for {Internal} {Algorithmic} {Auditing}},
	abstract = {Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.},
	language = {en},
	author = {Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
	year = {2020},
	pages = {12},
}

@article{guyan_constructing_2021,
	title = {Constructing a queer population? {Asking} about sexual orientation in {Scotland}’s 2022 census},
	volume = {0},
	issn = {0958-9236},
	shorttitle = {Constructing a queer population?},
	url = {https://doi.org/10.1080/09589236.2020.1866513},
	doi = {10.1080/09589236.2020.1866513},
	abstract = {For the first time, Scotland’s 2022 census will ask a question about sexual orientation. Correspondence between National Records of Scotland, the Scottish Parliament’s Culture, Tourism, Europe and External Affairs Committee and campaign groups present insights into decisions made, the uneasy relationship between queer identities and state data collection practices, and the question of who is counted when we count LGBTQ people. Building on Foucault’s critique of projects that construct population knowledge, the census is framed primarily as a tool to facilitate the state’s capacity to govern. My engagement in the design process enabled me to critically examine decisions made about the exclusion of non-binary identities and the use of predictive text technology. These decisions demonstrate how the design process constructed a queer population that ‘made sense’ to the heteronormative majority and ‘designed-out’ queer lives that the state did not wish to bring into being.},
	number = {0},
	urldate = {2021-02-02},
	journal = {Journal of Gender Studies},
	author = {Guyan, Kevin},
	month = jan,
	year = {2021},
	keywords = {Census, data collection, lgbtq, queer, sexual orientation},
	pages = {1--11},
}

@book{kevin_guyan_queer_2022,
	address = {London},
	series = {Bloomsbury {Studies} in {Digital} {Cultures} {Ser}},
	title = {Queer {Data} : {Using} {Gender}, {Sex} and {Sexuality} {Data} for {Action}},
	isbn = {978-1-350-23072-9},
	url = {https://search.ebscohost.com/login.aspx?direct=true&db=nlebk&AN=3077276&site=ehost-live},
	abstract = {Data has never mattered more. Our lives are increasingly shaped by it and how it is defined, collected and used. But who counts in the collection, analysis and application of data?This important book is the first to look at queer data – defined as data relating to gender, sex, sexual orientation and trans identity/history. The author shows us how current data practices reflect an incomplete account of LGBTQ lives and helps us understand how data biases are used to delegitimise the everyday experiences of queer people.Guyan demonstrates why it is important to understand, collect and analyse queer data, the benefits and challenges involved in doing so, and how we might better use queer data in our work. Arming us with the tools for action, this book shows how greater knowledge about queer identities is instrumental in informing decisions about resource allocation, changes to legislation, access to services, representation and visibility.},
	language = {English},
	publisher = {Bloomsbury Academic},
	author = {{Kevin Guyan}},
	year = {2022},
	keywords = {Data mining--Statistical methods, Decision making--Statistical methods, EDUCATION / Inclusive Education, HISTORY / Europe / Great Britain / 21st Century, LITERARY CRITICISM / LGBTQ+, SOCIAL SCIENCE / Gender Studies, SOCIAL SCIENCE / LGBTQ+ Studies / Transgender Studies, SOCIAL SCIENCE / Sociology / General, Sexual minorities--Research, Sexual minorities--Statistics},
}

@book{guyan_count_nodate,
	title = {Count {Me} {In}? {A} {Queer} {Account} of {Gender}, {Sex} and {Sexuality} {Data} in {Scotland}},
	shorttitle = {Count {Me} {In}?},
	url = {https://www.meetup.com/qcscot/events/275449026/},
	abstract = {Tue, Feb 2, 2021, 7:00 PM: In March 2022, Scotland’s census will collect data about people’s sexual orientation and trans status/history for the first time. This landmark moment signals the counting o},
	language = {en},
	urldate = {2021-02-02},
	author = {Guyan, Kevin},
}

@inproceedings{blodgett_language_2020,
	address = {Online},
	title = {Language ({Technology}) is {Power}: {A} {Critical} {Survey} of “{Bias}” in {NLP}},
	shorttitle = {Language ({Technology}) is {Power}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.485},
	doi = {10.18653/v1/2020.acl-main.485},
	abstract = {We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”—i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.},
	urldate = {2020-10-27},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Barocas, Solon and Daumé III, Hal and Wallach, Hanna},
	month = jul,
	year = {2020},
	pages = {5454--5476},
}

@inproceedings{blodgett_stereotyping_2021,
	title = {Stereotyping {Norwegian} {Salmon}: {An} {Inventory} of {Pitfalls} in {Fairness} {Benchmark} {Datasets}},
	shorttitle = {Stereotyping {Norwegian} {Salmon}},
	url = {https://www.microsoft.com/en-us/research/publication/stereotyping-norwegian-salmon-an-inventory-of-pitfalls-in-fairness-benchmark-datasets/},
	abstract = {Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system’s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP […]},
	language = {en-US},
	urldate = {2021-07-13},
	author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
	month = aug,
	year = {2021},
}

@misc{widder_open_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Open ({For} {Business}): {Big} {Tech}, {Concentrated} {Power}, and the {Political} {Economy} of {Open} {AI}},
	shorttitle = {Open ({For} {Business})},
	url = {https://papers.ssrn.com/abstract=4543807},
	doi = {10.2139/ssrn.4543807},
	abstract = {This paper examines ‘open’ AI in the context of recent attention to open and open source AI systems. We find that the terms ‘open’ and ‘open source’ are used in confusing and diverse ways, often constituting more aspiration or marketing than technical descriptor, and frequently blending concepts from both open source software and open science. This complicates an already complex landscape, in which there is currently no agreed on definition of ‘open’ in the context of AI, and as such the term is being applied to widely divergent offerings with little reference to a stable descriptor. So, what exactly is ‘open’ about ‘open’ AI, and what does ‘open’ AI enable? To better answer these questions we begin this paper by looking at the various resources required to create and deploy AI systems, alongside the components that comprise these systems. We do this with an eye to which of these can, or cannot, be made open to scrutiny, reuse, and extension. What does ‘open’ mean in practice, and what are its limits in the context of AI? We find that while a handful of maximally open AI systems exist, which offer intentional and extensive transparency, reusability, and extensibility– the resources needed to build AI from scratch, and to deploy large AI systems at scale, remain ‘closed’—available only to those with significant (almost always corporate) resources. From here, we zoom out and examine the history of open source, its cleave from free software in the mid 1990s, and the contested processes by which open source has been incorporated into, and instrumented by, large tech corporations. As a current day example of the overbroad and ill-defined use of the term by tech companies, we look at  ‘open’ in the context of OpenAI the company. We trace its moves from a humanity-focused nonprofit to a for-profit partnered with Microsoft, and its shifting position on ‘open’ AI. Finally, we examine the current discourse around ‘open’ AI–looking at how the term and the (mis)understandings about what ‘open’ enables are being deployed to shape the public’s and policymakers’ understanding about AI, its capabilities, and the power of the AI industry. In particular, we examine the arguments being made for and against ‘open’ and open source AI, who’s making them, and how they are being deployed in the debate over AI regulation. Taken together, we find that ‘open’ AI can, in its more maximal instantiations, provide transparency, reusability, and extensibility that can enable third parties to deploy and build on top of powerful off-the-shelf AI models. These maximalist forms of ‘open’ AI can also allow some forms of auditing and oversight. But even the most open of ‘open’ AI systems do not, on their own, ensure democratic access to or meaningful competition in AI, nor does openness alone solve the problem of oversight and scrutiny. While we recognize that there is a vibrant community of earnest contributors building and contributing to ‘open’ AI efforts in the name of expanding access and insight, we also find that marketing around openness and investment in (somewhat) open AI systems is being leveraged by powerful companies to bolster their positions in the face of growing interest in AI regulation. And that some companies have moved to embrace ‘open’ AI as a mechanism to entrench dominance, using the rhetoric of ‘open’ AI to expand market power while investing in ‘open’ AI efforts in ways that allow them to set standards of development while benefiting from the free labor of open source contributors.},
	language = {en},
	urldate = {2024-02-02},
	author = {Widder, David Gray and West, Sarah and Whittaker, Meredith},
	month = aug,
	year = {2023},
	keywords = {AI, Big Tech, artificial intelligence, competition, data, open source, policy, political economy, privacy},
}

@article{urquhart_moral-it_2020,
	title = {The {Moral}-{IT} {Deck}: {A} {Tool} for {Ethics} by {Design}},
	shorttitle = {The {Moral}-{IT} {Deck}},
	url = {http://arxiv.org/abs/2007.07514},
	abstract = {This paper presents the design process and empirical evaluation of a new tool for enabling ethics by design: The Moral-IT Cards. Better tools are needed to support the role of technologists in addressing ethical issues during system design. These physical cards support reflection by technologists on normative aspects of technology development, specifically on emerging risks, appropriate safeguards and challenges of implementing these in the system. We discuss how the cards were developed and tested within 5 workshops with 20 participants from both research and commercial settings. We consider the role of technologists in ethics from different EU/UK policymaking initiatives and disciplinary perspectives (i.e. Science and Technology Studies (STS), IT Law, Human Computer Interaction (HCI), Computer/Engineering Ethics). We then examine existing ethics by design tools, and other cards based tools before arguing why cards can be a useful medium for addressing complex ethical issues. We present the development process for the Moral-IT cards, document key features of our card design, background on the content, the impact assessment board process for using them and how this was formulated. We discuss our study design and methodology before examining key findings which are clustered around three overarching themes. These are: the value of our cards as a tool, their impact on the technology design process and how they structure ethical reflection practices. We conclude with key lessons and concepts such as how they level the playing field for debate; enable ethical clustering, sorting and comparison; provide appropriate anchors for discussion and highlighted the intertwined nature of ethics.},
	urldate = {2020-10-29},
	journal = {arXiv:2007.07514 [cs]},
	author = {Urquhart, Lachlan and Craigon, Peter},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.07514},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
}

@inproceedings{hovy_social_2016,
	address = {Berlin, Germany},
	title = {The {Social} {Impact} of {Natural} {Language} {Processing}},
	url = {https://www.aclweb.org/anthology/P16-2096},
	doi = {10.18653/v1/P16-2096},
	urldate = {2021-02-08},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk and Spruit, Shannon L.},
	month = aug,
	year = {2016},
	pages = {591--598},
}

@inproceedings{shah_predictive_2020,
	address = {Online},
	title = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}: {A} {Conceptual} {Framework} and {Overview}},
	shorttitle = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.468},
	doi = {10.18653/v1/2020.acl-main.468},
	abstract = {An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the ﬁeld. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical deﬁnitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overampliﬁcation, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.},
	language = {en},
	urldate = {2021-03-31},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shah, Deven Santosh and Schwartz, H. Andrew and Hovy, Dirk},
	year = {2020},
	pages = {5248--5264},
}

@article{havens_situated_2020,
	title = {Situated {Data}, {Situated} {Systems}: {A} {Methodology} to {Engage} with {Power} {Relations} in {Natural} {Language} {Processing} {Research}},
	shorttitle = {Situated {Data}, {Situated} {Systems}},
	url = {http://arxiv.org/abs/2011.05911},
	abstract = {We propose a bias-aware methodology to engage with power relations in natural language processing (NLP) research. NLP research rarely engages with bias in social contexts, limiting its ability to mitigate bias. While researchers have recommended actions, technical methods, and documentation practices, no methodology exists to integrate critical reflections on bias with technical NLP methods. In this paper, after an extensive and interdisciplinary literature review, we contribute a bias-aware methodology for NLP research. We also contribute a definition of biased text, a discussion of the implications of biased NLP systems, and a case study demonstrating how we are executing the bias-aware methodology in research on archival metadata descriptions.},
	urldate = {2020-12-01},
	journal = {arXiv:2011.05911 [cs]},
	author = {Havens, Lucy and Terras, Melissa and Bach, Benjamin and Alex, Beatrice},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.05911},
	keywords = {Computer Science - Computation and Language},
}

@article{gebru_datasheets_2020,
	title = {Datasheets for {Datasets}},
	url = {http://arxiv.org/abs/1803.09010},
	abstract = {The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
	urldate = {2021-02-02},
	journal = {arXiv:1803.09010 [cs]},
	author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daumé III, Hal and Crawford, Kate},
	month = mar,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
}

@techreport{solaiman_release_2019,
	title = {Release {Strategies} and the {Social} {Impacts} of {Language} {Models}},
	url = {http://arxiv.org/abs/1908.09203},
	abstract = {Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.},
	number = {arXiv:1908.09203},
	urldate = {2022-06-06},
	institution = {arXiv},
	author = {Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and McCain, Miles and Newhouse, Alex and Blazakis, Jason and McGuffie, Kris and Wang, Jasmine},
	month = nov,
	year = {2019},
	note = {arXiv:1908.09203 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, I.2, I.2.7, K.4},
}

@inproceedings{maronikolakis_listening_2022,
	address = {Dublin, Ireland},
	title = {Listening to {Affected} {Communities} to {Define} {Extreme} {Speech}: {Dataset} and {Experiments}},
	shorttitle = {Listening to {Affected} {Communities} to {Define} {Extreme} {Speech}},
	url = {https://aclanthology.org/2022.findings-acl.87},
	doi = {10.18653/v1/2022.findings-acl.87},
	language = {en},
	urldate = {2022-07-28},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Maronikolakis, Antonis and Wisiorek, Axel and Nann, Leah and Jabbar, Haris and Udupa, Sahana and Schuetze, Hinrich},
	year = {2022},
	pages = {1089--1104},
}

@inproceedings{devinney_theories_2022,
	address = {New York, NY, USA},
	series = {{FAccT} '22},
	title = {Theories of “{Gender}” in {NLP} {Bias} {Research}},
	isbn = {978-1-4503-9352-2},
	url = {https://doi.org/10.1145/3531146.3534627},
	doi = {10.1145/3531146.3534627},
	abstract = {The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how “gender” is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define “bias.” Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.},
	urldate = {2022-11-14},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Devinney, Hannah and Björklund, Jenny and Björklund, Henrik},
	month = jun,
	year = {2022},
	keywords = {gender bias, gender studies, natural language processing},
	pages = {2083--2102},
}

@article{liang_embracing_2021,
	title = {Embracing {Four} {Tensions} in {Human}-{Computer} {Interaction} {Research} with {Marginalized} {People}},
	volume = {28},
	issn = {1073-0516, 1557-7325},
	url = {https://dl.acm.org/doi/10.1145/3443686},
	doi = {10.1145/3443686},
	abstract = {Human-computer interaction has a long history of working with marginalized people. We sought to understand how HCI researchers navigate work that engages with marginalized people and considerations researchers might work through to expand benefits and mitigate potential harms. In total, 24 HCI researchers, located primarily in the United States, participated in an interview, survey, or both. Through a reflexive thematic analysis, we identified four tensions—exploitation, membership, disclosure, and allyship. We explore the complexity involved in each, demonstrating that an equitable endpoint may not be possible, but this work is still worth pursuing when researchers make certain considerations. We emphasize that researchers who work with marginalized people should account for each tension in their research approaches to move forward. Finally, we propose an allyship-oriented approach to research that draws inspiration from discourse occurring in tangential fields and activist spaces and pushes the field into a new paradigm of research with marginalized people.},
	language = {en},
	number = {2},
	urldate = {2023-01-02},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Liang, Calvin A. and Munson, Sean A. and Kientz, Julie A.},
	month = apr,
	year = {2021},
	keywords = {Marginalized people, allyship, tensions, disclosure, exploitation, membership},
	pages = {1--47},
}

@inproceedings{markl_mind_2022,
	address = {Dublin, Ireland},
	title = {Mind the data gap(s): {Investigating} power in speech and language datasets},
	shorttitle = {Mind the data gap(s)},
	url = {https://aclanthology.org/2022.ltedi-1.1},
	doi = {10.18653/v1/2022.ltedi-1.1},
	abstract = {Algorithmic oppression is an urgent and persistent problem in speech and language technologies. Considering power relations embedded in datasets before compiling or using them to train or test speech and language technologies is essential to designing less harmful, more just technologies. This paper presents a reflective exercise to recognise and challenge gaps and the power relations they reveal in speech and language datasets by applying principles of Data Feminism and Design Justice, and building on work on dataset documentation and sociolinguistics.},
	urldate = {2022-08-22},
	booktitle = {Proceedings of the {Second} {Workshop} on {Language} {Technology} for {Equality}, {Diversity} and {Inclusion}},
	publisher = {Association for Computational Linguistics},
	author = {Markl, Nina},
	month = may,
	year = {2022},
	pages = {1--12},
}

@article{benjamin_what_2021,
	title = {What we do with data: a performative critique of data 'collection'},
	volume = {10},
	issn = {2197-6775},
	shorttitle = {What we do with data},
	url = {https://policyreview.info/articles/analysis/what-we-do-data-performative-critique-data-collection},
	doi = {10.14763/2021.4.1588},
	abstract = {Data collection is everywhere. It happens overtly and behind the scenes. It is a specific moment of legal obligation, the point at which the purpose and conditions of the data are legitimised. But what does the term data collection mean? What does it say or not say? Does it really capture the extraction or imposition taking place? How do terms and practices relate in defining the norms of data in society? This article undertakes a critique of data collection using data feminism and a performative theory of privacy: as a resource, an objective discovery and an assumption. It also discusses alternative terms and the implications of how we describe practices of ‘collecting’ data.},
	language = {en},
	number = {4},
	urldate = {2022-11-02},
	journal = {Internet Policy Review},
	author = {Benjamin, Garfield},
	month = dec,
	year = {2021},
}

@inproceedings{birhane_values_2022,
	address = {Seoul Republic of Korea},
	title = {The {Values} {Encoded} in {Machine} {Learning} {Research}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533083},
	doi = {10.1145/3531146.3533083},
	abstract = {Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15\%) and far fewer discuss negative potential (1\%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.},
	language = {en},
	urldate = {2022-11-03},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Birhane, Abeba and Kalluri, Pratyusha and Card, Dallas and Agnew, William and Dotan, Ravit and Bao, Michelle},
	month = jun,
	year = {2022},
	pages = {173--184},
}

@inproceedings{sloane_participation_2022,
	address = {Arlington VA USA},
	title = {Participation {Is} not a {Design} {Fix} for {Machine} {Learning}},
	isbn = {978-1-4503-9477-2},
	url = {https://dl.acm.org/doi/10.1145/3551624.3555285},
	doi = {10.1145/3551624.3555285},
	abstract = {This paper critiques popular modes of participation in design practice and machine learning. It examines three existing kinds of participation in design practice and machine learning participation as work, participation as consultation, and as participation as justice – to argue that the machine learning community must become attuned to possibly exploitative and extractive forms of community involvement and shift away from the prerogatives of context independent scalability. Cautioning against “participation washing”, it argues that the notion of “participation” should be expanded to acknowledge more subtle, and possibly exploitative, forms of community involvement in participatory machine learning design. Specifically, it suggests that it is imperative to recognize design participation as work; to ensure that participation as consultation is context-specific; and that participation as justice must be genuine and long term. The paper argues that such a development can only be scaffolded by a new epistemology around design harms, including, but not limited to, in machine learning. To facilitate such a development, the paper suggests developing we argue that developing a cross-sectoral database of design participation failures that is cross-referenced with socio-structural dimensions and highlights “edge cases” that can and must be learned from.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Equity and {Access} in {Algorithms}, {Mechanisms}, and {Optimization}},
	publisher = {ACM},
	author = {Sloane, Mona and Moss, Emanuel and Awomolo, Olaitan and Forlano, Laura},
	month = oct,
	year = {2022},
	pages = {1--6},
}

@article{weisz_toward_2023,
	title = {Toward {General} {Design} {Principles} for {Generative} {AI} {Applications}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2301.05578},
	doi = {10.48550/ARXIV.2301.05578},
	abstract = {Generative AI technologies are growing in power, utility, and use. As generative technologies are being incorporated into mainstream applications, there is a need for guidance on how to design those applications to foster productive and safe use. Based on recent research on human-AI co-creation within the HCI and AI communities, we present a set of seven principles for the design of generative AI applications. These principles are grounded in an environment of generative variability. Six principles are focused on designing for characteristics of generative AI: multiple outcomes \&amp; imperfection; exploration \&amp; control; and mental models \&amp; explanations. In addition, we urge designers to design against potential harms that may be caused by a generative model's hazardous output, misuse, or potential for human displacement. We anticipate these principles to usefully inform design decisions made in the creation of novel human-AI applications, and we invite the community to apply, revise, and extend these principles to their own work.},
	urldate = {2023-02-13},
	author = {Weisz, Justin D. and Muller, Michael and He, Jessica and Houde, Stephanie},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, Human-Computer Interaction (cs.HC)},
}

@inproceedings{goldfarb-tarrant_this_2023,
	address = {Toronto, Canada},
	title = {This prompt is measuring {\textless}mask{\textgreater}: evaluating bias evaluation in language models},
	url = {https://aclanthology.org/2023.findings-acl.139},
	doi = {10.18653/v1/2023.findings-acl.139},
	abstract = {Bias research in NLP seeks to analyse models for social biases, thus helping NLP practitioners uncover, measure, and mitigate social harms. We analyse the body of work that uses prompts and templates to assess bias in language models. We draw on a measurement modelling framework to create a taxonomy of attributes that capture what a bias test aims to measure and how that measurement is carried out. By applying this taxonomy to 90 bias tests, we illustrate qualitatively and quantitatively that core aspects of bias test conceptualisations and operationalisations are frequently unstated or ambiguous, carry implicit assumptions, or be mismatched. Our analysis illuminates the scope of possible bias types the field is able to measure, and reveals types that are as yet under-researched. We offer guidance to enable the community to explore a wider section of the possible bias space, and to better close the gap between desired outcomes and experimental design, both for bias and for evaluating language models more broadly.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Goldfarb-Tarrant, Seraphina and Ungless, Eddie and Balkir, Esma and Blodgett, Su Lin},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {2209--2225},
}

@inproceedings{dev_harms_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Harms of {Gender} {Exclusivity} and {Challenges} in {Non}-{Binary} {Representation} in {Language} {Technologies}},
	url = {https://aclanthology.org/2021.emnlp-main.150},
	doi = {10.18653/v1/2021.emnlp-main.150},
	abstract = {Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non-binary genders in society. In this paper, we explain the complexity of gender and language around it, and survey non-binary persons to understand harms associated with the treatment of gender as binary in English language technologies. We also detail how current language representations (e.g., GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information.},
	urldate = {2023-01-11},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Dev, Sunipa and Monajatipoor, Masoud and Ovalle, Anaelia and Subramonian, Arjun and Phillips, Jeff and Chang, Kai-Wei},
	month = nov,
	year = {2021},
	pages = {1968--1994},
}

@article{paullada_data_2020,
	title = {Data and its (dis)contents: {A} survey of dataset development and use in machine learning research},
	shorttitle = {Data and its (dis)contents},
	url = {https://www.semanticscholar.org/reader/c09f44e0088342ec618c7a2deeab1526d73b2d6b},
	doi = {10.1016/j.patter.2021.100336},
	abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
	language = {en},
	urldate = {2023-02-13},
	journal = {Patterns},
	author = {Paullada, Amandalynne and Raji, Inioluwa Deborah and Bender, Emily M. and Denton, Emily L. and Hanna, A.},
	year = {2020},
}

@inproceedings{dev_measures_2022,
	address = {Online only},
	title = {On {Measures} of {Biases} and {Harms} in {NLP}},
	url = {https://aclanthology.org/2022.findings-aacl.24},
	abstract = {Recent studies show that Natural Language Processing (NLP) technologies propagate societal biases about demographic groups associated with attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While existing works propose bias evaluation and mitigation methods for various tasks, there remains a need to cohesively understand the biases and the specific harms they measure, and how different measures compare with each other. To address this gap, this work presents a practical framework of harms and a series of questions that practitioners can answer to guide the development of bias measures. As a validation of our framework and documentation questions, we also present several case studies of how existing bias measures in NLP—both intrinsic measures of bias in representations and extrinsic measures of bias of downstream applications—can be aligned with different harms and how our proposed documentation questions facilitates more holistic understanding of what bias measures are measuring.},
	urldate = {2023-03-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {AACL}-{IJCNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Dev, Sunipa and Sheng, Emily and Zhao, Jieyu and Amstutz, Aubrie and Sun, Jiao and Hou, Yu and Sanseverino, Mattie and Kim, Jiin and Nishi, Akihiro and Peng, Nanyun and Chang, Kai-Wei},
	month = nov,
	year = {2022},
	pages = {246--267},
}

@inproceedings{knowles_trustworthy_2023,
	address = {New York, NY, USA},
	series = {{FAccT} '23},
	title = {Trustworthy {AI} and the {Logics} of {Intersectional} {Resistance}},
	isbn = {9798400701924},
	url = {https://dl.acm.org/doi/10.1145/3593013.3593986},
	doi = {10.1145/3593013.3593986},
	abstract = {Growing awareness of the capacity of AI to inflict harm has inspired efforts to delineate principles for ‘trustworthy AI’ and, from these, objective indicators of ‘trustworthiness’ for auditors and regulators. Such efforts run the risk of formalizing a distinctly privileged perspective on trustworthiness which is insensitive (or else indifferent) to the legitimate reasons for distrust held by marginalized people. By exploring a neglected conative element of trust, we broaden understandings of trust and trustworthiness to make sense of, and identify principles for responding productively to, distrust of ostensibly ‘trustworthy’ AI. Bringing social science scholarship into dialogue with AI criticism, we show that AI is being used to construct a digital underclass that is rhetorically labelled as ‘undeserving’, and highlight how this process fulfills functions for more privileged people and institutions. We argue that distrust of AI is warranted and healthy when the AI contributes to marginalization and structural violence, and that Trustworthy AI may fuel public resistance to the use of AI unless it addresses this dimension of untrustworthiness. To this end, we offer reformulations of core principles of Trustworthy AI—fairness, accountability, and transparency—that substantively address the deeper issues animating widespread public distrust of AI, including: stewardship and care, openness and vulnerability, and humility and empowerment. In light of legitimate reasons for distrust, we call on the field to to re-evaluate why the public would embrace the expansion of AI into all corners of society; in short, what makes it worthy of their trust.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 2023 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Knowles, Bran and Fledderjohann, Jasmine and Richards, John T. and Varshney, Kush R.},
	month = jun,
	year = {2023},
	keywords = {Trust, accountability, artificial intelligence, bias, distrust, fairness, inequality, intersectionality, transparency},
	pages = {172--182},
}

@article{mahelona_openais_2023,
	title = {{OpenAI}’s whisper is another case study in colonisation},
	url = {https://blog.papareo.nz/whisper-is-another-case-study-in-colonisation/},
	journal = {Papa Reo},
	author = {Mahelona, Keoni and Leoni, Gianna and Duncan, Suzanne and Thompson, Miles},
	month = jan,
	year = {2023},
}

@misc{santy_nlpositionality_2023,
	title = {{NLPositionality}: {Characterizing} {Design} {Biases} of {Datasets} and {Models}},
	shorttitle = {{NLPositionality}},
	url = {http://arxiv.org/abs/2306.01943},
	doi = {10.48550/arXiv.2306.01943},
	abstract = {Design biases in NLP systems, such as performance differences for different populations, often stem from their creator's positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks -- social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries. We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Santy, Sebastin and Liang, Jenny T. and Bras, Ronan Le and Reinecke, Katharina and Sap, Maarten},
	month = jun,
	year = {2023},
	note = {arXiv:2306.01943 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
}

@article{mohammad_ethics_2022-1,
	title = {Ethics {Sheet} for {Automatic} {Emotion} {Recognition} and {Sentiment} {Analysis}},
	volume = {48},
	url = {https://aclanthology.org/2022.cl-2.1},
	doi = {10.1162/coli_a_00433},
	abstract = {The importance and pervasiveness of emotions in our lives makes affective computing a tremendously important and vibrant line of work. Systems for automatic emotion recognition (AER) and sentiment analysis can be facilitators of enormous progress (e.g., in improving public health and commerce) but also enablers of great harm (e.g., for suppressing dissidents and manipulating voters). Thus, it is imperative that the affective computing community actively engage with the ethical ramifications of their creations. In this article, I have synthesized and organized information from AI Ethics and Emotion Recognition literature to present fifty ethical considerations relevant to AER. Notably, this ethics sheet fleshes out assumptions hidden in how AER is commonly framed, and in the choices often made regarding the data, method, and evaluation. Special attention is paid to the implications of AER on privacy and social groups. Along the way, key recommendations are made for responsible AER. The objective of the ethics sheet is to facilitate and encourage more thoughtfulness on why to automate, how to automate, and how to judge success well before the building of AER systems. Additionally, the ethics sheet acts as a useful introductory document on emotion recognition (complementing survey articles).},
	number = {2},
	urldate = {2024-02-02},
	journal = {Computational Linguistics},
	author = {Mohammad, Saif M.},
	month = jun,
	year = {2022},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {239--278},
}
