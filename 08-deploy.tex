\section{Deployment}\label{sec:deploy}
\noindent\textbf{\textit{Overview of ethical issues related to deployment and dissemination of NLP artefacts.}}

\subsection{Likely Harms}\label{subsec:harms}\noindent\textbf{\textit{An overview of the likely harms of LLMs.}}
\newline 

\noindent With the recent introduction of LLMs and generative AI into society, there has been an increased interest in the application of NLP tools and research.\footnote{As of January 2022, 15\% of businesses in the United Kingdom where using some form of AI and another 10\% planning to invest in AI~\cite{CapitalEconomics_AI_2022}.} 
Increases in the application of research, and its outputs, requires a particular attention and care from researchers to avoid enacting harms.

In their broad overview of the harms that arise from generative AI, ~\citet{Solaiman_Evaluating_2024} present seven over-arching categories of social harms from technical systems, including bias, stereotypes, and representational harms (we discuss bias in \Cref{sec:harmeval}); cultural values and sensitive content (we discuss best practice for sensitive content in \Cref{sec:whose}); disparate performances (we discuss fairness and performance in \Cref{sec:eval}); environmental costs and carbon emissions (we discuss environmental costs in \Cref{subsec:energy}); privacy and data protections (we discuss consent and safety in \Cref{sec:whose}); financial costs; and data and content moderation labour (we discuss crowdworkers' rights in \Cref{subsec:rights}). However, in recognition that these cannot be separated from impacts on society, \citet{Solaiman_Evaluating_2024} also present categories of ``impacts'' on society, including trustworthiness and autonomy (which we discuss below), 
inequality, marginalisation and violence, 
the concentration of authority, 
labour and creativity, 
and ecosystem and environmental impacts. 
Alongside \citet{Solaiman_Evaluating_2024}, \citet{weidinger_ethical_2021} and \citet{kumar_language_2022} have also addressed risks of generative AI, finding a subset of the risks and concerns identified by \citet{Solaiman_Evaluating_2024}. While these sets of authors have focused on generative AI, many of the same concerns---such as bias, stereotypes, and representational harms---have been well documented for other NLP technologies~\cite[e.g.,][]{Anand_Don_2024,Bolukbasi_Man_2016,Davidson_Racial_2019,De-Arteaga_Bias_2019}.

A pressing issue is that LLMs suffer from the issue of \textit{hallucination}, or the production of misinformation.
Indeed, as \citet{Solaiman_Evaluating_2024} remind us, ``[increased] access to generating content and potential misinformation, and difficulty distinguishing between human and AI-generated content, poses risks to trust in media and content authenticity,'' thus presenting a challenge to knowledge-based societies. Training data may then become ``polluted'' with generated misinformative content which then degrades future performance \citep{pan_risk_2023}. Several researchers have proposed watermarking of LLM-generated content \cite{kirchenbauer_watermark_2023, grinbaum_ethical_2022}. This would help to identify generated content, and also plagiarism (in the sense of using LLMs to write supposedly original content); current detection methods suffer from bias against non-native speakers \citep{liang_gpt_2023}. LLMs can also be guilty of plagiarism in the sense of reproducing copyright content. Reproduction of the training data can infringe on both copyright and privacy, as PI is reproduced by the models \citep{lee_language_2022, huang_are_2022, lukas_analyzing_2023} (see \Cref{subsec:clean} on best practice to avoid this). 
Here we have given an overview of the likely harms of LLMs, but you should consult \citet{Solaiman_Evaluating_2024,weidinger_ethical_2021, kumar_language_2022} for more details. 

\subsubsection{Dual use}
\noindent\textbf{\textit{Considering the dual -- both negative and positive -- use of LLMs.}}
\newline 

\noindent The risks of harms arising from dual use of language technologies was first raised by \citet{hovy_social_2016}, describing how they can be used for good, e.g., ``shed[ing] light on the provenance of historic texts''
while also holding potential for negative consequences, e.g., censorship and ``endanger[ing] the anonymity of political dissenters.''
It is often hard to imagine how one's research artefacts might be used for nefarious purposes (though see Section \cref{subsec:ideation} for some toolkits to anticipate harm).
Yet, as NLP research is increasingly integrated into consumer- and business-facing products, considerations of how technologies might be misused become increasingly important. Addressing risks of dual use requires in-depth and contextual consideration of the technology and deployment contexts~\citep{kaffee_thorny_2023}, to which end \citet{kaffee_thorny_2023} present a definition of dual use, and a checklist for consideration in research projects.
With their checklist, \citet{kaffee_thorny_2023} invite practitioners\eddie{shall we use practioners throughout in place of researchers and/or developers} to reflect on the risks of dual use and potential mitigation strategies.

\citet{derczynski_assessing_2023} argue for a need for \textit{Risk Cards}, which seek to make clear the risks that can come from models. Risk Cards centre each risk, e.g., the generation of misinformation, rather than the model in which a harm might manifest. They are specifically developed with LLMs in mind, and consider harms that can arise from LLMs, such as representational harms. Risk Cards are best used as living documents, where each new model you develop is subject to tests for a risk, and as the risk is expanded on, e.g. by identifying sub-risks, models are retested. This presents a change from the current practice of only testing models once.
Beyond documentation, one can also engage in partial release practices~\cite{kaffee_thorny_2023,solaiman_release_2019}, and using license terms that prohibit specific uses~\cite{McDuff_Standardization_2024,jernite_data_2022} (see below). 


\subsection{Release and Beyond}
Herein we discuss ethical considerations related to release of a new technology, specifically we discuss ramifications of level of openness; public perceptions of new and even retired technologies, and the impact of new technologies on their context.

\subsubsection{Release Strategy}
Releasing LLMs requires balancing the benefits of different release approaches with the potential risks of misuse. Some LLMs are advertised as being ``open'' or ``open source'', though the term ``open'' lacks an agreed definition and can refer to various aspects of the model release e.g. publicly accessible model parameters ("open weights"), transparent training procedures, open data for training etc \cite{solaiman_release_2019,widder_open_2023, groeneveld_olmo_2024}.
Openness enhances transparency, reproducibility and fosters collaborative advancements \cite{widder_open_2023,spirling_why_2023}, allowing for community-driven improvements \cite{madnani_building_2017}.
However, fully open releases may carry significant risks, such as misuse for unintended harmful activities, including after alterations \cite{arditi2024refusal}. While a model with restricted access can be easily taken down, an openly released model may be impossible to retract.
These risks can be mitigated by adopting careful strategies before, during and after deployment. Pre-release audits can help identify biases and security vulnerabilities \cite{madnani_building_2017,ji2023ai}. The release process should be staged, with an initial release to trusted researchers, followed by a gradual wider release \cite{solaiman_release_2019}, with post-deployment monitoring to assess new risks \cite{anderljung2023frontier}. Commercially deployed systems should provide a reporting or complaints procedure. 

\subsubsection{Public perception}
Concomitant with product release types, public perception must be considered. One task where this has been explored in detail is automated decision-making (ADM).
How fair ADM systems are perceived to be varies between studies \cite{lee_understanding_2018, araujo_ai_2020}, and is influenced by a number of individual factors \citep{wang_factors_2020}; this is likely true for the fairness of other AI models. Exacerbating these concerns, algorithmic harms can have long lasting consequences on how people understand their lives and AI, even after a technology is ``retired'' \citep{ehsan_algorithmic_2022}. It is not enough to mitigate harms such as unfairness: the public must also believe these harms to have been mitigated, or their concerns will remain. You must take into account perceptions of your model -- not just how it ``really works'' -- when trying to limit its harmful impact.  This pertains to release strategy, marketing language, interface design and more.

\subsubsection{Ripples and Coils}
When deploying LLMs, you may fall into common traps well known from previous automated systems, pertaining to fairness, justice, and due process \cite{selbst_fairness_2019}. For example the Ripple Effect Trap, when developers fail to consider how technology changes existing social systems. This is particularly relevant to LLMs, as their use can feedback on the data we collect \citep{pan_risk_2023}, the social world we model, and the decisions we make, leading to a looping effect known as data-coiling \cite{beer_problem_2022}.
To avoid these traps, design processes should accurately model the specific context for deployment \cite{selbst_fairness_2019}, and engage with the conceptual problems and questions that arise from data-coiling \cite{beer_problem_2022}.

\subsection{Interventions at Inference}\label{subsec:inf}
\noindent\textbf{\textit{Limited success of interventions at inference time to minimise harm.}}
\newline 

\noindent There are a number of ways harmful content from LLMs might be prevented at inference, including guardrails (systems designed to detect when the model might output offensive content, and substitute an alternative), output filtering and prompt modification. Guardrails are widely used on closed-source LLMs such as ChatGPT, Bard etc. Unfortunately, as evidenced by numerous media reports (and on X, formerly Twitter) these guardrails are very brittle \citep{cuthbertson_chatgpt_2023}.

It has been suggested that model output might be improved with regards to harms by simple prompt modification. Natural language interventions using ethical prompts have been unsuccessful for older LLMs \citep{zhao_ethical-advice_2021}. They have also been explored for text-to-image models \citep{bansal_how_2022}, with mixed success
\citep{shin_can_2024}. Prompt modification may also be happening ``behind the scenes'' as part of the guard rails discussed above, as has been demonstrated for Microsoft Bing \citep{edwards_ai-powered_2023}. However, prompt modification can also be problematic if done insensitively. \citet{ungless_stereotypes_2023} find transgender people reject the removal of sensitive terms to prevent harm, and \citet{vynck_google_2024} document significant push back to Google's decision to modify prompts to increase ethnic and gender diversity including in inappropriate contexts.

Finally, a very simple attempt to lessen harm at inference is to warn of the potential for offensive or inaccurate output.\footnote{e.g. ChatGPT has a warning message ``ChatGPT can make mistakes. Check important info.''} However research suggests warning messages are unpopular amongst marginalised groups (at least for text-to-image models \cite{ungless_stereotypes_2023}). Further, whilst research suggests warning labels may be useful for possible AI-generated misinformation \citep{wittenberg_labeling_2024}, this is currently no evidence to support that these warnings actually prevent harm of offensive content. They are far too vague to enable people to avoid being exposed to particular upsetting content (e.g. offensive content could be racist stereotypes, graphic gore, sexually explicit etc).

It seems that guardrails and other interventions at inference are brittle and have shown limited success at addressing ethical issues ``baked into'' LLMs.

\subsection{Using LLMs to replace humans}\label{sec:replacehumans}\noindent\textbf{\textit{Ethical ramifications of using LLM to replace humans at various different tasks.}}
\newline 

\noindent The proliferation of LLMs makes it tempting to replace humans by LLMs to increase efficiency. LLMs are being considered even for professions with high levels of impact, such as content creation, journalism and creative writing \cite{wga_negotiating_comittee_wga_2023}, as well as education \cite{walczak2023challenges, kasneci2023chatgpt, sok2023chatgpt}. While automating some repetitive aspect of a person's job might have ethical justification, replacing humans with machines could lead to significant problems with unemployment and could do fundamental damage to society. Journalists and educators do far more than deliver information efficiently and serious study of stakeholders and risks should be undertaken before any project is approved.   

That said in education, LLMs show some promise for automated creation of educational content, improvement of student engagement and personalisation of the learning experience \cite{kasneci2023chatgpt}. However, responsible application of these tools requires digital literacy as well as understanding of capabilities and limitations by both teachers and students \cite{walczak2023challenges, kasneci2023chatgpt}. Deployment in this context demands adherence to privacy, security and regulatory requirements \cite{kasneci2023chatgpt} while universities should put more emphasis on teaching life-long self learning skills \cite{walczak2023challenges}.

LLMs have also been considered for replacing human participants in domains such as psychology, user research and AI technology development. This holds the promise of cutting costs, avoiding potential participant harms and increasing (apparent) demographic diversity \cite{agnew_illusion_2024}.
Notably, \citet{chiang_can_2023} reported success with replacing humans for evaluation; they find LLM story evaluation was consistent with evaluations by human experts.
However, this result is contrasted by a range of other studies reporting major shortcomings.
\citet{aher_using_2022} find that some LLMs have a tendency to give unhumanly accurate answers. Similarly, \citet{bavaresco_llms_2024} observe shortcomings in replicating human judgements, where correlations between human and LLM judgements varied considerably across data sets.
LLMs also performed poorly when evaluated on the labelling of a fairness benchmark from a community survey \cite{felkner_gpt_2024}.
These studies question the readiness of LLMs to replace human participants. Going further, \citet{agnew_illusion_2024} argue that LLM replacement of participants undermines foundational values of work with human participants, namely representation of participants' interests, inclusion of participants in the development process and understanding. These shortcomings are fundamental and cannot be fixed with better training procedures \cite{agnew_illusion_2024}.

An alternative avenue to explore the potential of LLMs is to keep humans in the loop. \citet{shneiderman_human-centered_2020} propose a framework called Human-Centered Artificial Intelligence, which aims to use AI techniques in the right way to increase human performance and ensure that AI systems are reliable, safe and trustworthy. The framework also defines scenarios where full control or automation is justified.


\subsection{Dissemination}\label{subsec:diss}
Your responsibility for ethical outcomes does not end when you finish work on a given system \citep{widder2023dislocated}. In most cases our intention is that others will further develop what we have done. We share responsibility for that ongoing work, in part because the way in which we distribute our work influences how others are predisposed to interact with it. The most obvious responsibility is in making sure future practitioners have the information they require to make informed ethical decisions. This highlights the importance of openness about processes, data \citep{mieskes_quantitative_2017}, and other resources used \cite{schwartz_green_2019}. Toolkits like Model Cards and Datasheets \citep{mitchell_model_2019, gebru2021datasheets} are a good start, but we shouldn't let our responsible practice deteriorate to just another technical documentation task \citep{widder_epistemic_2024}. 

Both in descriptions of our own work, and in discussion of the field as a whole, the way we talk about our technologies can have a wider impact on understanding. While the demands of competitive funding calls may lead us to oversell, chains of this happening over and over again, catalysed by the media, contribute to hype cycles \citep{markelius2024mechanisms}. Ultimately this means decision makers in government and in wider society are left misinformed about LLM capabilities, leading to harmful decisions about their governance or deployment. 

Finally, it is worth remembering that you can be a role model: how you demonstrate the centrality of ethical  thinking in your work will in turn encourage others around you to think differently about it. This includes your own use of LLMs \citep{guleria_chatgpt_2023,lund_chatgpt_2023}.

\subsection{Key Resources}
Do's and Don'ts
\begin{itemize}
    \item \textcolor{ForestGreen}{\textbf{Do}} consider integrating watermarking into your generative models --  \textcolor{red}{\textbf{don't}} rely on supervised detection models alone
    \item \textcolor{ForestGreen}{\textbf{Do}} pre-release audits to identify biases and security vulnerabilities \cite{madnani_building_2017} -- \textcolor{red}{\textbf{don't}} put the onus on marginalised people to discover harms
    
    \item \textcolor{ForestGreen}{\textbf{Do}} release LLMs in stages, with an initial release to trusted researchers, followed by a gradual wider release \cite{solaiman_release_2019} -- \textcolor{red}{\textbf{don't}} forget the model will change its own environment in terms of both training data and people's expectations
    \item \textcolor{ForestGreen}{\textbf{Do}} continually monitor post-deployment to assess new risks and create a complaints procedure \cite{anderljung2023frontier} -- \textcolor{red}{\textbf{don't}} count on brittle guardrails to prevent harm
    \item \textcolor{ForestGreen}{\textbf{Do}} consider how AI might enhance human experience of work, as well as performance -- \textcolor{red}{\textbf{don't}} assume LLMs can effectively replace human participants 
    \item \textcolor{ForestGreen}{\textbf{Do}} consider how the public perceive your technology -- \textcolor{red}{\textbf{don't}} contribute to the hype cycle
     
\end{itemize}



\noindent Useful Tool(kit)s: 
\begin{itemize}
    \item Framework to encourage AI that enhances rather than replaces human performance -- \citet{shneiderman_human-centered_2020}
    \item Overview of harms and ramifications of generative AI technologies -- \citet{Solaiman_Evaluating_2024}
    \item A definition of dual use, and a checklist for consideration in research projects -- \citet{kaffee_thorny_2023}
    \item Documentation methodology for risks of LLMs, that could be adapted to document dual use impact -- \citet{derczynski_assessing_2023}
\end{itemize}
