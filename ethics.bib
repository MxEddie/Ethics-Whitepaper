

@inproceedings{weidinger2022taxonomy,
  title={Taxonomy of risks posed by language models},
  author={Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and others},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={214--229},
  year={2022}
}

@article{caliskan2017semantics,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Caliskan, Aylin and Bryson, Joanna J and Narayanan, Arvind},
  journal={Science},
  volume={356},
  number={6334},
  pages={183--186},
  year={2017},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{mcnamara2018does,
  title={Does ACM’s code of ethics change ethical decision making in software development?},
  author={McNamara, Andrew and Smith, Justin and Murphy-Hill, Emerson},
  booktitle={Proceedings of the 2018 26th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering},
  pages={729--733},
  year={2018}
}
@inproceedings{Talat_You_2022,
  title = {You Reap What You Sow: {{On}} the Challenges of Bias Evaluation under Multilingual Settings},
  booktitle = {Proceedings of {{BigScience}} Episode \#5 -- Workshop on Challenges \& Perspectives in Creating Large Language Models},
  author = {Talat, Zeerak and N{\'e}v{\'e}ol, Aur{\'e}lie and Biderman, Stella and Clinciu, Miruna and Dey, Manan and Longpre, Shayne and Luccioni, Sasha and Masoud, Maraim and Mitchell, Margaret and Radev, Dragomir and Sharma, Shanya and Subramonian, Arjun and Tae, Jaesung and Tan, Samson and Tunuguntla, Deepak and Van Der Wal, Oskar},
  year = {2022},
  month = may,
  pages = {26--41},
  publisher = {Association for Computational Linguistics},
  address = {virtual+Dublin},
  url = {https://aclanthology.org/2022.bigscience-1.3},
  abstract = {Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies.},
  copyright = {All rights reserved}
}

@phdthesis{Talat_It_2021,
  title = {"{{It}} Ain't All Good:" {{Machinic}} Abuse Detection and Marginalisation in Machine Learning},
  author = {Talat, Zeerak},
  year = {2021},
  month = may,
  address = {Sheffield},
  url = {https://etheses.whiterose.ac.uk/30950/},
  abstract = {Online abusive language has been given increasing prominence as a societal problem over the past few years as people are increasingly communicating on online platforms. This increase in prominence has resulted in an increase in academic attention to the issue, particularly within the field of Natural Language Processing (NLP), which has proposed multiple datasets and machine learning methods for the detection of text-based abuse. Recently, the issue of disparate impacts of machine learning has been given attention, showing that marginalised groups in society are disproportionately negatively affected by automated content moderation systems. Moreover, a number of challenges have been identified for abusive language detection technologies, including poor model performance across datasets and a lack of ability of models to contextualise potentially abusive speech within the context of speaker intentions. This dissertation aims to ask how NLP models for online abuse detection can address issues of generalisation and context. Through critically examining the task of online abuse detection, I highlight how content moderation acts as protective filter that seeks to maintain a sanitised environment. I find that when considering automated content moderation systems through this lens, it is made clear that such systems are centred around experiences of some bodies at the expense of others, often those who are already marginalised. In efforts to address this, I propose two different modelling processes that a) centre the the mental and emotional states of the speaker by representing documents through the Linguistic Inquiry and Word Count (LIWC) categories that they invoke, and using Multi-Task Learning (MTL) to model abuse, such that the model takes aims to take account the intentions of the speaker. I find that through the use of LIWC for representing documents, machine learning models for online abuse detection can see improvements in classification scores on in-domain and out-of-domain datasets. Similarly, I show that through a use of MTL, machine learning models can gain improvements by using a variety of auxiliary tasks that combine data for content moderation systems and data for related tasks such as sarcasm detection. Finally, I critique the machine learning pipeline in an effort to identify paths forward that can bring into focus the people who are excluded and are likely to experience harms from machine learning models for content moderation.},
  copyright = {All rights reserved},
  school = {University of Sheffield}
}

@book{Mulvin_Proxies_2021,
  title = {Proxies: The Cultural Work of Standing In},
  shorttitle = {Proxies},
  author = {Mulvin, Dylan},
  year = {2021},
  series = {Infrastructures {{Ser}}.},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {How those with the power to design technology, in the very moment of design, are allowed to imagine who is included - and who is excluded - in the future. The open access edition of this book was made possible by generous funding from Arcadia -- a charitable fund of Lisbet Rausing and Peter Baldwin.Our world is built on an array of standards we are compelled to share. In Proxies, Dylan Mulvin examines how we arrive at those standards, asking, ``To whom and to what do we delegate the power to stand in for the world?'' Mulvin shows how those with the power to design technology, in the very moment of design, are allowed to imagine who is included - and who is excluded - in the future. For designers of technology, some bits of the world end up standing in for other bits, standards with which they build and calibrate. These ``proxies'' carry specific values, even as they disappear from view. Mulvin explores the ways technologies, standards, and infrastructures inescapably reflect the cultural milieus of their bureaucratic homes. Drawing on archival research, he investigates some of the basic building-blocks of our shared infrastructures. He tells the history of technology through the labor and communal practices of, among others, the people who clean kilograms to make the metric system run, the women who pose as test images, and the actors who embody disease and disability for medical students. Each case maps the ways standards and infrastructure rely on prototypical ideas of whiteness, able-bodiedness, and purity to control and contain the messiness of reality. Standards and infrastructures, Mulvin argues, shape and distort the possibilities of representation, the meaning of difference, and the levers of change and social justice.},
  isbn = {978-0-262-36624-3},
  langid = {english}
}

@article{Dunn_Mapping_2020,
  title = {Mapping Languages: The {{Corpus}} of {{Global Language Use}}},
  shorttitle = {Mapping Languages},
  author = {Dunn, Jonathan},
  year = {2020},
  month = dec,
  journal = {Language Resources and Evaluation},
  volume = {54},
  number = {4},
  pages = {999--1018},
  issn = {1574-020X, 1574-0218},
  doi = {10.1007/s10579-020-09489-2},
  url = {http://link.springer.com/10.1007/s10579-020-09489-2},
  urldate = {2021-03-02},
  langid = {english}
}

@article{Jamieson_Reflexivity_2023,
  title = {Reflexivity in Quantitative Research: {{A}} Rationale and Beginner's Guide},
  shorttitle = {Reflexivity in Quantitative Research},
  author = {Jamieson, Michelle K. and Govaart, Gisela H. and Pownall, Madeleine},
  year = {2023},
  month = apr,
  journal = {Social and Personality Psychology Compass},
  volume = {17},
  number = {4},
  pages = {e12735},
  issn = {1751-9004, 1751-9004},
  doi = {10.1111/spc3.12735},
  url = {https://compass.onlinelibrary.wiley.com/doi/10.1111/spc3.12735},
  urldate = {2024-08-18},
  abstract = {Abstract             Reflexivity is the act of examining one's own assumption, belief, and judgement systems, and thinking carefully and critically about how these influence the research process. The practice of reflexivity confronts and questions who we are as researchers and how this guides our work. It is central in debates on objectivity, subjectivity, and the very foundations of social science research and generated knowledge. Incorporating reflexivity in the research process is traditionally recognized as one of the most notable differences between qualitative and quantitative methodologies. Qualitative research centres and celebrates the participants' personal and unique lived experience. Therefore, qualitative researchers are readily encouraged to consider how their own unique positionalities inform the research process and this forms an important part of training within this paradigm. Quantitative methodologies in social and personality psychology, and more generally, on the other hand, have remained seemingly detached from this level of reflexivity and general reflective practice. In this commentary, we, three quantitative researchers who have grappled with the compatibility of reflexivity within our own research, argue that reflexivity has much to offer quantitative methodologists. The act of reflexivity prompts researchers to acknowledge and centre their own positionalities, encourages a more thoughtful engagement with every step of the research process, and thus, as we argue, contributes to the ongoing reappraisal of openness and transparency in psychology. In this paper, we make the case for integrating reflexivity across all research approaches, before providing a `beginner's guide' for quantitative researchers wishing to engage reflexively with their own work, providing concrete recommendations, worked examples, and reflexive prompts.},
  langid = {english}
}



@misc{McDuff_Standardization_2024,
  title = {On the {{Standardization}} of {{Behavioral Use Clauses}} and {{Their Adoption}} for {{Responsible Licensing}} of {{AI}}},
  author = {McDuff, Daniel and Korjakow, Tim and Cambo, Scott and Benjamin, Jesse Josua and Lee, Jenny and Jernite, Yacine and Ferrandis, Carlos Mu{\~n}oz and Gokaslan, Aaron and Tarkowski, Alek and Lindley, Joseph and Cooper, A. Feder and Contractor, Danish},
  year = {2024},
  month = feb,
  number = {arXiv:2402.05979},
  eprint = {2402.05979},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2402.05979},
  urldate = {2024-08-12},
  abstract = {Growing concerns over negligent or malicious uses of AI have increased the appetite for tools that help manage the risks of the technology. In 2018, licenses with behaviorial-use clauses (commonly referred to as Responsible AI Licenses) were proposed to give developers a framework for releasing AI assets while specifying their users to mitigate negative applications. As of the end of 2023, on the order of 40,000 software and model repositories have adopted responsible AI licenses licenses. Notable models licensed with behavioral use clauses include BLOOM (language) and LLaMA2 (language), Stable Diffusion (image), and GRID (robotics). This paper explores why and how these licenses have been adopted, and why and how they have been adapted to fit particular use cases. We use a mixed-methods methodology of qualitative interviews, clustering of license clauses, and quantitative analysis of license adoption. Based on this evidence we take the position that responsible AI licenses need standardization to avoid confusing users or diluting their impact. At the same time, customization of behavioral restrictions is also appropriate in some contexts (e.g., medical domains). We advocate for ``standardized customization'' that can meet users' needs and can be supported via tooling.},
  archiveprefix = {arXiv}
}

@incollection{Dencik_Datafied_2022,
  title = {The {{Datafied Welfare State}}: {{A Perspective}} from the {{UK}}},
  shorttitle = {The {{Datafied Welfare State}}},
  booktitle = {New {{Perspectives}} in {{Critical Data Studies}}},
  author = {Dencik, Lina},
  editor = {Hepp, Andreas and Jarke, Juliane and Kramp, Leif},
  year = {2022},
  pages = {145--165},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-96180-0_7},
  url = {https://link.springer.com/10.1007/978-3-030-96180-0_7},
  urldate = {2024-08-12},
  abstract = {Abstract             The crisis emerging from the COVID-19 pandemic has elevated the relevance of the welfare state as well as the role of platforms and data infrastructures across key areas of public and social life. Whilst the crisis shed light on the ways in which these might intersect, the turn to data-driven systems in public administration has been a prominent development in several countries for quite some time. In this chapter I focus on the UK as a pertinent example of key trends at the intersection of technological infrastructures and the welfare state. In particular, using developments in UK welfare sectors as a lens, I advance a two-part argument about the ways in which data infrastructures are transforming state-citizen relations through on the one hand advancing an actuarial logic based on personalised risk and the individualisation of social problems (what I refer to as responsibilisation) and, on the other, entrenching a dependency on an economic model that perpetuates the circulation of data accumulation (what I refer to as rentierism). These mechanisms, I argue, fundamentally shift the `matrix of social power' that made the modern welfare state possible and position questions of data infrastructures as a core component of how we need to understand social change.},
  isbn = {978-3-030-96179-4 978-3-030-96180-0},
  langid = {english}
}

@misc{Cosentino_Personal_2024,
  title = {Towards a {{Personal Health Large Language Model}}},
  author = {Cosentino, Justin and Belyaeva, Anastasiya and Liu, Xin and Furlotte, Nicholas A. and Yang, Zhun and Lee, Chace and Schenck, Erik and Patel, Yojan and Cui, Jian and Schneider, Logan Douglas and Bryant, Robby and Gomes, Ryan G. and Jiang, Allen and Lee, Roy and Liu, Yun and Perez, Javier and Rogers, Jameson K. and Speed, Cathy and Tailor, Shyam and Walker, Megan and Yu, Jeffrey and Althoff, Tim and Heneghan, Conor and Hernandez, John and Malhotra, Mark and Stern, Leor and Matias, Yossi and Corrado, Greg S. and Patel, Shwetak and Shetty, Shravya and Zhan, Jiening and Prabhakara, Shruthi and McDuff, Daniel and McLean, Cory Y.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.06474},
  eprint = {2406.06474},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2406.06474},
  urldate = {2024-08-12},
  abstract = {In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring. Here we present Personal Health Large Language Model (PH-LLM), fine-tuned from Gemini for understanding and reasoning over numerical time-series personal health data. We created and curated three datasets that test 1) production of personalized insights and recommendations from sleep patterns, physical activity, and physiological responses, 2) expert domain knowledge, and 3) prediction of self-reported sleep outcomes. For the first task we designed 857 case studies in collaboration with domain experts to assess real-world scenarios in sleep and fitness. Through comprehensive evaluation of domain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not statistically different from expert performance in fitness and, while experts remain superior for sleep, fine-tuning PH-LLM provided significant improvements in using relevant domain knowledge and personalizing information for sleep insights. We evaluated PH-LLM domain knowledge using multiple choice sleep medicine and fitness examinations. PH-LLM achieved 79\% on sleep and 88\% on fitness, exceeding average scores from a sample of human experts. Finally, we trained PH-LLM to predict self-reported sleep quality outcomes from textual and multimodal encoding representations of wearable data, and demonstrate that multimodal encoding is required to match performance of specialized discriminative models. Although further development and evaluation are necessary in the safety-critical personal health domain, these results demonstrate both the broad knowledge and capabilities of Gemini models and the benefit of contextualizing physiological data for personal health applications as done with PH-LLM.},
  archiveprefix = {arXiv}
}

@inproceedings{Zhao_Men_2017,
  title = {Men Also like Shopping: {{Reducing}} Gender Bias Amplification Using Corpus-Level Constraints},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  year = {2017},
  month = sep,
  pages = {2979--2989},
  publisher = {Association for Computational Linguistics},
  address = {Copenhagen, Denmark},
  doi = {10.18653/v1/D17-1323},
  url = {https://aclanthology.org/D17-1323},
  abstract = {Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5\% and 40.5\% for multilabel classification and visual semantic role labeling, respectively。}
}

@article{Dastin_Insight_2018,
  title = {Insight - {{Amazon}} Scraps Secret {{AI}} Recruiting Tool That Showed Bias against Women},
  author = {Dastin, Jeffrey},
  year = {2018},
  month = oct,
  journal = {Reuters},
  url = {https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/},
  urldate = {2024-08-12},
  abstract = {Amazon.com Inc's \&lt;AMZN.O\&gt; machine-learning specialists uncovered a big problem: their new recruiting engine did not like women.},
  chapter = {World},
  langid = {american}
}

@inproceedings{De-Arteaga_Bias_2019,
  title = {Bias in {{Bios}}: {{A Case Study}} of {{Semantic Representation Bias}} in a {{High-Stakes Setting}}},
  shorttitle = {Bias in {{Bios}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {{De-Arteaga}, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
  year = {2019},
  month = jan,
  pages = {120--128},
  publisher = {ACM},
  address = {Atlanta GA USA},
  doi = {10.1145/3287560.3287572},
  url = {https://dl.acm.org/doi/10.1145/3287560.3287572},
  urldate = {2022-02-26},
  isbn = {978-1-4503-6125-5},
  langid = {english}
}

@inproceedings{Davidson_Racial_2019,
  title = {Racial {{Bias}} in {{Hate Speech}} and {{Abusive Language Detection Datasets}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Abusive Language Online}}},
  author = {Davidson, Thomas and Bhattacharya, Debasmita and Weber, Ingmar},
  year = {2019},
  pages = {25--35},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-3504},
  url = {https://www.aclweb.org/anthology/W19-3504},
  urldate = {2021-02-19},
  langid = {english}
}

@inproceedings{Bolukbasi_Man_2016,
  title = {Man Is to {{Computer Programmer}} as {{Woman}} Is to {{Homemaker}}? {{Debiasing Word Embeddings}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
  year = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf}
}

@misc{Anand_Don_2024,
  title = {Don't {{Blame}} the {{Data}}, {{Blame}} the {{Model}}: {{Understanding Noise}} and {{Bias When Learning}} from {{Subjective Annotations}}},
  shorttitle = {Don't {{Blame}} the {{Data}}, {{Blame}} the {{Model}}},
  author = {Anand, Abhishek and Mokhberian, Negar and Kumar, Prathyusha Naresh and Saha, Anweasha and He, Zihao and Rao, Ashwin and Morstatter, Fred and Lerman, Kristina},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04085},
  eprint = {2403.04085},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.04085},
  url = {http://arxiv.org/abs/2403.04085},
  urldate = {2024-03-11},
  abstract = {Researchers have raised awareness about the harms of aggregating labels especially in subjective tasks that naturally contain disagreements among human annotators. In this work we show that models that are only provided aggregated labels show low confidence on high-disagreement data instances. While previous studies consider such instances as mislabeled, we argue that the reason the high-disagreement text instances have been hard-to-learn is that the conventional aggregated models underperform in extracting useful signals from subjective tasks. Inspired by recent studies demonstrating the effectiveness of learning from raw annotations, we investigate classifying using Multiple Ground Truth (Multi-GT) approaches. Our experiments show an improvement of confidence for the high-disagreement instances.},
  archiveprefix = {arXiv}
}

@misc{Solaiman_Evaluating_2024,
  title = {Evaluating the {{Social Impact}} of {{Generative AI Systems}} in {{Systems}} and {{Society}}},
  author = {Solaiman, Irene and Talat, Zeerak and Agnew, William and Ahmad, Lama and Baker, Dylan and Blodgett, Su Lin and Chen, Canyu and Daum{\'e} III, Hal and Dodge, Jesse and Duan, Isabella and Evans, Ellie and Friedrich, Felix and Ghosh, Avijit and Gohar, Usman and Hooker, Sara and Jernite, Yacine and Kalluri, Ria and Lusoli, Alberto and Leidinger, Alina and Lin, Michelle and Lin, Xiuzhu and Luccioni, Sasha and Mickel, Jennifer and Mitchell, Margaret and Newman, Jessica and Ovalle, Anaelia and Png, Marie-Therese and Singh, Shubham and Strait, Andrew and Struppek, Lukas and Subramonian, Arjun},
  year = {2024},
  month = jun,
  number = {arXiv:2306.05949},
  eprint = {2306.05949},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2306.05949},
  urldate = {2024-08-12},
  abstract = {Generative AI systems across modalities, ranging from text (including code), image, audio, and video, have broad social impacts, but there is no official standard for means of evaluating those impacts or for which impacts should be evaluated. In this paper, we present a guide that moves toward a standard approach in evaluating a base generative AI system for any modality in two overarching categories: what can be evaluated in a base system independent of context and what can be evaluated in a societal context. Importantly, this refers to base systems that have no predetermined application or deployment context, including a model itself, as well as system components, such as training data. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to listed generative modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what can be evaluated in a broader societal context, each with its own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm.},
  archiveprefix = {arXiv}
}

@techreport{CapitalEconomics_AI_2022,
  type = {Commissioned {{Report}}},
  title = {{{AI}} Activity in {{UK}} Businesses: {{Executive Summary}}},
  shorttitle = {{{AI}} Activity in {{UK}} Businesses},
  author = {{Capital Economics}},
  year = {2022},
  month = dec,
  address = {London, UK},
  institution = {Department for Digital, Culture, Media \& Sport},
  url = {https://www.gov.uk/government/publications/ai-activity-in-uk-businesses/ai-activity-in-uk-businesses-executive-summary},
  urldate = {2024-08-12},
  langid = {english}
}

@article{liu2012enterprise,
  title={The enterprise risk management and the risk oriented internal audit},
  author={Liu, Jie and others},
  journal={Ibusiness},
  volume={4},
  number={03},
  pages={287},
  year={2012}
}

@inproceedings{niven-kao-2019-probing,
    title = "Probing Neural Network Comprehension of Natural Language Arguments",
    author = "Niven, Timothy  and
      Kao, Hung-Yu",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1459",
    doi = "10.18653/v1/P19-1459",
    pages = "4658--4664",
    abstract = "We are surprised to find that BERT{'}s peak performance of 77{\%} on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.",
}

@misc{solaiman2024evaluatingsocialimpactgenerative,
      title={Evaluating the Social Impact of Generative AI Systems in Systems and Society}, 
      author={Irene Solaiman and Zeerak Talat and William Agnew and Lama Ahmad and Dylan Baker and Su Lin Blodgett and Canyu Chen and Hal Daumé III au2 and Jesse Dodge and Isabella Duan and Ellie Evans and Felix Friedrich and Avijit Ghosh and Usman Gohar and Sara Hooker and Yacine Jernite and Ria Kalluri and Alberto Lusoli and Alina Leidinger and Michelle Lin and Xiuzhu Lin and Sasha Luccioni and Jennifer Mickel and Margaret Mitchell and Jessica Newman and Anaelia Ovalle and Marie-Therese Png and Shubham Singh and Andrew Strait and Lukas Struppek and Arjun Subramonian},
      year={2024},
      eprint={2306.05949},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2306.05949}, 
}

@article{anwar2024foundational,
  title={Foundational challenges in assuring alignment and safety of large language models},
  author={Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and others},
  journal={arXiv preprint arXiv:2404.09932},
  year={2024}
}

@article{weidinger2023sociotechnical,
  title={Sociotechnical safety evaluation of generative ai systems},
  author={Weidinger, Laura and Rauh, Maribeth and Marchal, Nahema and Manzini, Arianna and Hendricks, Lisa Anne and Mateos-Garcia, Juan and Bergman, Stevie and Kay, Jackie and Griffin, Conor and Bariach, Ben and others},
  journal={arXiv preprint arXiv:2310.11986},
  year={2023}
}

@article{van2020embedding,
  title={Embedding values in artificial intelligence (AI) systems},
  author={Van de Poel, Ibo},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={385--409},
  year={2020},
  publisher={Springer}
}

@article{gallegos2024bias,
  title={Bias and fairness in large language models: A survey},
  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
  journal={Computational Linguistics},
  pages={1--79},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}


@article{kasirzadeh2021reasons,
  title={Reasons, values, stakeholders: A philosophical framework for explainable artificial intelligence},
  author={Kasirzadeh, Atoosa},
  journal={arXiv preprint arXiv:2103.00752},
  year={2021}
}

@article{pistilli2024civics,
  title={CIVICS: Building a Dataset for Examining Culturally-Informed Values in Large Language Models},
  author={Pistilli, Giada and Leidinger, Alina and Jernite, Yacine and Kasirzadeh, Atoosa and Luccioni, Alexandra Sasha and Mitchell, Margaret},
  journal={arXiv preprint arXiv:2405.13974},
  year={2024}
}

@inproceedings{welch-etal-2020-compositional,
    title = "Compositional Demographic Word Embeddings",
    author = "Welch, Charles  and
      Kummerfeld, Jonathan K.  and
      P{\'e}rez-Rosas, Ver{\'o}nica  and
      Mihalcea, Rada",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.334",
    doi = "10.18653/v1/2020.emnlp-main.334",
    pages = "4076--4089",
    abstract = "Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.",
}

@inproceedings{10.1145/3630106.3659021,
author = {Davani, Aida and D\'{\i}az, Mark and Baker, Dylan and Prabhakaran, Vinodkumar},
title = {Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659021},
doi = {10.1145/3630106.3659021},
abstract = {Recent years have seen substantial investments in AI-based tools designed to detect offensive language at scale, aiming to moderate social media platforms, and ensure safety of conversational AI technologies such as ChatGPT and Bard. These efforts largely treat this task as a technical endeavor, relying on data annotated for offensiveness by a global crowd workforce, without considering crowd workers’ socio-cultural backgrounds or the values their perceptions reflect. Existing research that examines systematic variations in annotators’ judgments often reduces these differences to socio-demographic categories along racial, or gender dimensions, overlooking the diversity of perspectives within such groups. On the other hand, social psychology literature highlights the crucial role that both cultural and psychological factors play in human perceptions and judgments. Through a large-scale cross-cultural study of 4309 participants from 21 countries across eight cultural regions, we demonstrate substantial cross-cultural and individual moral value-based differences in interpretations of offensiveness. Our study reveals specific regions that are significantly more sensitive to offensive language. Furthermore, using the Moral Foundations Theory, we study the underlying moral values that contribute to these cross-cultural differences. Notably, we find that participants’ moral values play a far more important role in shaping their perceptions of offensiveness than geo-cultural distinctions. Our investigation, using a non-monolithic framework to understand cross-cultural moral concerns, reveals crucial insights that can be extrapolated to building AI models for the pluralistic world. Our results call for more extensive consideration of diverse human moral values when deploying AI models across diverse geo-cultural contexts.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2007–2021},
numpages = {15},
keywords = {Annotation, Offensiveness, Pluralism, Subjectivity, Value Alignment},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{abercrombie-etal-2024-revisiting,
    title = "Revisiting Annotation of Online Gender-Based Violence",
    author = "Abercrombie, Gavin  and
      Vitsakis, Nikolas  and
      Jiang, Aiqi  and
      Konstas, Ioannis",
    editor = "Abercrombie, Gavin  and
      Basile, Valerio  and
      Bernadi, Davide  and
      Dudy, Shiran  and
      Frenda, Simona  and
      Havens, Lucy  and
      Tonelli, Sara",
    booktitle = "Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives) @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.nlperspectives-1.3",
    pages = "31--41",
    abstract = "Online Gender-Based Violence is an increasing problem, but existing datasets fail to capture the plurality of possible annotator perspectives or ensure representation of affected groups. In a pilot study, we revisit the annotation of a widely used dataset to investigate the relationship between annotator identities and underlying attitudes and the responses they give to a sexism labelling task. We collect demographic and attitudinal information about crowd-sourced annotators using two validated surveys from Social Psychology. While we do not find any correlation between underlying attitudes and annotation behaviour, ethnicity does appear to be related to annotator responses for this pool of crowd-workers. We also conduct initial classification experiments using Large Language Models, finding that a state-of-the-art model trained with human feedback benefits from our broad data collection to perform better on the new labels. This study represents the initial stages of a wider data collection project, in which we aim to develop a taxonomy of GBV in partnership with affected stakeholders.",
}

@article{Chmielewski-Kucker-2020-MturkCrisis,
author = {Michael Chmielewski and Sarah C. Kucker},
title ={An MTurk Crisis? Shifts in Data Quality and the Impact on Study Results},
journal = {Social Psychological and Personality Science},
volume = {11},
number = {4},
pages = {464-473},
year = {2020},
doi = {10.1177/1948550619875149},
URL = {https://doi.org/10.1177/1948550619875149},
eprint = {https://doi.org/10.1177/1948550619875149}
,
abstract = { Amazon’s Mechanical Turk (MTurk) is arguably one of the most important research tools of the past decade. The ability to rapidly collect large amounts of high-quality human subjects data has advanced multiple fields, including personality and social psychology. Beginning in summer 2018, concerns arose regarding MTurk data quality leading to questions about the utility of MTurk for psychological research. We present empirical evidence of a substantial decrease in data quality using a four-wave naturalistic experimental design: pre-, during, and post-summer 2018. During and to some extent post-summer 2018, we find significant increases in participants failing response validity indicators, decreases in reliability and validity of a widely used personality measure, and failures to replicate well-established findings. However, these detrimental effects can be mitigated by using response validity indicators and screening the data. We discuss implications and offer suggestions to ensure data quality. }
}




@inproceedings{feng-etal-2023-pretraining,
    title = "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair {NLP} Models",
    author = "Feng, Shangbin  and
      Park, Chan Young  and
      Liu, Yuhan  and
      Tsvetkov, Yulia",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.656",
    doi = "10.18653/v1/2023.acl-long.656",
    pages = "11737--11762",
    abstract = "Language models (LMs) are pretrained on diverse data sources{---}news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.",
}

@inproceedings{sap-etal-2022-annotators,
    title = "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
    author = "Sap, Maarten  and
      Swayamdipta, Swabha  and
      Vianna, Laura  and
      Zhou, Xuhui  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.431",
    doi = "10.18653/v1/2022.naacl-main.431",
    pages = "5884--5906",
    abstract = "The perceived toxicity of language can vary based on someone{'}s identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system{'}s ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.",
}

@inproceedings{lee-etal-2024-exploring-cross,
    title = "Exploring Cross-Cultural Differences in {E}nglish Hate Speech Annotations: From Dataset Construction to Analysis",
    author = "Lee, Nayeon  and
      Jung, Chani  and
      Myung, Junho  and
      Jin, Jiho  and
      Camacho-Collados, Jose  and
      Kim, Juho  and
      Oh, Alice",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.236",
    doi = "10.18653/v1/2024.naacl-long.236",
    pages = "4205--4224",
    abstract = "Most hate speech datasets neglect the cultural diversity within a single language, resulting in a critical shortcoming in hate speech detection. To address this, we introduce CREHate, a CRoss-cultural English Hate speech dataset. To construct CREHate, we follow a two-step procedure: 1) cultural post collection and 2) cross-cultural annotation. We sample posts from the SBIC dataset, which predominantly represents North America, and collect posts from four geographically diverse English-speaking countries (Australia, United Kingdom, Singapore, and South Africa) using culturally hateful keywords we retrieve from our survey. Annotations are collected from the four countries plus the United States to establish representative labels for each country. Our analysis highlights statistically significant disparities across countries in hate speech annotations. Only 56.2{\%} of the posts in CREHate achieve consensus among all countries, with the highest pairwise label difference rate of 26{\%}. Qualitative analysis shows that label disagreement occurs mostly due to different interpretations of sarcasm and the personal bias of annotators on divisive topics. Lastly, we evaluate large language models (LLMs) under a zero-shot setting and show that current LLMs tend to show higher accuracies on Anglosphere country labels in CREHate.Our dataset and codes are available at: https://github.com/nlee0212/CREHate",
}

@inproceedings{10.1145/3543507.3583290,
author = {Gupta, Soumyajit and Lee, Sooyong and De-Arteaga, Maria and Lease, Matthew},
title = {Same Same, But Different: Conditional Multi-Task Learning for Demographic-Specific Toxicity Detection},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583290},
doi = {10.1145/3543507.3583290},
abstract = {Algorithmic bias often arises as a result of differential subgroup validity, in which predictive relationships vary across groups. For example, in toxic language detection, comments targeting different demographic groups can vary markedly across groups. In such settings, trained models can be dominated by the relationships that best fit the majority group, leading to disparate performance. We propose framing toxicity detection as multi-task learning (MTL), allowing a model to specialize on the relationships that are relevant to each demographic group while also leveraging shared properties across groups. With toxicity detection, each task corresponds to identifying toxicity against a particular demographic group. However, traditional MTL requires labels for all tasks to be present for every data point. To address this, we propose Conditional MTL (CondMTL), wherein only training examples relevant to the given demographic group are considered by the loss function. This lets us learn group specific representations in each branch which are not cross contaminated by irrelevant labels. Results on synthetic and real data show that using CondMTL improves predictive recall over various baselines in general and for the minority demographic group in particular, while having similar overall accuracy.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3689–3700},
numpages = {12},
keywords = {Multi-task learning, conditional loss, differential subgroup validity},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{fleisig-etal-2023-majority,
    title = "When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks",
    author = "Fleisig, Eve  and
      Abebe, Rediet  and
      Klein, Dan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.415",
    doi = "10.18653/v1/2023.emnlp-main.415",
    pages = "6715--6726",
    abstract = "Though majority vote among annotators is typically used for ground truth labels in machine learning, annotator disagreement in tasks such as hate speech detection may reflect systematic differences in opinion across groups, not noise. Thus, a crucial problem in hate speech detection is determining if a statement is offensive to the demographic group that it targets, when that group may be a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to predict the ratings of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22{\%} at predicting individual annotators{'} ratings and by 33{\%} at predicting variance among annotators, which provides a metric for model uncertainty downstream. We find that annotators{'} ratings can be predicted using their demographic information as well as opinions on online content, and that non-invasive questions on annotators{'} online experiences minimize the need to collect demographic information when predicting annotators{'} opinions.",
}

@article{davani-etal-2023-hate,
    title = "Hate Speech Classifiers Learn Normative Social Stereotypes",
    author = "Davani, Aida Mostafazadeh  and
      Atari, Mohammad  and
      Kennedy, Brendan  and
      Dehghani, Morteza",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.18",
    doi = "10.1162/tacl_a_00550",
    pages = "300--319",
    abstract = "Social stereotypes negatively impact individuals{'} judgments about different groups and may have a critical role in understanding language directed toward marginalized groups. Here, we assess the role of social stereotypes in the automated detection of hate speech in the English language by examining the impact of social stereotypes on annotation behaviors, annotated datasets, and hate speech classifiers. Specifically, we first investigate the impact of novice annotators{'} stereotypes on their hate-speech-annotation behavior. Then, we examine the effect of normative stereotypes in language on the aggregated annotators{'} judgments in a large annotated corpus. Finally, we demonstrate how normative stereotypes embedded in language resources are associated with systematic prediction errors in a hate-speech classifier. The results demonstrate that hate-speech classifiers reflect social stereotypes against marginalized groups, which can perpetuate social inequalities when propagated at scale. This framework, combining social-psychological and computational-linguistic methods, provides insights into sources of bias in hate-speech moderation, informing ongoing debates regarding machine learning fairness.",
}

@inproceedings{leonardelli-etal-2023-semeval,
    title = "{S}em{E}val-2023 Task 11: Learning with Disagreements ({L}e{W}i{D}i)",
    author = "Leonardelli, Elisa  and
      Abercrombie, Gavin  and
      Almanea, Dina  and
      Basile, Valerio  and
      Fornaciari, Tommaso  and
      Plank, Barbara  and
      Rieser, Verena  and
      Uma, Alexandra  and
      Poesio, Massimo",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Da San Martino, Giovanni  and
      Tayyar Madabushi, Harish  and
      Kumar, Ritesh  and
      Sartori, Elisa},
    booktitle = "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.semeval-1.314",
    doi = "10.18653/v1/2023.semeval-1.314",
    pages = "2304--2318",
    abstract = "NLP datasets annotated with human judgments are rife with disagreements between the judges. This is especially true for tasks depending on subjective judgments such as sentiment analysis or offensive language detection. Particularly in these latter cases, the NLP community has come to realize that the common approach of reconciling{'} these different subjective interpretations risks misrepresenting the evidence. Many NLP researchers have therefore concluded that rather than eliminating disagreements from annotated corpora, we should preserve themindeed, some argue that corpora should aim to preserve all interpretations produced by annotators. But this approach to corpus creation for NLP has not yet been widely accepted. The objective of the Le-Wi-Di series of shared tasks is to promote this approach to developing NLP models by providing a unified framework for training and evaluating with such datasets. We report on the second such shared task, which differs from the first edition in three crucial respects: (i) it focuses entirely on NLP, instead of both NLP and computer vision tasks in its first edition; (ii) it focuses on subjective tasks, instead of covering different types of disagreements as training with aggregated labels for subjective NLP tasks is in effect a misrepresentation of the data; and (iii) for the evaluation, we concentrated on soft approaches to evaluation. This second edition of Le-Wi-Di attracted a wide array of partici- pants resulting in 13 shared task submission papers.",
}

@inproceedings{rottger-etal-2022-two,
    title = "Two Contrasting Data Annotation Paradigms for Subjective {NLP} Tasks",
    author = "Rottger, Paul  and
      Vidgen, Bertie  and
      Hovy, Dirk  and
      Pierrehumbert, Janet",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.13",
    doi = "10.18653/v1/2022.naacl-main.13",
    pages = "175--190",
    abstract = "Labelled data is the foundation of most natural language processing tasks. However, labelling data is difficult and there often are diverse valid beliefs about what the correct data labels should be. So far, dataset creators have acknowledged annotator subjectivity, but rarely actively managed it in the annotation process. This has led to partly-subjective datasets that fail to serve a clear downstream use. To address this issue, we propose two contrasting paradigms for data annotation. The descriptive paradigm encourages annotator subjectivity, whereas the prescriptive paradigm discourages it. Descriptive annotation allows for the surveying and modelling of different beliefs, whereas prescriptive annotation enables the training of models that consistently apply one belief. We discuss benefits and challenges in implementing both paradigms, and argue that dataset creators should explicitly aim for one or the other to facilitate the intended use of their dataset. Lastly, we conduct an annotation experiment using hate speech data that illustrates the contrast between the two paradigms.",
}

@article{yao2024survey,
  title={A survey on large language model (llm) security and privacy: The good, the bad, and the ugly},
  author={Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  journal={High-Confidence Computing},
  pages={100211},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{lee-etal-2023-large,
    title = "Can Large Language Models Capture Dissenting Human Voices?",
    author = "Lee, Noah  and
      An, Na Min  and
      Thorne, James",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.278",
    doi = "10.18653/v1/2023.emnlp-main.278",
    pages = "4569--4585",
    abstract = "Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques to estimate the multinomial distribution: Monte Carlo Estimation (MCE) and Log Probability Estimation (LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.",
}

@inproceedings{mokhberian-etal-2024-capturing,
    title = "Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks",
    author = "Mokhberian, Negar  and
      Marmarelis, Myrl  and
      Hopp, Frederic  and
      Basile, Valerio  and
      Morstatter, Fred  and
      Lerman, Kristina",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.407",
    pages = "7337--7349",
    abstract = "Supervised classification heavily depends on datasets annotated by humans. However, in subjective tasks such as toxicity classification, these annotations often exhibit low agreement among raters. Annotations have commonly been aggregated by employing methods like majority voting to determine a single ground truth label. In subjective tasks, aggregating labels will result in biased labeling and, consequently, biased models that can overlook minority opinions. Previous studies have shed light on the pitfalls of label aggregation and have introduced a handful of practical approaches to tackle this issue. Recently proposed multi-annotator models, which predict labels individually per annotator, are vulnerable to under-determination for annotators with few samples. This problem is exacerbated in crowdsourced datasets. In this work, we propose Annotator Aware Representations for Texts (AART) for subjective classification tasks. Our approach involves learning representations of annotators, allowing for exploration of annotation behaviors. We show the improvement of our method on metrics that assess the performance on capturing individual annotators{'} perspectives. Additionally, we demonstrate fairness metrics to evaluate our model{'}s equability of performance for marginalized annotators compared to others.",
}

@inproceedings{biester-etal-2022-analyzing,
    title = "Analyzing the Effects of Annotator Gender across {NLP} Tasks",
    author = "Biester, Laura  and
      Sharma, Vanita  and
      Kazemi, Ashkan  and
      Deng, Naihao  and
      Wilson, Steven  and
      Mihalcea, Rada",
    editor = "Abercrombie, Gavin  and
      Basile, Valerio  and
      Tonelli, Sara  and
      Rieser, Verena  and
      Uma, Alexandra",
    booktitle = "Proceedings of the 1st Workshop on Perspectivist Approaches to NLP @LREC2022",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.nlperspectives-1.2",
    pages = "10--19",
    abstract = "Recent studies have shown that for subjective annotation tasks, the demographics, lived experiences, and identity of annotators can have a large impact on how items are labeled. We expand on this work, hypothesizing that gender may correlate with differences in annotations for a number of NLP benchmarks, including those that are fairly subjective (e.g., affect in text) and those that are typically considered to be objective (e.g., natural language inference). We develop a robust framework to test for differences in annotation across genders for four benchmark datasets. While our results largely show a lack of statistically significant differences in annotation by males and females for these tasks, the framework can be used to analyze differences in annotation between various other demographic groups in future work. Finally, we note that most datasets are collected without annotator demographics and released only in aggregate form; we call on the community to consider annotator demographics as data is collected, and to release dis-aggregated data to allow for further work analyzing variability among annotators.",
}

@inproceedings{huang-etal-2023-incorporating,
    title = "Incorporating Worker Perspectives into {MT}urk Annotation Practices for {NLP}",
    author = "Huang, Olivia  and
      Fleisig, Eve  and
      Klein, Dan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.64",
    doi = "10.18653/v1/2023.emnlp-main.64",
    pages = "1010--1028",
    abstract = "Current practices regarding data collection for natural language processing on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on data quality and heuristics shared among NLP researchers. However, without considering the perspectives of MTurk workers, these approaches are susceptible to issues regarding workers{'} rights and poor response quality. We conducted a critical literature review and a survey of MTurk workers aimed at addressing open questions regarding best practices for fair payment, worker privacy, data quality, and considering worker incentives. We found that worker preferences are often at odds with received wisdom among NLP researchers. Surveyed workers preferred reliable, reasonable payments over uncertain, very high payments; reported frequently lying on demographic questions; and expressed frustration at having work rejected with no explanation. We also found that workers view some quality control methods, such as requiring minimum response times or Master{'}s qualifications, as biased and largely ineffective. Based on the survey results, we provide recommendations on how future NLP studies may better account for MTurk workers{'} experiences in order to respect workers{'} rights and improve data quality.",
}

@article{eyal2021data,
  title={Data quality of platforms and panels for online behavioral research},
  author={Eyal, Peer and David, Rothschild and Andrew, Gordon and Zak, Evernden and Ekaterina, Damer},
  journal={Behavior research methods},
  pages={1--20},
  year={2021},
  publisher={Springer}
}

@article{kennedy2020shape,
  title={The shape of and solutions to the MTurk quality crisis},
  author={Kennedy, Ryan and Clifford, Scott and Burleigh, Tyler and Waggoner, Philip D and Jewell, Ryan and Winter, Nicholas JG},
  journal={Political Science Research and Methods},
  volume={8},
  number={4},
  pages={614--629},
  year={2020},
  publisher={Cambridge University Press}
}

@ARTICLE{Allahbakhsh-etal-2013-QualityCrowdIssue,
  author={Allahbakhsh, Mohammad and Benatallah, Boualem and Ignjatovic, Aleksandar and Motahari-Nezhad, Hamid Reza and Bertino, Elisa and Dustdar, Schahram},
  journal={IEEE Internet Computing}, 
  title={Quality Control in Crowdsourcing Systems: Issues and Directions}, 
  year={2013},
  volume={17},
  number={2},
  pages={76-81},
  keywords={Crowdsourcing;Communities;Quality control;Encyclopedias;Electronic publishing;crowdsourcing;quality control;crowdsourcing workflows},
  doi={10.1109/MIC.2013.20}}

@article{Daniel-etal-2018-QualityCrowdSurvey,
author = {Daniel, Florian and Kucherbaev, Pavel and Cappiello, Cinzia and Benatallah, Boualem and Allahbakhsh, Mohammad},
title = {Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques, and Assurance Actions},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3148148},
doi = {10.1145/3148148},
abstract = {Crowdsourcing enables one to leverage on the intelligence and wisdom of potentially large groups of individuals toward solving problems. Common problems approached with crowdsourcing are labeling images, translating or transcribing text, providing opinions or ideas, and similar—all tasks that computers are not good at or where they may even fail altogether. The introduction of humans into computations and/or everyday work, however, also poses critical, novel challenges in terms of quality control, as the crowd is typically composed of people with unknown and very diverse abilities, skills, interests, personal objectives, and technological resources. This survey studies quality in the context of crowdsourcing along several dimensions, so as to define and characterize it and to understand the current state of the art. Specifically, this survey derives a quality model for crowdsourcing tasks, identifies the methods and techniques that can be used to assess the attributes of the model, and the actions and strategies that help prevent and mitigate quality problems. An analysis of how these features are supported by the state of the art further identifies open issues and informs an outlook on hot future research directions.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {7},
numpages = {40},
keywords = {quality model, attributes, assurance, assessment, Crowdsourcing}
}


@article{xu2024hallucination,
  title={Hallucination is inevitable: An innate limitation of large language models},
  author={Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2401.11817},
  year={2024}
}

@inproceedings{abercrombie-etal-2023-mirages,
    title = "Mirages. On Anthropomorphism in Dialogue Systems",
    author = "Abercrombie, Gavin  and
      Cercas Curry, Amanda  and
      Dinkar, Tanvi  and
      Rieser, Verena  and
      Talat, Zeerak",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.290",
    doi = "10.18653/v1/2023.emnlp-main.290",
    pages = "4776--4790",
    abstract = "Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise thereof, including reinforcing gender stereotypes and conceptions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.",
}


@article{kirk2023empty,
  title={The empty signifier problem: Towards clearer paradigms for operationalising" alignment" in large language models},
  author={Kirk, Hannah Rose and Vidgen, Bertie and R{\"o}ttger, Paul and Hale, Scott A},
  journal={arXiv preprint arXiv:2310.02457},
  year={2023}
}
@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}



@article{sorensen2024roadmap,
  title={A roadmap to pluralistic alignment},
  author={Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others},
  journal={arXiv preprint arXiv:2402.05070},
  year={2024}
}

@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{kasirzadeh2023conversation,
  title={In conversation with artificial intelligence: aligning language models with human values},
  author={Kasirzadeh, Atoosa and Gabriel, Iason},
  journal={Philosophy \& Technology},
  volume={36},
  number={2},
  pages={27},
  year={2023},
  publisher={Springer}
}

@inproceedings{santurkar2023whose,
  title={Whose opinions do language models reflect?},
  author={Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  booktitle={International Conference on Machine Learning},
  pages={29971--30004},
  year={2023},
  organization={PMLR}
}

@inproceedings{kasirzadeh2021ethical,
  title={The ethical gravity thesis: Marrian levels and the persistence of bias in automated decision-making systems},
  author={Kasirzadeh, Atoosa and Klein, Colin},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={618--626},
  year={2021}
}

@article{kasneci2023chatgpt,
  title={ChatGPT for good? On opportunities and challenges of large language models for education},
  author={Kasneci, Enkelejda and Se{\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},
  journal={Learning and individual differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}

@article{Lucchi_2023, 
    title={ChatGPT: A Case Study on Copyright Challenges for Generative Artificial Intelligence Systems}, 
    DOI={10.1017/err.2023.59}, 
    journal={European Journal of Risk Regulation}, 
    author={Lucchi, Nicola}, 
    year={2023}, 
    pages={1–23}
} 

@inproceedings{basile2020s,
  title={It’s the end of the gold standard as we know it: Leveraging non-aggregated data for better evaluation and explanation of subjective tasks},
  author={Basile, Valerio},
  booktitle={International Conference of the Italian Association for Artificial Intelligence},
  pages={441--453},
  year={2020},
  organization={Springer}
}

@article{hovy2021five,
  title={Five sources of bias in natural language processing},
  author={Hovy, Dirk and Prabhumoye, Shrimai},
  journal={Language and linguistics compass},
  volume={15},
  number={8},
  pages={e12432},
  year={2021},
  publisher={Wiley Online Library}
}

@article{uma2022scaling,
  title={Scaling and disagreements: Bias, noise, and ambiguity},
  author={Uma, Alexandra and Almanea, Dina and Poesio, Massimo},
  journal={Frontiers in Artificial Intelligence},
  volume={5},
  pages={818451},
  year={2022},
  publisher={Frontiers Media SA}
}

@article{uma2021learning,
  title={Learning from disagreement: A survey},
  author={Uma, Alexandra N and Fornaciari, Tommaso and Hovy, Dirk and Paun, Silviu and Plank, Barbara and Poesio, Massimo},
  journal={Journal of Artificial Intelligence Research},
  volume={72},
  pages={1385--1470},
  year={2021}
}

@article{sok2023chatgpt,
  title={ChatGPT for education and research: A review of benefits and risks},
  author={Sok, Sarin and Heng, Kimkong},
  journal={Cambodian Journal of Educational Research},
  volume={3},
  number={1},
  pages={110--121},
  year={2023}
}

@article{walczak2023challenges,
  title={Challenges for higher education in the era of widespread access to Generative AI},
  author={Walczak, Krzysztof and Cellary, Wojciech},
  journal={Economics and Business Review},
  volume={9},
  number={2},
  pages={71--100},
  year={2023}
}

@article{llorens2021gender,
  title={Gender bias in academia: A lifetime problem that needs solutions},
  author={Llorens, Ana{\"\i}s and Tzovara, Athina and Bellier, Ludovic and Bhaya-Grossman, Ilina and Bidet-Caulet, Aur{\'e}lie and Chang, William K and Cross, Zachariah R and Dominguez-Faus, Rosa and Flinker, Adeen and Fonken, Yvonne and others},
  journal={Neuron},
  volume={109},
  number={13},
  pages={2047--2074},
  year={2021},
  publisher={Elsevier}
}

@article{dupree2021racial,
  title={Racial inequality in academia: Systemic origins, modern challenges, and policy recommendations},
  author={Dupree, Cydney H and Boykin, C Malik},
  journal={Policy Insights from the Behavioral and Brain Sciences},
  volume={8},
  number={1},
  pages={11--18},
  year={2021},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{madaio2022assessing,
  title={Assessing the fairness of ai systems: Ai practitioners' processes, challenges, and needs for support},
  author={Madaio, Michael and Egede, Lisa and Subramonyam, Hariharan and Wortman Vaughan, Jennifer and Wallach, Hanna},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={CSCW1},
  pages={1--26},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{haque2024we,
  title={Are We Asking the Right Questions?: Designing for Community Stakeholders’ Interactions with AI in Policing},
  author={Haque, MD Romael and Saxena, Devansh and Weathington, Katy and Chudzik, Joseph and Guha, Shion},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--20},
  year={2024}
}



@article{yang2023impact,
  title={The impact of ChatGPT and LLMs on medical imaging stakeholders: perspectives and use cases},
  author={Yang, Jiancheng and Li, Hongwei Bran and Wei, Donglai},
  journal={Meta-Radiology},
  pages={100007},
  year={2023},
  publisher={Elsevier}
}

@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}

@article{anderljung2023frontier,
  title={Frontier AI regulation: Managing emerging risks to public safety},
  author={Anderljung, Markus and Barnhart, Joslyn and Leung, Jade and Korinek, Anton and O'Keefe, Cullen and Whittlestone, Jess and Avin, Shahar and Brundage, Miles and Bullock, Justin and Cass-Beggs, Duncan and others},
  journal={arXiv preprint arXiv:2307.03718},
  year={2023}
}

@inproceedings{arditi2024refusal,
  title={Refusal in llms is mediated by a single direction},
  author={Arditi, Andy and Balcells, O and Syed, A and Gurnee, W and Nanda, N},
  booktitle={Alignment Forum},
  pages={15},
  year={2024}
}

@article{miller2021technology,
  title={Is technology value-neutral?},
  author={Miller, Boaz},
  journal={Science, Technology, \& Human Values},
  volume={46},
  number={1},
  pages={53--80},
  year={2021},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@inproceedings{raji2021you,
  title={You can't sit with us: Exclusionary pedagogy in ai ethics education},
  author={Raji, Inioluwa Deborah and Scheuerman, Morgan Klaus and Amironesei, Razvan},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={515--525},
  year={2021}
}

@article{markelius2024mechanisms,
  title={The mechanisms of AI hype and its planetary and social costs},
  author={Markelius, Alva and Wright, Connor and Kuiper, Joahna and Delille, Natalie and Kuo, Yu-Ting},
  journal={AI and Ethics},
  pages={1--16},
  year={2024},
  publisher={Springer}
}

@incollection{winner2017artifacts,
  title={Do artifacts have politics?},
  author={Winner, Langdon},
  booktitle={Computer ethics},
  pages={177--192},
  year={2017},
  publisher={Routledge}
}

@article{gold2017robots,
  title={Robots welcome: Ethical and legal considerations for web crawling and scraping},
  author={Gold, Zachary and Latonero, Mark},
  journal={Washington Journal of Law, Technology \& Arts},
  volume={13},
  pages={275},
  year={2017}
}


@article{widder2023dislocated,
  title={Dislocated accountabilities in the “AI supply chain”: Modularity and developers’ notions of responsibility},
  author={Widder, David Gray and Nafus, Dawn},
  journal={Big Data \& Society},
  volume={10},
  number={1},
  pages={20539517231177620},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}

@incollection{danks2022digital,
  title={Digital ethics as translational ethics},
  author={Danks, David},
  booktitle={Applied ethics in a digital world},
  pages={1--15},
  year={2022},
  publisher={IGI Global}
}

@article{malazita2019infrastructures,
  title={Infrastructures of abstraction: how computer science education produces anti-political subjects},
  author={Malazita, James W and Resetar, Korryn},
  journal={Digital Creativity},
  volume={30},
  number={4},
  pages={300--312},
  year={2019},
  publisher={Taylor \& Francis}
}

@phdthesis{goldfarb-tarrant2024,
  title        = {Fairness in Transfer Learning for Natural Language Processing},
  author       = {Seraphina Goldfarb-Tarrant},
  year         = 2024,
  month        = {June},
  address      = {Edinburgh, Scotland},
  school       = {University of Example},
  type         = {PhD thesis},
  doi = {10.7488/era/458010.1145/3531146.3534637}
}

@article{navigli2023biases,
  title={Biases in large language models: origins, inventory, and discussion},
  author={Navigli, Roberto and Conia, Simone and Ross, Bj{\"o}rn},
  journal={ACM Journal of Data and Information Quality},
  volume={15},
  number={2},
  pages={1--21},
  year={2023},
  publisher={ACM New York, NY}
}

@incollection{rawdata2013,
  author      = "Lisa Gitelman and Virginia Jackson",
  title       = "Introduction",
  editor      = "Lisa Gitelman",
  booktitle   = "Raw Data Is an Oxymoron",
  publisher   = "MIT Press",
  year        = 2013,
  pages       = "1-14",
}

@online{openai2024,
  title        = {{How ChatGPT and our language models are developed}},
  author       = {OpenAI},
  year         = 2024,
  howpublished          = {\url{https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-language-models-are-developed}},
  note         = {Accessed 14 August 2024}, 
}

@article{gebru2021datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daum{\'e} and Crawford, Kate},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={86--92},
  year={2021},
  publisher={ACM New York, NY, USA}
}